{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>length_title</th>\n",
       "      <th>view count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tnTPaLOaHz8</td>\n",
       "      <td>$10,000 Every Day You Survive In A Grocery Store</td>\n",
       "      <td>9</td>\n",
       "      <td>217933713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wdjh81uH6FU</td>\n",
       "      <td>$1 vs $10,000,000 Job!</td>\n",
       "      <td>4</td>\n",
       "      <td>155681496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7dYTw-jAYkY</td>\n",
       "      <td>I Spent 7 Days Buried Alive</td>\n",
       "      <td>6</td>\n",
       "      <td>153336529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mwKJfNYwvm8</td>\n",
       "      <td>I Built 100 Wells In Africa</td>\n",
       "      <td>6</td>\n",
       "      <td>165967222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>QjvpjXdgugA</td>\n",
       "      <td>World’s Deadliest Laser Maze!</td>\n",
       "      <td>4</td>\n",
       "      <td>169840468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>JinpVA6p8Mo</td>\n",
       "      <td>Snowball Machine Gun- How to make</td>\n",
       "      <td>6</td>\n",
       "      <td>21369170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>FRlbNOno5VA</td>\n",
       "      <td>BEST Guess Who Strategy- 96% WIN record using ...</td>\n",
       "      <td>9</td>\n",
       "      <td>28131975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>nsnyl8llfH4</td>\n",
       "      <td>1st place Egg Drop project ideas- using SCIENCE</td>\n",
       "      <td>8</td>\n",
       "      <td>37534623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>8Vc-69M-UWk</td>\n",
       "      <td>iPhone ATM PIN code hack- HOW TO PREVENT</td>\n",
       "      <td>8</td>\n",
       "      <td>38813844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>-RjJtO51ykY</td>\n",
       "      <td>EASY Pinewood Derby Car WINS using Science!!!</td>\n",
       "      <td>7</td>\n",
       "      <td>30351978</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>337 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                              title  \\\n",
       "0    tnTPaLOaHz8   $10,000 Every Day You Survive In A Grocery Store   \n",
       "1    Wdjh81uH6FU                             $1 vs $10,000,000 Job!   \n",
       "2    7dYTw-jAYkY                        I Spent 7 Days Buried Alive   \n",
       "3    mwKJfNYwvm8                        I Built 100 Wells In Africa   \n",
       "4    QjvpjXdgugA                      World’s Deadliest Laser Maze!   \n",
       "..           ...                                                ...   \n",
       "332  JinpVA6p8Mo                  Snowball Machine Gun- How to make   \n",
       "333  FRlbNOno5VA  BEST Guess Who Strategy- 96% WIN record using ...   \n",
       "334  nsnyl8llfH4    1st place Egg Drop project ideas- using SCIENCE   \n",
       "335  8Vc-69M-UWk           iPhone ATM PIN code hack- HOW TO PREVENT   \n",
       "336  -RjJtO51ykY      EASY Pinewood Derby Car WINS using Science!!!   \n",
       "\n",
       "     length_title  view count  \n",
       "0               9   217933713  \n",
       "1               4   155681496  \n",
       "2               6   153336529  \n",
       "3               6   165967222  \n",
       "4               4   169840468  \n",
       "..            ...         ...  \n",
       "332             6    21369170  \n",
       "333             9    28131975  \n",
       "334             8    37534623  \n",
       "335             8    38813844  \n",
       "336             7    30351978  \n",
       "\n",
       "[337 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_csv('data/raw_data.csv')\n",
    "length_title_view_count = pd.read_csv('data/length_title_view_count.csv')  \n",
    "length_title_view_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n",
      "2048\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "f = open('frame_features.json')\n",
    "frames_features = json.load(f)\n",
    "frames_features.items()\n",
    "# length_title_view_count = length_title_view_count[frames_features.keys()]\n",
    "# length_title_view_count\n",
    "\n",
    "lst = []\n",
    "for key, val in frames_features.items():\n",
    "    print(len(val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>items</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>view count</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tnTPaLOaHz8</td>\n",
       "      <td>[{'startMillis': 0, 'durationMillis': 12920, '...</td>\n",
       "      <td>$10,000 Every Day You Survive In A Grocery Store</td>\n",
       "      <td>I didn’t expect him to stay that long \\nShop K...</td>\n",
       "      <td>217172859</td>\n",
       "      <td>Entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Wdjh81uH6FU</td>\n",
       "      <td>[{'startMillis': 0, 'durationMillis': 9390, 'i...</td>\n",
       "      <td>$1 vs $10,000,000 Job!</td>\n",
       "      <td>I can’t believe they actually hired me lol\\nTr...</td>\n",
       "      <td>155154041</td>\n",
       "      <td>Entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>7dYTw-jAYkY</td>\n",
       "      <td>[{'startMillis': 0, 'durationMillis': 11200, '...</td>\n",
       "      <td>I Spent 7 Days Buried Alive</td>\n",
       "      <td>Please don't try this at home lol\\nVerizon 5G ...</td>\n",
       "      <td>152864305</td>\n",
       "      <td>Entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>mwKJfNYwvm8</td>\n",
       "      <td>[{'startMillis': 0, 'durationMillis': 6300, 'i...</td>\n",
       "      <td>I Built 100 Wells In Africa</td>\n",
       "      <td>Click the link below to donate \\nhttps://www.b...</td>\n",
       "      <td>165621129</td>\n",
       "      <td>Entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>QjvpjXdgugA</td>\n",
       "      <td>[{'startMillis': 0, 'durationMillis': 15590, '...</td>\n",
       "      <td>World’s Deadliest Laser Maze!</td>\n",
       "      <td>I can’t believe what happened at the end…\\nDri...</td>\n",
       "      <td>169254119</td>\n",
       "      <td>Entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>332</td>\n",
       "      <td>JinpVA6p8Mo</td>\n",
       "      <td>[{'startMillis': 0, 'durationMillis': 3150, 'i...</td>\n",
       "      <td>Snowball Machine Gun- How to make</td>\n",
       "      <td>Here's how to make a Snowball Machine Gun that...</td>\n",
       "      <td>21356779</td>\n",
       "      <td>Howto &amp; Style</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>333</td>\n",
       "      <td>FRlbNOno5VA</td>\n",
       "      <td>[{'startMillis': 0, 'durationMillis': 7450, 'i...</td>\n",
       "      <td>BEST Guess Who Strategy- 96% WIN record using ...</td>\n",
       "      <td>Here is a strategy for winning 96% of your com...</td>\n",
       "      <td>28110693</td>\n",
       "      <td>Howto &amp; Style</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>334</td>\n",
       "      <td>nsnyl8llfH4</td>\n",
       "      <td>[{'startMillis': 0, 'durationMillis': 5890, 'i...</td>\n",
       "      <td>1st place Egg Drop project ideas- using SCIENCE</td>\n",
       "      <td>5 designs guaranteed to win 1st place or your ...</td>\n",
       "      <td>37505427</td>\n",
       "      <td>Howto &amp; Style</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>335</td>\n",
       "      <td>8Vc-69M-UWk</td>\n",
       "      <td>[{'startMillis': 0, 'durationMillis': 2260, 'i...</td>\n",
       "      <td>iPhone ATM PIN code hack- HOW TO PREVENT</td>\n",
       "      <td>This is a new way for bad guys to steal your A...</td>\n",
       "      <td>38803285</td>\n",
       "      <td>Howto &amp; Style</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>336</td>\n",
       "      <td>-RjJtO51ykY</td>\n",
       "      <td>[{'startMillis': 0, 'durationMillis': 9590, 'i...</td>\n",
       "      <td>EASY Pinewood Derby Car WINS using Science!!!</td>\n",
       "      <td>7 simple steps that ACTUALLY MATTER to buildin...</td>\n",
       "      <td>30348596</td>\n",
       "      <td>Howto &amp; Style</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>337 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0           id  \\\n",
       "0             0  tnTPaLOaHz8   \n",
       "1             1  Wdjh81uH6FU   \n",
       "2             2  7dYTw-jAYkY   \n",
       "3             3  mwKJfNYwvm8   \n",
       "4             4  QjvpjXdgugA   \n",
       "..          ...          ...   \n",
       "332         332  JinpVA6p8Mo   \n",
       "333         333  FRlbNOno5VA   \n",
       "334         334  nsnyl8llfH4   \n",
       "335         335  8Vc-69M-UWk   \n",
       "336         336  -RjJtO51ykY   \n",
       "\n",
       "                                                 items  \\\n",
       "0    [{'startMillis': 0, 'durationMillis': 12920, '...   \n",
       "1    [{'startMillis': 0, 'durationMillis': 9390, 'i...   \n",
       "2    [{'startMillis': 0, 'durationMillis': 11200, '...   \n",
       "3    [{'startMillis': 0, 'durationMillis': 6300, 'i...   \n",
       "4    [{'startMillis': 0, 'durationMillis': 15590, '...   \n",
       "..                                                 ...   \n",
       "332  [{'startMillis': 0, 'durationMillis': 3150, 'i...   \n",
       "333  [{'startMillis': 0, 'durationMillis': 7450, 'i...   \n",
       "334  [{'startMillis': 0, 'durationMillis': 5890, 'i...   \n",
       "335  [{'startMillis': 0, 'durationMillis': 2260, 'i...   \n",
       "336  [{'startMillis': 0, 'durationMillis': 9590, 'i...   \n",
       "\n",
       "                                                 title  \\\n",
       "0     $10,000 Every Day You Survive In A Grocery Store   \n",
       "1                               $1 vs $10,000,000 Job!   \n",
       "2                          I Spent 7 Days Buried Alive   \n",
       "3                          I Built 100 Wells In Africa   \n",
       "4                        World’s Deadliest Laser Maze!   \n",
       "..                                                 ...   \n",
       "332                  Snowball Machine Gun- How to make   \n",
       "333  BEST Guess Who Strategy- 96% WIN record using ...   \n",
       "334    1st place Egg Drop project ideas- using SCIENCE   \n",
       "335           iPhone ATM PIN code hack- HOW TO PREVENT   \n",
       "336      EASY Pinewood Derby Car WINS using Science!!!   \n",
       "\n",
       "                                           description  view count  \\\n",
       "0    I didn’t expect him to stay that long \\nShop K...   217172859   \n",
       "1    I can’t believe they actually hired me lol\\nTr...   155154041   \n",
       "2    Please don't try this at home lol\\nVerizon 5G ...   152864305   \n",
       "3    Click the link below to donate \\nhttps://www.b...   165621129   \n",
       "4    I can’t believe what happened at the end…\\nDri...   169254119   \n",
       "..                                                 ...         ...   \n",
       "332  Here's how to make a Snowball Machine Gun that...    21356779   \n",
       "333  Here is a strategy for winning 96% of your com...    28110693   \n",
       "334  5 designs guaranteed to win 1st place or your ...    37505427   \n",
       "335  This is a new way for bad guys to steal your A...    38803285   \n",
       "336  7 simple steps that ACTUALLY MATTER to buildin...    30348596   \n",
       "\n",
       "          Category  \n",
       "0    Entertainment  \n",
       "1    Entertainment  \n",
       "2    Entertainment  \n",
       "3    Entertainment  \n",
       "4    Entertainment  \n",
       "..             ...  \n",
       "332  Howto & Style  \n",
       "333  Howto & Style  \n",
       "334  Howto & Style  \n",
       "335  Howto & Style  \n",
       "336  Howto & Style  \n",
       "\n",
       "[337 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = raw_data['title']\n",
    "type(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>items</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>view count</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Wdjh81uH6FU</td>\n",
       "      <td>[{'startMillis': 0, 'durationMillis': 9390, 'i...</td>\n",
       "      <td>$1 vs $10,000,000 Job!</td>\n",
       "      <td>I can’t believe they actually hired me lol\\nTr...</td>\n",
       "      <td>155154041</td>\n",
       "      <td>Entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>7dYTw-jAYkY</td>\n",
       "      <td>[{'startMillis': 0, 'durationMillis': 11200, '...</td>\n",
       "      <td>I Spent 7 Days Buried Alive</td>\n",
       "      <td>Please don't try this at home lol\\nVerizon 5G ...</td>\n",
       "      <td>152864305</td>\n",
       "      <td>Entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>mwKJfNYwvm8</td>\n",
       "      <td>[{'startMillis': 0, 'durationMillis': 6300, 'i...</td>\n",
       "      <td>I Built 100 Wells In Africa</td>\n",
       "      <td>Click the link below to donate \\nhttps://www.b...</td>\n",
       "      <td>165621129</td>\n",
       "      <td>Entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>QjvpjXdgugA</td>\n",
       "      <td>[{'startMillis': 0, 'durationMillis': 15590, '...</td>\n",
       "      <td>World’s Deadliest Laser Maze!</td>\n",
       "      <td>I can’t believe what happened at the end…\\nDri...</td>\n",
       "      <td>169254119</td>\n",
       "      <td>Entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3ryID_SwU5E</td>\n",
       "      <td>[{'startMillis': 0, 'durationMillis': 10560, '...</td>\n",
       "      <td>$1 vs $100,000,000 House!</td>\n",
       "      <td>I can’t believe how expensive the last house i...</td>\n",
       "      <td>200283545</td>\n",
       "      <td>Entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>332</td>\n",
       "      <td>JinpVA6p8Mo</td>\n",
       "      <td>[{'startMillis': 0, 'durationMillis': 3150, 'i...</td>\n",
       "      <td>Snowball Machine Gun- How to make</td>\n",
       "      <td>Here's how to make a Snowball Machine Gun that...</td>\n",
       "      <td>21356779</td>\n",
       "      <td>Howto &amp; Style</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>333</td>\n",
       "      <td>FRlbNOno5VA</td>\n",
       "      <td>[{'startMillis': 0, 'durationMillis': 7450, 'i...</td>\n",
       "      <td>BEST Guess Who Strategy- 96% WIN record using ...</td>\n",
       "      <td>Here is a strategy for winning 96% of your com...</td>\n",
       "      <td>28110693</td>\n",
       "      <td>Howto &amp; Style</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>334</td>\n",
       "      <td>nsnyl8llfH4</td>\n",
       "      <td>[{'startMillis': 0, 'durationMillis': 5890, 'i...</td>\n",
       "      <td>1st place Egg Drop project ideas- using SCIENCE</td>\n",
       "      <td>5 designs guaranteed to win 1st place or your ...</td>\n",
       "      <td>37505427</td>\n",
       "      <td>Howto &amp; Style</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>335</td>\n",
       "      <td>8Vc-69M-UWk</td>\n",
       "      <td>[{'startMillis': 0, 'durationMillis': 2260, 'i...</td>\n",
       "      <td>iPhone ATM PIN code hack- HOW TO PREVENT</td>\n",
       "      <td>This is a new way for bad guys to steal your A...</td>\n",
       "      <td>38803285</td>\n",
       "      <td>Howto &amp; Style</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>336</td>\n",
       "      <td>-RjJtO51ykY</td>\n",
       "      <td>[{'startMillis': 0, 'durationMillis': 9590, 'i...</td>\n",
       "      <td>EASY Pinewood Derby Car WINS using Science!!!</td>\n",
       "      <td>7 simple steps that ACTUALLY MATTER to buildin...</td>\n",
       "      <td>30348596</td>\n",
       "      <td>Howto &amp; Style</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>333 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0           id  \\\n",
       "0             1  Wdjh81uH6FU   \n",
       "1             2  7dYTw-jAYkY   \n",
       "2             3  mwKJfNYwvm8   \n",
       "3             4  QjvpjXdgugA   \n",
       "4             5  3ryID_SwU5E   \n",
       "..          ...          ...   \n",
       "328         332  JinpVA6p8Mo   \n",
       "329         333  FRlbNOno5VA   \n",
       "330         334  nsnyl8llfH4   \n",
       "331         335  8Vc-69M-UWk   \n",
       "332         336  -RjJtO51ykY   \n",
       "\n",
       "                                                 items  \\\n",
       "0    [{'startMillis': 0, 'durationMillis': 9390, 'i...   \n",
       "1    [{'startMillis': 0, 'durationMillis': 11200, '...   \n",
       "2    [{'startMillis': 0, 'durationMillis': 6300, 'i...   \n",
       "3    [{'startMillis': 0, 'durationMillis': 15590, '...   \n",
       "4    [{'startMillis': 0, 'durationMillis': 10560, '...   \n",
       "..                                                 ...   \n",
       "328  [{'startMillis': 0, 'durationMillis': 3150, 'i...   \n",
       "329  [{'startMillis': 0, 'durationMillis': 7450, 'i...   \n",
       "330  [{'startMillis': 0, 'durationMillis': 5890, 'i...   \n",
       "331  [{'startMillis': 0, 'durationMillis': 2260, 'i...   \n",
       "332  [{'startMillis': 0, 'durationMillis': 9590, 'i...   \n",
       "\n",
       "                                                 title  \\\n",
       "0                               $1 vs $10,000,000 Job!   \n",
       "1                          I Spent 7 Days Buried Alive   \n",
       "2                          I Built 100 Wells In Africa   \n",
       "3                        World’s Deadliest Laser Maze!   \n",
       "4                            $1 vs $100,000,000 House!   \n",
       "..                                                 ...   \n",
       "328                  Snowball Machine Gun- How to make   \n",
       "329  BEST Guess Who Strategy- 96% WIN record using ...   \n",
       "330    1st place Egg Drop project ideas- using SCIENCE   \n",
       "331           iPhone ATM PIN code hack- HOW TO PREVENT   \n",
       "332      EASY Pinewood Derby Car WINS using Science!!!   \n",
       "\n",
       "                                           description  view count  \\\n",
       "0    I can’t believe they actually hired me lol\\nTr...   155154041   \n",
       "1    Please don't try this at home lol\\nVerizon 5G ...   152864305   \n",
       "2    Click the link below to donate \\nhttps://www.b...   165621129   \n",
       "3    I can’t believe what happened at the end…\\nDri...   169254119   \n",
       "4    I can’t believe how expensive the last house i...   200283545   \n",
       "..                                                 ...         ...   \n",
       "328  Here's how to make a Snowball Machine Gun that...    21356779   \n",
       "329  Here is a strategy for winning 96% of your com...    28110693   \n",
       "330  5 designs guaranteed to win 1st place or your ...    37505427   \n",
       "331  This is a new way for bad guys to steal your A...    38803285   \n",
       "332  7 simple steps that ACTUALLY MATTER to buildin...    30348596   \n",
       "\n",
       "          Category  \n",
       "0    Entertainment  \n",
       "1    Entertainment  \n",
       "2    Entertainment  \n",
       "3    Entertainment  \n",
       "4    Entertainment  \n",
       "..             ...  \n",
       "328  Howto & Style  \n",
       "329  Howto & Style  \n",
       "330  Howto & Style  \n",
       "331  Howto & Style  \n",
       "332  Howto & Style  \n",
       "\n",
       "[333 rows x 7 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of video IDs to remove\n",
    "vid_ids = frames_features.keys()\n",
    "\n",
    "# Filter the DataFrame to exclude rows with the specified video IDs\n",
    "raw_data = raw_data[raw_data['id'].isin(vid_ids)]\n",
    "raw_data = raw_data.reset_index(drop=True)\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TOcGSwJBPMQ'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames_key_list = list(frames_features.keys())\n",
    "frames_key_list[255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>length_title</th>\n",
       "      <th>view count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wdjh81uH6FU</td>\n",
       "      <td>$1 vs $10,000,000 Job!</td>\n",
       "      <td>4</td>\n",
       "      <td>155681496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7dYTw-jAYkY</td>\n",
       "      <td>I Spent 7 Days Buried Alive</td>\n",
       "      <td>6</td>\n",
       "      <td>153336529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mwKJfNYwvm8</td>\n",
       "      <td>I Built 100 Wells In Africa</td>\n",
       "      <td>6</td>\n",
       "      <td>165967222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>QjvpjXdgugA</td>\n",
       "      <td>World’s Deadliest Laser Maze!</td>\n",
       "      <td>4</td>\n",
       "      <td>169840468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3ryID_SwU5E</td>\n",
       "      <td>$1 vs $100,000,000 House!</td>\n",
       "      <td>4</td>\n",
       "      <td>200784748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>JinpVA6p8Mo</td>\n",
       "      <td>Snowball Machine Gun- How to make</td>\n",
       "      <td>6</td>\n",
       "      <td>21369170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>FRlbNOno5VA</td>\n",
       "      <td>BEST Guess Who Strategy- 96% WIN record using ...</td>\n",
       "      <td>9</td>\n",
       "      <td>28131975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>nsnyl8llfH4</td>\n",
       "      <td>1st place Egg Drop project ideas- using SCIENCE</td>\n",
       "      <td>8</td>\n",
       "      <td>37534623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>8Vc-69M-UWk</td>\n",
       "      <td>iPhone ATM PIN code hack- HOW TO PREVENT</td>\n",
       "      <td>8</td>\n",
       "      <td>38813844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>-RjJtO51ykY</td>\n",
       "      <td>EASY Pinewood Derby Car WINS using Science!!!</td>\n",
       "      <td>7</td>\n",
       "      <td>30351978</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>333 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                              title  \\\n",
       "0    Wdjh81uH6FU                             $1 vs $10,000,000 Job!   \n",
       "1    7dYTw-jAYkY                        I Spent 7 Days Buried Alive   \n",
       "2    mwKJfNYwvm8                        I Built 100 Wells In Africa   \n",
       "3    QjvpjXdgugA                      World’s Deadliest Laser Maze!   \n",
       "4    3ryID_SwU5E                          $1 vs $100,000,000 House!   \n",
       "..           ...                                                ...   \n",
       "328  JinpVA6p8Mo                  Snowball Machine Gun- How to make   \n",
       "329  FRlbNOno5VA  BEST Guess Who Strategy- 96% WIN record using ...   \n",
       "330  nsnyl8llfH4    1st place Egg Drop project ideas- using SCIENCE   \n",
       "331  8Vc-69M-UWk           iPhone ATM PIN code hack- HOW TO PREVENT   \n",
       "332  -RjJtO51ykY      EASY Pinewood Derby Car WINS using Science!!!   \n",
       "\n",
       "     length_title  view count  \n",
       "0               4   155681496  \n",
       "1               6   153336529  \n",
       "2               6   165967222  \n",
       "3               4   169840468  \n",
       "4               4   200784748  \n",
       "..            ...         ...  \n",
       "328             6    21369170  \n",
       "329             9    28131975  \n",
       "330             8    37534623  \n",
       "331             8    38813844  \n",
       "332             7    30351978  \n",
       "\n",
       "[333 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_title_view_count = length_title_view_count[length_title_view_count['id'].isin(vid_ids)]\n",
    "length_title_view_count = length_title_view_count.reset_index(drop=True)\n",
    "length_title_view_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\howel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                 $1 vs $10,000,000 Job!\n",
      "1                            I Spent 7 Days Buried Alive\n",
      "2                            I Built 100 Wells In Africa\n",
      "3                          World’s Deadliest Laser Maze!\n",
      "4                              $1 vs $100,000,000 House!\n",
      "                             ...                        \n",
      "328                    Snowball Machine Gun- How to make\n",
      "329    BEST Guess Who Strategy- 96% WIN record using ...\n",
      "330      1st place Egg Drop project ideas- using SCIENCE\n",
      "331             iPhone ATM PIN code hack- HOW TO PREVENT\n",
      "332        EASY Pinewood Derby Car WINS using Science!!!\n",
      "Name: title, Length: 333, dtype: object\n",
      "Sentence: $1 vs $10,000,000 Job!\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Spent 7 Days Buried Alive\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Built 100 Wells In Africa\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: World’s Deadliest Laser Maze!\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: $1 vs $100,000,000 House!\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: World's Most Dangerous Trap!\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: $1 vs $100,000,000 Car!\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Lamborghini Vs World's Largest Shredder\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Every Country On Earth Fights For $250,000!\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: $1 vs $250,000 Vacation!\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: 7 Days Stranded At Sea\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Train Vs Giant Pit\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: $1 vs $1,000,000,000 Yacht!\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Ages 1 - 100 Fight For $500,000\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: 1,000 Deaf People Hear For The First Time\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Paid A Real Assassin To Try To Kill Me\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: 1,000 Blind People See For The First Time\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Survived 50 Hours In Antarctica\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Hydraulic Press Vs Lamborghini\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: 100 Kids Vs 100 Adults For $500,000\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Last To Take Hand Off Jet, Keeps It!\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: $1 vs $1,000,000 Hotel Room!\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Survive 100 Days In Circle, Win $500,000\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Hunted 100 People!\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Survived A Plane Crash\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Gave My 100,000,000th Subscriber An Island\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: 100 Boys Vs 100 Girls For $500,000\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Didn’t Eat Food For 30 Days\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Built Willy Wonka's Chocolate Factory!\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: $10,000 Every Day You Survive Prison\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Would You Swim With Sharks For $100,000?\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: World’s Most Dangerous Escape Room!\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Extreme $1,000,000 Hide And Seek\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: $1,000,000 Influencer Tournament!\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Got Hunted By The Military\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: $456,000 Squid Game In Real Life!\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Cleaned The World’s Dirtiest Beach #TeamSeas\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Last To Take Hand Off Lamborghini, Keeps It\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: First To Rob Bank Wins $100,000\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Extreme $500,000 Game Of Tag!\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Got Hunted By The FBI\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: If You Can Carry $1,000,000 You Keep It!\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Would YOU Quit School For $100,000?\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Survived 50 Hours In A Maximum Security Prison\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Hit The Target, Win $300,000\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Anything You Can Fit In The Triangle I’ll Pay For\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Got Hunted By A Real Bounty Hunter\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Extreme $100,000 Game of Tag!\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Would You Sit In Snakes For $10,000?\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Bought The World's Largest Mystery Box! ($500,000)\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Spent 50 Hours Buried Alive\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Offering People $100,000 To Quit Their Job\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Sold My House For $1\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Donated $300,000 To People In Need\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Ate $100,000 Golden Ice Cream\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Bought Everything In 5 Stores\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Youtube Rewind 2020, Thank God It's Over\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Put 1,000,000,000 Christmas Lights On A House (World Record)\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Opened A Restaurant That Pays You To Eat At It\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Gave People $1,000,000 But ONLY 1 Minute To Spend It!\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Steal This $100,000 Diamond, You Keep It\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Press This Button To Win $100,000!\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Survived 24 Hours Straight In Ice\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Uber’d People And Let Them Keep The Car\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Filled My Brother’s House With Slime & Bought Him A New One\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Hi Me In 5 Years\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Would You Rather Have A Giant Diamond or $100,000?\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Gave My 40,000,000th Subscriber 40 Cars\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Why I Haven’t Been Uploading\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Get This Random Person 1,000,000 Subscribers\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Would YOU Rather Have A Lamborghini or This House?\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Bought A Private Island\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Spent $1,000,000 On Lottery Tickets and WON\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Ate The World’s Largest Slice Of Pizza\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Adopted EVERY Dog In A Dog Shelter\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Bought The World's Largest Firework ($600,000)\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Broke Into A House\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Would You Rather Have $100,000 OR This Mystery Key?\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Built The World's Largest Lego Tower\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Spent 50 Hours In Solitary Confinement\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: If You Click This Video I'll Give My Friend .001$\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Gave $200,000 To People Who Lost Their Jobs (Corona Virus)\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Spent 24 Hours In A Doomsday Bunker\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Giving $1,000,000 Of Food To People In Need\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: $70,000 Extreme Hide And Seek - Challenge\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Anything You Can Fit In The Circle I’ll Pay For\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Lamborghini Race, Winner Keeps Lamborghini\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Gave My Credit Card To Random People\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Went Back To Boy Scouts For A Day\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: $60,000 Extreme Hide And Seek - Challenge\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Giving 10,000 Presents To Kids For Christmas\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Spending $1,000,000 In 24 Hours\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Last To Take Hand Off $1,000,000 Keeps It\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Last To Stop Riding Bike Wins $1,000,000 (Part 4)\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Last To Leave Toilet Wins $1,000,000 (Part 3)\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Ordered Pizza And Tipped The House\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Last to Stop Swinging Wins $1,000,000 (Part 2)\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: $50,000 Game Of Extreme Hide And Seek - Challenge\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Opened A FREE BANK\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Going Around A Ferris Wheel 1,000 Times Straight\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Last To Fall Wins $1,000,000 (Part 1)\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Surviving 24 Hours Straight In The Bermuda Triangle\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Spent $100,000 On Lottery Tickets And Won!\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Planting 20,000,000 Trees, My Biggest Project Ever!\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Last To Take Hand Off Boat, Keeps It\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Searched 100 Dumpsters, Here's What I Found\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Going Through The Same Drive Thru 1,000 Times\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Donating $100,000 To Random Streamers\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Surviving 24 Hours Straight In A Rain Forest\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Last To Leave Ramen Noodle Pool Wins $20,000\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Anything You Can Carry, I'll Pay For #2\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Gave My Brother 24 Hours To Spend $100,000\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Last To Leave Pool Of $20,000 Keeps It\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Surviving 24 Hours On A Deserted Island\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Last To Leave Roller Coaster Wins $20,000 - Challenge\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Won Every Prize At A Theme Park\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Destroying My Friend's Car And Surprising Him With A New One\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Spent 24 Hours Straight At Area 51\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Can 50,000 Magnets Catch A Cannon Ball?\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: World's Largest Game Of Dodgeball\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Last To Leave Revolving Door Wins $50,000\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Tipping Waitresses With Real Gold Bars\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Opened The World's First FREE Store\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Spending 24 Hours On Top Of A Mountain\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Last To Leave VR Wins $20,000\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Last To Stop Running Wins $20,000\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Paying People $10,000 To Eat Ghost Pepper\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: 24 Hours In The Most Haunted Place On Earth\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Bought Everything In A Store - Challenge\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: World's Largest Bowl Of Cereal\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Spending 24 Hours In A City With No Laws\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Giving $10,000 To Random People And Saying Nothing\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Molten Lava Vs Giant Ice Block Experiment\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Anything You Can Carry, I'll Pay For Challenge\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Homeless Man Buys A Lamborghini\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Playing Battleship With Real Ships\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Donating $100,000 To Shroud In Real Life\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Last To Leave The Tesla, Keeps It Challenge\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Donating $100,000 To Streamers With 0 Viewers\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Surviving 24 Hours Straight In A Desert\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: $200,000 Youtuber Battle Royale\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Surprising Strangers With 100 Zombies - Experiment\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Gave A Homeless Man A Home\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Standing Still For 24 Hours Straight - Statue Challenge\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Giant Monopoly Game With Real Money\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Spent 24 Hours Straight In Insane Asylum\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Advertised Pewdiepie At The Super Bowl\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Went Back To 1st Grade For A Day\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Opened The World’s Cheapest Store\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Put 10 Million Legos in Friend's House\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Make This Video The Most Liked Video On Youtube\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: 20,000 Magnets Vs A Car\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Eating A $10,000 Golden Steak (24k Gold)\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Last To Take Hand Off House Keeps It!\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Gave Him One Hour To Spend $100,000!\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: How I Gave Away $1,000,000\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Giving $100,000 To A Homeless Person\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Spent 24 Hours Straight In Prison - Challenge\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Donating $10000 To Pewdiepie\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Last Youtuber To Leave Wins $100,000 - Challenge\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Put Millions Of Pennies In My Friends Backyard\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Surprising TFue With A Fortnite Battle Bus In Real Life\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Built A Giant House Using Only Legos\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Last To Leave Slime Pit Wins $20,000 - Challenge\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Destroyed Friend's House And Bought Him A New One\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Put 100 Million Orbeez In My Friend's Backyard\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Spending $40,000 In One Hour Challenge\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Giving My 10,000,000th Subscriber 10,000,000 ___\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: $10,000 Games Of Rock Paper Scissors\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Last To Remove Hand, Gets Lamborghini Challenge\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: You Pick The Right Cup, You Win $100,000!\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Bought Every Billboard In My City For This\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Ordering Water Then Tipping $30,000\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Spent $50,000 On Lottery Tickets And Won ____\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Last To Leave Circle Wins $10,000 - Challenge\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Spent 24 Hours Straight In Slime\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Surprising Tfue With $10,000 Live - Fortnite\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Spent $30,000 On Lottery Tickets And Won ____\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Flat Earth PROVEN By Independent Research\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Things That SHOULDN'T Be Sold On Amazon\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Surprising My 8,000,000th Subscriber With 8,000,000 ___\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Donated $25,000 To Random Kids Streaming Fortnite\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Hope This Magic Trick Works\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Can A Glowing 1000 Degree Sword Slice A Car In Half?\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Rocket League In Real Life!\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Destroying My Friend's Car And Surprising Him With A New One - Slime\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Spending 24 Hours Straight Under Water Challenge\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Attempting The Impossible Maze - $10,000 Challenge\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Donating $100,000 To A Random Fortnite Streamer\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Moving 10 Pounds Using ONLY Sound\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Gave My 6,000,000th Subscriber 6,000,000 ____\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Can 1,000 Fans At Max Speed Push A Car?\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Ubered Random People In A Tank\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: World's Hottest Substance Vs Coldest Substance\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: The Real Way Dinosaurs Went Extinct\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Donated $1000 Every Time She Blinked - Fortnite Streamers\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Built A Working Car Using Only LEGOS\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Do Water Repellent Shoes Actually Work?\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: We Drove 3,000 Miles For The World's Best Burger\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Gave My 5,000,000th Subscriber 5,000,000 ______\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Made Money Grow On Trees\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Took Apart My Friends Car And Put It In His Room\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: How Many Rubber Bands Does It Take To Snap A Safe?\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Pulling Cars Over Using A Toy Police Car\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Gave $500,000 To Random People\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: How Many Toy Cars Does It Take To Pull A Real Car?\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Gave $20,000 To People From An Ice Cream Truck\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: $10000 Ice Sculpture Vs Flame\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Tying My Shoes In Every State (World Record)\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Bad Uber Driver Prank!\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Tried The Most Dangerous Alarm Clocks\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Stole America’s Got Talent!\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Brought 50 Competitive Eaters To A Buffet!\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Fake Tour Guide Prank!\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Survived 24 Hours In Australia\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: 24 Hours In The Most Dangerous Underground City\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Tested The World’s Most Expensive Hotel\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Survived The World's Largest Water Park\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Bought The Most Expensive Plane Ticket!\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Survived Overnight In Shrek’s Swamp\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Crossed Italy In A Perfectly Straight Line - Day 6\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Crossed Switzerland In A Perfectly Straight Line - Day 5\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Crossed Switzerland In A Perfectly Straight Line - Day 4\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Crossed France In A Perfectly Straight Line - Day 3\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Crossed The UK In A Perfectly Straight Line - Day 2\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Crossed Ireland In A Perfectly Straight Line - Day 1\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Have To Delete My Channel.\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Tried Banned Inventions\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Try Not To Scream: IRL\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Tried Every IMPOSSIBLE Sport\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Sneaking Into Celebrity Pools\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Tried The World’s Fastest Vehicles\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Hired 100 Fake Paparazzi\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Tried Every Drive Thru\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Invisible Driver Prank\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Extreme Dares In Public!\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: 100 Dates In 24 Hours!\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I ACTUALLY Got MrBeast Arrested\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Escaped Prisoner Prank\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Ai Girl Speed Dates 10 YouTubers\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Human Hamster Wheel Vs Tesla: Can I Power It?\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Walking On Water Prank\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Will An Ex Convict Return $20,000 Rolex?\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Trapped 25 Celebrities In A Box\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Made The World's Largest Pizza (132 Feet)\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Exposed Strangers Lying In Public\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Truth Or Dare: IRL\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Would You Rather: IRL\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Capture The Flag: IRL\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: 1 Nerd VS 100 Supermodels\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Hit 10,000,000 Subscribers\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Caught Strangers Lying In Public\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Trapped 100 Strangers In Complete Darkness\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Try Not To Laugh: IRL\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Never Have I Ever: IRL\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Built A REAL Primitive Survival Mansion\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: What Happens If You Hire 100 Bodyguards?\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Could You Survive On Mars?\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Trapped 25 Strangers In A Box\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Escape or Die: IRL\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Ordered 10,000 Amazon Packages!\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Humans VS Robots: 100 Experiments\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: If You Scream, You Lose: IRL\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: 24 Hours To Live: Challenge\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: 100 Dates In 24 Hours!\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: AGE 12 TO MARRIED – I Took A Photo Every Day\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: CECI EST UNE FOIRE AUX QUESTIONS #2 (FAQ Squeezie)\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: If Two Girls Falling Asleep on You at the Same Time | Prank 当两个姑娘同时靠在自己肩膀上睡着了，路人小哥坐过了站\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Car Thief Gets Instant Karma (the FINAL Glitterbomb 6.0)\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Octopus vs Underwater Maze\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Acid vs Lava- Testing Liquids That Melt Everything\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Candy Thieves vs Rigged Candy Bowl\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: World’s Smallest Nerf Gun Shoots an Ant\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: How to Escape a Police Sniffing Dog\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: I Gave the 2023 MIT Commencement Speech\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: This Ball is Impossible to Hit\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Amazing Invention- This Drone Will Change Everything\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Bed Bugs- What You've Been Told is Totally False\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Car Thieves vs the Almost Final GlitterBomb 5.0\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Backyard Squirrelympics 3.0- The Summer Games\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Beating 5 Scam Arcade Games with Science\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: My Secret Warehouse Tour\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Pranks Destroy Scam Callers- GlitterBomb Payback\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Robot Piano Catches Fire Playing Rush E (World’s Hardest Song)\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: World's Largest T-Shirt Cannon (breaks the roof)\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: EXPLODING Glitter Bomb 4.0 vs. Package Thieves\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: World's Tallest Elephant Toothpaste Volcano\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: World Record Domino Robot (100k dominoes in 24hrs)\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Shark vs. GoPro\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Backyard Squirrel Maze 2.0- The Walnut Heist\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Glitterbomb Trap Catches Phone Scammer (who gets arrested)\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Glitterbomb 3.0 vs. Porch Pirates\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Shark Attack Test- Human Blood vs. Fish Blood\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Backyard Squirrel Maze 1.0- Ninja Warrior Course\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: How To See Germs Spread Experiment (Coronavirus)\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: CAR vs. WORLD’S STRONGEST TRAMPOLINE- 150ft (45m) drop\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Feeding Bill Gates a Fake Burger (to save the world)\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Glitter Bomb 2.0 vs Porch Pirates\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: World's First Automatic Strike Bowling Ball\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: World's Largest Elephant Toothpaste Experiment\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Testing if Sharks Can Smell a Drop of Blood\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Stealing Baseball Signs with a Phone (Machine Learning)\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Rocket Powered Golf Club at 100,000 FPS\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Drinking Nasty Swamp Water (to save the world)\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Glitter Bomb 1.0 vs Porch Pirates\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: World's Largest Horn Shatters Glass\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Playing Card Machine Gun- Card Throwing Trick Shots\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Flying Phone Scam Exposed (so I built a REAL one)\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Beat Any Escape Room- 10 proven tricks and tips\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Rock Skip Robot- The Science of Perfect Rock Skipping\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: 1st place Mousetrap Car Ideas\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: This Arcade Game is a SCAM (I have proof)\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Liquid Sand Hot Tub- Fluidized air bed\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Measuring How Much Pee Is In Your Pool\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: World's Largest Super Soaker\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: NICEST Car Horn Ever- DIY\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Hot Wheels STUNT RACE- Slow Mo (2500 FPS)\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Automatic Bullseye, MOVING Dartboard\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: How to save 51 billion lives for 68 cents with simple Engineering\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Myth-testing Christmas movies with Science Experiments\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: How to CHEER THE LOUDEST using SCIENCE!\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: PAINT Super Soaker Battle (w/ Colin Furze)- Splatoon IRL\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: GLOWING WALL DIY- EASY and AWESOME\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Bare Hand Bottle Busting- Science Investigation\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: 25 Million Orbeez in a pool- Do you sink or float?\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: World's Largest Nerf Gun\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: How to Survive a Grenade Blast\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Solar System Model From a Drone's View\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Snowball Machine Gun- How to make\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: BEST Guess Who Strategy- 96% WIN record using MATH\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: 1st place Egg Drop project ideas- using SCIENCE\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: iPhone ATM PIN code hack- HOW TO PREVENT\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: EASY Pinewood Derby Car WINS using Science!!!\n",
      "Embedding: (384,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "# import numpy as np\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Our sentences we like to encode\n",
    "sentences = raw_data['title']\n",
    "print(sentences)\n",
    "\n",
    "# Sentences are encoded by calling model.encode()\n",
    "vocab_embeds = model.encode(sentences)\n",
    "\n",
    "# Print the embeddings\n",
    "for sentence, embedding in zip(sentences, vocab_embeds):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding.shape)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999755521558455"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X = np.column_stack((length_title_view_count['length_title'], vocab_embeds))\n",
    "y = np.array(length_title_view_count['view count'])\n",
    "reg = LinearRegression().fit(X, y)\n",
    "reg.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x18340e67250>,\n",
       " <matplotlib.lines.Line2D at 0x18340e67340>,\n",
       " <matplotlib.lines.Line2D at 0x18340e67430>,\n",
       " <matplotlib.lines.Line2D at 0x18340e67520>,\n",
       " <matplotlib.lines.Line2D at 0x18340e67610>,\n",
       " <matplotlib.lines.Line2D at 0x18340e67700>,\n",
       " <matplotlib.lines.Line2D at 0x18340e677f0>,\n",
       " <matplotlib.lines.Line2D at 0x18340e678e0>,\n",
       " <matplotlib.lines.Line2D at 0x18340e679d0>,\n",
       " <matplotlib.lines.Line2D at 0x18340e67ac0>,\n",
       " <matplotlib.lines.Line2D at 0x18340e67bb0>,\n",
       " <matplotlib.lines.Line2D at 0x18340e67c70>,\n",
       " <matplotlib.lines.Line2D at 0x18340e67d60>,\n",
       " <matplotlib.lines.Line2D at 0x18340e67e50>,\n",
       " <matplotlib.lines.Line2D at 0x18340e67f40>,\n",
       " <matplotlib.lines.Line2D at 0x18340ea8070>,\n",
       " <matplotlib.lines.Line2D at 0x18340ea8160>,\n",
       " <matplotlib.lines.Line2D at 0x18340ea8250>,\n",
       " <matplotlib.lines.Line2D at 0x18340ea8340>,\n",
       " <matplotlib.lines.Line2D at 0x18340ea8430>,\n",
       " <matplotlib.lines.Line2D at 0x18340ea8520>,\n",
       " <matplotlib.lines.Line2D at 0x18340ea8610>,\n",
       " <matplotlib.lines.Line2D at 0x18340ea8700>,\n",
       " <matplotlib.lines.Line2D at 0x18340ea87f0>,\n",
       " <matplotlib.lines.Line2D at 0x18340ea88e0>,\n",
       " <matplotlib.lines.Line2D at 0x18340ea89d0>,\n",
       " <matplotlib.lines.Line2D at 0x18340ea8ac0>,\n",
       " <matplotlib.lines.Line2D at 0x18340ea8bb0>,\n",
       " <matplotlib.lines.Line2D at 0x18340ea8ca0>,\n",
       " <matplotlib.lines.Line2D at 0x18340ea8d90>,\n",
       " <matplotlib.lines.Line2D at 0x18340ea8e80>,\n",
       " <matplotlib.lines.Line2D at 0x18340ea8f70>,\n",
       " <matplotlib.lines.Line2D at 0x18340ea9060>,\n",
       " <matplotlib.lines.Line2D at 0x18340ea9150>,\n",
       " <matplotlib.lines.Line2D at 0x18340ea9240>,\n",
       " <matplotlib.lines.Line2D at 0x18340ea9330>,\n",
       " <matplotlib.lines.Line2D at 0x18340ea9420>,\n",
       " <matplotlib.lines.Line2D at 0x18340ea9510>,\n",
       " <matplotlib.lines.Line2D at 0x18340ea9600>,\n",
       " <matplotlib.lines.Line2D at 0x18340ea96f0>,\n",
       " <matplotlib.lines.Line2D at 0x18340ea97e0>,\n",
       " <matplotlib.lines.Line2D at 0x18340ea98d0>,\n",
       " <matplotlib.lines.Line2D at 0x18340ea99c0>,\n",
       " <matplotlib.lines.Line2D at 0x18340ea9ab0>,\n",
       " <matplotlib.lines.Line2D at 0x18340ea9ba0>,\n",
       " <matplotlib.lines.Line2D at 0x18340ea9c90>,\n",
       " <matplotlib.lines.Line2D at 0x18340ea9d80>,\n",
       " <matplotlib.lines.Line2D at 0x18340ea9e70>,\n",
       " <matplotlib.lines.Line2D at 0x18340ea9f60>,\n",
       " <matplotlib.lines.Line2D at 0x18340eaa050>,\n",
       " <matplotlib.lines.Line2D at 0x18340eaa140>,\n",
       " <matplotlib.lines.Line2D at 0x18340eaa230>,\n",
       " <matplotlib.lines.Line2D at 0x18340eaa320>,\n",
       " <matplotlib.lines.Line2D at 0x18340eaa410>,\n",
       " <matplotlib.lines.Line2D at 0x18340eaa500>,\n",
       " <matplotlib.lines.Line2D at 0x18340eaa5f0>,\n",
       " <matplotlib.lines.Line2D at 0x18340eaa6e0>,\n",
       " <matplotlib.lines.Line2D at 0x18340eaa7d0>,\n",
       " <matplotlib.lines.Line2D at 0x18340eaa8c0>,\n",
       " <matplotlib.lines.Line2D at 0x18340eaa9b0>,\n",
       " <matplotlib.lines.Line2D at 0x18340eaaaa0>,\n",
       " <matplotlib.lines.Line2D at 0x18340eaab90>,\n",
       " <matplotlib.lines.Line2D at 0x18340eaac80>,\n",
       " <matplotlib.lines.Line2D at 0x18340eaad70>,\n",
       " <matplotlib.lines.Line2D at 0x18340eaae60>,\n",
       " <matplotlib.lines.Line2D at 0x18340eaaf50>,\n",
       " <matplotlib.lines.Line2D at 0x18340eab040>,\n",
       " <matplotlib.lines.Line2D at 0x18340eab130>,\n",
       " <matplotlib.lines.Line2D at 0x18340eab220>,\n",
       " <matplotlib.lines.Line2D at 0x18340eab310>,\n",
       " <matplotlib.lines.Line2D at 0x18340eab400>,\n",
       " <matplotlib.lines.Line2D at 0x18340eab4f0>,\n",
       " <matplotlib.lines.Line2D at 0x18340eab5e0>,\n",
       " <matplotlib.lines.Line2D at 0x18340eab6d0>,\n",
       " <matplotlib.lines.Line2D at 0x18340eab7c0>,\n",
       " <matplotlib.lines.Line2D at 0x18340eab8b0>,\n",
       " <matplotlib.lines.Line2D at 0x18340eab9a0>,\n",
       " <matplotlib.lines.Line2D at 0x18340eaba90>,\n",
       " <matplotlib.lines.Line2D at 0x18340eabb80>,\n",
       " <matplotlib.lines.Line2D at 0x18340eabc70>,\n",
       " <matplotlib.lines.Line2D at 0x18340eabd60>,\n",
       " <matplotlib.lines.Line2D at 0x18340eabe50>,\n",
       " <matplotlib.lines.Line2D at 0x18340eabf40>,\n",
       " <matplotlib.lines.Line2D at 0x18340ecc070>,\n",
       " <matplotlib.lines.Line2D at 0x18340ecc160>,\n",
       " <matplotlib.lines.Line2D at 0x18340ecc250>,\n",
       " <matplotlib.lines.Line2D at 0x18340ecc340>,\n",
       " <matplotlib.lines.Line2D at 0x18340ecc430>,\n",
       " <matplotlib.lines.Line2D at 0x18340ecc520>,\n",
       " <matplotlib.lines.Line2D at 0x18340ecc610>,\n",
       " <matplotlib.lines.Line2D at 0x18340ecc700>,\n",
       " <matplotlib.lines.Line2D at 0x18340ecc7f0>,\n",
       " <matplotlib.lines.Line2D at 0x18340ecc8e0>,\n",
       " <matplotlib.lines.Line2D at 0x18340ecc9d0>,\n",
       " <matplotlib.lines.Line2D at 0x18340eccac0>,\n",
       " <matplotlib.lines.Line2D at 0x18340eccbb0>,\n",
       " <matplotlib.lines.Line2D at 0x18340eccca0>,\n",
       " <matplotlib.lines.Line2D at 0x18340eccd90>,\n",
       " <matplotlib.lines.Line2D at 0x18340ecce80>,\n",
       " <matplotlib.lines.Line2D at 0x18340eccf70>,\n",
       " <matplotlib.lines.Line2D at 0x18340ecd060>,\n",
       " <matplotlib.lines.Line2D at 0x18340ecd150>,\n",
       " <matplotlib.lines.Line2D at 0x18340ecd240>,\n",
       " <matplotlib.lines.Line2D at 0x18340ecd330>,\n",
       " <matplotlib.lines.Line2D at 0x18340ecd420>,\n",
       " <matplotlib.lines.Line2D at 0x18340ecd510>,\n",
       " <matplotlib.lines.Line2D at 0x18340ecd600>,\n",
       " <matplotlib.lines.Line2D at 0x18340ecd6f0>,\n",
       " <matplotlib.lines.Line2D at 0x18340e66350>,\n",
       " <matplotlib.lines.Line2D at 0x18340e66260>,\n",
       " <matplotlib.lines.Line2D at 0x18340dff670>,\n",
       " <matplotlib.lines.Line2D at 0x18340ecd870>,\n",
       " <matplotlib.lines.Line2D at 0x18340ecd960>,\n",
       " <matplotlib.lines.Line2D at 0x18340ecda50>,\n",
       " <matplotlib.lines.Line2D at 0x18340ecdb40>,\n",
       " <matplotlib.lines.Line2D at 0x18340ecdc30>,\n",
       " <matplotlib.lines.Line2D at 0x18340ecdd20>,\n",
       " <matplotlib.lines.Line2D at 0x18340ecde10>,\n",
       " <matplotlib.lines.Line2D at 0x18340ecdf00>,\n",
       " <matplotlib.lines.Line2D at 0x18340ecdff0>,\n",
       " <matplotlib.lines.Line2D at 0x18340ece0e0>,\n",
       " <matplotlib.lines.Line2D at 0x18340ece1d0>,\n",
       " <matplotlib.lines.Line2D at 0x18340ece2c0>,\n",
       " <matplotlib.lines.Line2D at 0x18340ece3b0>,\n",
       " <matplotlib.lines.Line2D at 0x18340ece4a0>,\n",
       " <matplotlib.lines.Line2D at 0x18340ece590>,\n",
       " <matplotlib.lines.Line2D at 0x18340ece680>,\n",
       " <matplotlib.lines.Line2D at 0x18340ece770>,\n",
       " <matplotlib.lines.Line2D at 0x18340ece860>,\n",
       " <matplotlib.lines.Line2D at 0x18340ece950>,\n",
       " <matplotlib.lines.Line2D at 0x18340ecea40>,\n",
       " <matplotlib.lines.Line2D at 0x18340eceb30>,\n",
       " <matplotlib.lines.Line2D at 0x18340ecec20>,\n",
       " <matplotlib.lines.Line2D at 0x18340eced10>,\n",
       " <matplotlib.lines.Line2D at 0x18340ecee00>,\n",
       " <matplotlib.lines.Line2D at 0x18340eceef0>,\n",
       " <matplotlib.lines.Line2D at 0x18340ecefe0>,\n",
       " <matplotlib.lines.Line2D at 0x18340ecf0d0>,\n",
       " <matplotlib.lines.Line2D at 0x18340ecf1c0>,\n",
       " <matplotlib.lines.Line2D at 0x18340ecf2b0>,\n",
       " <matplotlib.lines.Line2D at 0x18340ecf3a0>,\n",
       " <matplotlib.lines.Line2D at 0x18340ecf490>,\n",
       " <matplotlib.lines.Line2D at 0x18340ecf580>,\n",
       " <matplotlib.lines.Line2D at 0x18340ecf670>,\n",
       " <matplotlib.lines.Line2D at 0x18340ecf760>,\n",
       " <matplotlib.lines.Line2D at 0x18340ecf850>,\n",
       " <matplotlib.lines.Line2D at 0x18340ecf940>,\n",
       " <matplotlib.lines.Line2D at 0x18340ecfa30>,\n",
       " <matplotlib.lines.Line2D at 0x18340ecfb20>,\n",
       " <matplotlib.lines.Line2D at 0x18340ecfc10>,\n",
       " <matplotlib.lines.Line2D at 0x18340ecfd00>,\n",
       " <matplotlib.lines.Line2D at 0x18340ecfdf0>,\n",
       " <matplotlib.lines.Line2D at 0x18340ecfee0>,\n",
       " <matplotlib.lines.Line2D at 0x18340ecffd0>,\n",
       " <matplotlib.lines.Line2D at 0x18340ee8100>,\n",
       " <matplotlib.lines.Line2D at 0x18340ee81f0>,\n",
       " <matplotlib.lines.Line2D at 0x18340ee82e0>,\n",
       " <matplotlib.lines.Line2D at 0x18340ee83d0>,\n",
       " <matplotlib.lines.Line2D at 0x18340ee84c0>,\n",
       " <matplotlib.lines.Line2D at 0x18340ee85b0>,\n",
       " <matplotlib.lines.Line2D at 0x18340ee86a0>,\n",
       " <matplotlib.lines.Line2D at 0x18340ee8790>,\n",
       " <matplotlib.lines.Line2D at 0x18340ee8880>,\n",
       " <matplotlib.lines.Line2D at 0x18340ee8970>,\n",
       " <matplotlib.lines.Line2D at 0x18340ee8a60>,\n",
       " <matplotlib.lines.Line2D at 0x18340ee8b50>,\n",
       " <matplotlib.lines.Line2D at 0x18340ee8c40>,\n",
       " <matplotlib.lines.Line2D at 0x18340ee8d30>,\n",
       " <matplotlib.lines.Line2D at 0x18340ee8e20>,\n",
       " <matplotlib.lines.Line2D at 0x18340ee8f10>,\n",
       " <matplotlib.lines.Line2D at 0x18340ee9000>,\n",
       " <matplotlib.lines.Line2D at 0x18340ee90f0>,\n",
       " <matplotlib.lines.Line2D at 0x18340ee91e0>,\n",
       " <matplotlib.lines.Line2D at 0x18340ee92d0>,\n",
       " <matplotlib.lines.Line2D at 0x18340ee93c0>,\n",
       " <matplotlib.lines.Line2D at 0x18340ee94b0>,\n",
       " <matplotlib.lines.Line2D at 0x18340ee95a0>,\n",
       " <matplotlib.lines.Line2D at 0x18340ee9690>,\n",
       " <matplotlib.lines.Line2D at 0x18340ee9780>,\n",
       " <matplotlib.lines.Line2D at 0x18340ee9870>,\n",
       " <matplotlib.lines.Line2D at 0x18340ee9960>,\n",
       " <matplotlib.lines.Line2D at 0x18340ee9a50>,\n",
       " <matplotlib.lines.Line2D at 0x18340ee9b40>,\n",
       " <matplotlib.lines.Line2D at 0x18340ee9c30>,\n",
       " <matplotlib.lines.Line2D at 0x18340ee9d20>,\n",
       " <matplotlib.lines.Line2D at 0x18340ee9e10>,\n",
       " <matplotlib.lines.Line2D at 0x18340ee9f00>,\n",
       " <matplotlib.lines.Line2D at 0x18340ee9ff0>,\n",
       " <matplotlib.lines.Line2D at 0x18340eea0e0>,\n",
       " <matplotlib.lines.Line2D at 0x18340eea1d0>,\n",
       " <matplotlib.lines.Line2D at 0x18340eea2c0>,\n",
       " <matplotlib.lines.Line2D at 0x18340eea3b0>,\n",
       " <matplotlib.lines.Line2D at 0x18340eea4a0>,\n",
       " <matplotlib.lines.Line2D at 0x18340eea590>,\n",
       " <matplotlib.lines.Line2D at 0x18340eea680>,\n",
       " <matplotlib.lines.Line2D at 0x18340eea770>,\n",
       " <matplotlib.lines.Line2D at 0x18340eea860>,\n",
       " <matplotlib.lines.Line2D at 0x18340eea950>,\n",
       " <matplotlib.lines.Line2D at 0x18340eeaa40>,\n",
       " <matplotlib.lines.Line2D at 0x18340eeab30>,\n",
       " <matplotlib.lines.Line2D at 0x18340eeac20>,\n",
       " <matplotlib.lines.Line2D at 0x18340eead10>,\n",
       " <matplotlib.lines.Line2D at 0x18340eeae00>,\n",
       " <matplotlib.lines.Line2D at 0x18340eeaef0>,\n",
       " <matplotlib.lines.Line2D at 0x18340eeafe0>,\n",
       " <matplotlib.lines.Line2D at 0x18340eeb0d0>,\n",
       " <matplotlib.lines.Line2D at 0x18340eeb1c0>,\n",
       " <matplotlib.lines.Line2D at 0x18340eeb2b0>,\n",
       " <matplotlib.lines.Line2D at 0x18340eeb3a0>,\n",
       " <matplotlib.lines.Line2D at 0x18340eeb490>,\n",
       " <matplotlib.lines.Line2D at 0x18340eeb580>,\n",
       " <matplotlib.lines.Line2D at 0x18340eeb670>,\n",
       " <matplotlib.lines.Line2D at 0x18340eeb760>,\n",
       " <matplotlib.lines.Line2D at 0x18340eeb850>,\n",
       " <matplotlib.lines.Line2D at 0x18340eeb940>,\n",
       " <matplotlib.lines.Line2D at 0x18340eeba30>,\n",
       " <matplotlib.lines.Line2D at 0x18340eebb20>,\n",
       " <matplotlib.lines.Line2D at 0x18340eebc10>,\n",
       " <matplotlib.lines.Line2D at 0x18340eebd00>,\n",
       " <matplotlib.lines.Line2D at 0x18340eebdf0>,\n",
       " <matplotlib.lines.Line2D at 0x18340eebee0>,\n",
       " <matplotlib.lines.Line2D at 0x18340eebfd0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f08100>,\n",
       " <matplotlib.lines.Line2D at 0x18340f081f0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f082e0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f083d0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f084c0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f085b0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f086a0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f08790>,\n",
       " <matplotlib.lines.Line2D at 0x18340f08880>,\n",
       " <matplotlib.lines.Line2D at 0x18340f08970>,\n",
       " <matplotlib.lines.Line2D at 0x18340f08a60>,\n",
       " <matplotlib.lines.Line2D at 0x18340f08b50>,\n",
       " <matplotlib.lines.Line2D at 0x18340f08c40>,\n",
       " <matplotlib.lines.Line2D at 0x18340f08d30>,\n",
       " <matplotlib.lines.Line2D at 0x18340f08e20>,\n",
       " <matplotlib.lines.Line2D at 0x18340f08f10>,\n",
       " <matplotlib.lines.Line2D at 0x18340f09000>,\n",
       " <matplotlib.lines.Line2D at 0x18340f090f0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f091e0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f092d0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f093c0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f094b0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f095a0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f09690>,\n",
       " <matplotlib.lines.Line2D at 0x18340f09780>,\n",
       " <matplotlib.lines.Line2D at 0x18340f09870>,\n",
       " <matplotlib.lines.Line2D at 0x18340f09960>,\n",
       " <matplotlib.lines.Line2D at 0x18340f09a50>,\n",
       " <matplotlib.lines.Line2D at 0x18340f09b40>,\n",
       " <matplotlib.lines.Line2D at 0x18340f09c30>,\n",
       " <matplotlib.lines.Line2D at 0x18340f09d20>,\n",
       " <matplotlib.lines.Line2D at 0x18340f09e10>,\n",
       " <matplotlib.lines.Line2D at 0x18340f09f00>,\n",
       " <matplotlib.lines.Line2D at 0x18340f09ff0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f0a0e0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f0a1d0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f0a2c0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f0a3b0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f0a4a0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f0a590>,\n",
       " <matplotlib.lines.Line2D at 0x18340f0a680>,\n",
       " <matplotlib.lines.Line2D at 0x18340f0a770>,\n",
       " <matplotlib.lines.Line2D at 0x18340f0a860>,\n",
       " <matplotlib.lines.Line2D at 0x18340f0a950>,\n",
       " <matplotlib.lines.Line2D at 0x18340f0aa40>,\n",
       " <matplotlib.lines.Line2D at 0x18340f0ab30>,\n",
       " <matplotlib.lines.Line2D at 0x18340f0ac20>,\n",
       " <matplotlib.lines.Line2D at 0x18340f0ad10>,\n",
       " <matplotlib.lines.Line2D at 0x18340f0ae00>,\n",
       " <matplotlib.lines.Line2D at 0x18340f0aef0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f0afe0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f0b0d0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f0b1c0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f0b2b0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f0b3a0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f0b490>,\n",
       " <matplotlib.lines.Line2D at 0x18340f0b580>,\n",
       " <matplotlib.lines.Line2D at 0x18340f0b670>,\n",
       " <matplotlib.lines.Line2D at 0x18340f0b760>,\n",
       " <matplotlib.lines.Line2D at 0x18340f0b850>,\n",
       " <matplotlib.lines.Line2D at 0x18340f0b940>,\n",
       " <matplotlib.lines.Line2D at 0x18340f0ba30>,\n",
       " <matplotlib.lines.Line2D at 0x18340f0bb20>,\n",
       " <matplotlib.lines.Line2D at 0x18340f0bc10>,\n",
       " <matplotlib.lines.Line2D at 0x18340f0bd00>,\n",
       " <matplotlib.lines.Line2D at 0x18340f0bdf0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f0bee0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f0bfd0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2c100>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2c1f0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2c2e0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2c3d0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2c4c0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2c5b0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2c6a0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2c790>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2c880>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2c970>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2ca60>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2cb50>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2cc40>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2cd30>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2ce20>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2cf10>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2d000>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2d0f0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2d1e0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2d2d0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2d3c0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2d4b0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2d5a0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2d690>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2d780>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2d870>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2d960>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2da50>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2db40>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2dc30>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2dd20>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2de10>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2df00>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2dff0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2e0e0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2e1d0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2e2c0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2e3b0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2e4d0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2e5c0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2e6b0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2e7a0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2e890>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2e980>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2ea70>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2eb60>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2ec50>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2ed40>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2ee30>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2ef20>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2f010>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2f100>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2f1f0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2f2e0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2f3d0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2f4c0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2f5b0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2f6a0>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2f790>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2f880>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2f970>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2fa60>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2fb50>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2fc40>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2fd30>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2fe20>,\n",
       " <matplotlib.lines.Line2D at 0x18340f2ff10>,\n",
       " <matplotlib.lines.Line2D at 0x18341f54040>,\n",
       " <matplotlib.lines.Line2D at 0x18341f54130>,\n",
       " <matplotlib.lines.Line2D at 0x18341f54220>,\n",
       " <matplotlib.lines.Line2D at 0x18341f54310>,\n",
       " <matplotlib.lines.Line2D at 0x18341f54400>,\n",
       " <matplotlib.lines.Line2D at 0x18341f544f0>,\n",
       " <matplotlib.lines.Line2D at 0x18341f545e0>,\n",
       " <matplotlib.lines.Line2D at 0x18341f546d0>,\n",
       " <matplotlib.lines.Line2D at 0x18341f547c0>,\n",
       " <matplotlib.lines.Line2D at 0x18341f548b0>,\n",
       " <matplotlib.lines.Line2D at 0x18341f549a0>,\n",
       " <matplotlib.lines.Line2D at 0x18341f54a90>,\n",
       " <matplotlib.lines.Line2D at 0x18341f54b80>,\n",
       " <matplotlib.lines.Line2D at 0x18341f54c70>,\n",
       " <matplotlib.lines.Line2D at 0x18341f54d60>,\n",
       " <matplotlib.lines.Line2D at 0x18341f54e50>,\n",
       " <matplotlib.lines.Line2D at 0x18341f54f40>,\n",
       " <matplotlib.lines.Line2D at 0x18341f55030>,\n",
       " <matplotlib.lines.Line2D at 0x18341f55120>,\n",
       " <matplotlib.lines.Line2D at 0x18341f55210>,\n",
       " <matplotlib.lines.Line2D at 0x18341f55300>,\n",
       " <matplotlib.lines.Line2D at 0x18341f553f0>,\n",
       " <matplotlib.lines.Line2D at 0x18341f554e0>,\n",
       " <matplotlib.lines.Line2D at 0x18341f555d0>,\n",
       " <matplotlib.lines.Line2D at 0x18341f556c0>,\n",
       " <matplotlib.lines.Line2D at 0x18341f557b0>,\n",
       " <matplotlib.lines.Line2D at 0x18341f558a0>,\n",
       " <matplotlib.lines.Line2D at 0x18341f55990>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(X, y, 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([17.8666, 33.1936, 33.7177, 30.3152], grad_fn=<SqueezeBackward0>)\n",
      "tensor([30.6852, 31.3688, 15.3180,  8.9217], grad_fn=<SqueezeBackward0>)\n",
      "tensor([14.2861, 31.6776, -4.5517, 49.9827], grad_fn=<SqueezeBackward0>)\n",
      "tensor([21.8976,  8.6589,  7.2462,  5.3239], grad_fn=<SqueezeBackward0>)\n",
      "tensor([20.2570, 27.5044, 19.3428, 25.4061], grad_fn=<SqueezeBackward0>)\n",
      "tensor([43.2099,  3.4193, -1.5966,  6.3146], grad_fn=<SqueezeBackward0>)\n",
      "tensor([31.0157, 33.5908,  4.6145, 11.6525], grad_fn=<SqueezeBackward0>)\n",
      "tensor([25.9036, 17.7768, 48.1083, 39.0001], grad_fn=<SqueezeBackward0>)\n",
      "tensor([10.2503, 45.6926, 19.0936, 17.9164], grad_fn=<SqueezeBackward0>)\n",
      "tensor([29.3446, 27.5041, 45.5820, 26.9437], grad_fn=<SqueezeBackward0>)\n",
      "tensor([-3.6278, 18.1754, 11.4211, 14.7193], grad_fn=<SqueezeBackward0>)\n",
      "tensor([19.0018, 23.6865, 34.5810,  7.5703], grad_fn=<SqueezeBackward0>)\n",
      "tensor([21.5583, 41.7558, 41.8535, 10.3102], grad_fn=<SqueezeBackward0>)\n",
      "tensor([50.6084, 31.5840, 53.8439,  0.5634], grad_fn=<SqueezeBackward0>)\n",
      "tensor([28.7398, 24.0018, 25.7691, 12.6675], grad_fn=<SqueezeBackward0>)\n",
      "tensor([42.8804, 24.3587, 13.4764, 39.9626], grad_fn=<SqueezeBackward0>)\n",
      "tensor([36.5147, 44.1058, 25.9549, 29.4720], grad_fn=<SqueezeBackward0>)\n",
      "tensor([51.2311, 27.6498, 25.3318, 21.6241], grad_fn=<SqueezeBackward0>)\n",
      "tensor([22.9397, 35.9641, 38.4696, 25.9342], grad_fn=<SqueezeBackward0>)\n",
      "tensor([17.7323, 32.1453, 25.0441, 36.1646], grad_fn=<SqueezeBackward0>)\n",
      "tensor([25.7109, 14.1964, 36.1703, 48.6004], grad_fn=<SqueezeBackward0>)\n",
      "tensor([46.3660, 39.6900, 17.3111, 52.2643], grad_fn=<SqueezeBackward0>)\n",
      "tensor([47.5453, 58.2351, 48.7717, 46.2055], grad_fn=<SqueezeBackward0>)\n",
      "tensor([30.9795, 30.1010, 30.1925, 17.7758], grad_fn=<SqueezeBackward0>)\n",
      "tensor([42.9409, 44.9448, 39.7508, 30.9151], grad_fn=<SqueezeBackward0>)\n",
      "tensor([25.3815, 34.4367, 41.0358, 45.7129], grad_fn=<SqueezeBackward0>)\n",
      "tensor([39.0947, 45.2351, 34.4619, 37.2679], grad_fn=<SqueezeBackward0>)\n",
      "tensor([40.0535, 48.9549, 41.1548, 36.8884], grad_fn=<SqueezeBackward0>)\n",
      "tensor([57.8292, 51.8781, 19.3868, 40.7417], grad_fn=<SqueezeBackward0>)\n",
      "tensor([28.0472, 40.6554, 54.8172, 26.2042], grad_fn=<SqueezeBackward0>)\n",
      "tensor([57.9399, 29.5588, 45.0918, 53.1103], grad_fn=<SqueezeBackward0>)\n",
      "tensor([37.1534, 73.6560, 37.3765, 19.1105], grad_fn=<SqueezeBackward0>)\n",
      "tensor([59.5260, 60.7082, 20.6815, 23.2148], grad_fn=<SqueezeBackward0>)\n",
      "tensor([60.0761, 42.8121, 21.8705, 33.5433], grad_fn=<SqueezeBackward0>)\n",
      "tensor([47.9430, 25.5134, 67.9471, 88.3584], grad_fn=<SqueezeBackward0>)\n",
      "tensor([27.2144, 59.6349, 42.8476, 65.4548], grad_fn=<SqueezeBackward0>)\n",
      "tensor([41.5017, 61.7500, 47.3241, 44.7896], grad_fn=<SqueezeBackward0>)\n",
      "tensor([44.4354, 38.6965, 82.4654, 58.6899], grad_fn=<SqueezeBackward0>)\n",
      "tensor([63.4362, 54.8198, 52.3639, 43.5549], grad_fn=<SqueezeBackward0>)\n",
      "tensor([33.2645, 55.8900, 35.0277, 21.1503], grad_fn=<SqueezeBackward0>)\n",
      "tensor([53.9924, 32.6899, 53.0999, 50.3950], grad_fn=<SqueezeBackward0>)\n",
      "tensor([78.1490, 75.6828, 43.1248, 55.9822], grad_fn=<SqueezeBackward0>)\n",
      "tensor([59.1321, 48.0081, 23.6293, 43.0525], grad_fn=<SqueezeBackward0>)\n",
      "tensor([65.5606, 53.7167, 73.6908, 41.7386], grad_fn=<SqueezeBackward0>)\n",
      "tensor([61.9887, 67.0659, 30.3656, 51.7849], grad_fn=<SqueezeBackward0>)\n",
      "tensor([54.7909, 78.5590, 80.5517, 63.2585], grad_fn=<SqueezeBackward0>)\n",
      "tensor([47.8801, 79.3975, 46.7350, 55.9186], grad_fn=<SqueezeBackward0>)\n",
      "tensor([38.3827, 45.5418, 86.8646, 94.5855], grad_fn=<SqueezeBackward0>)\n",
      "tensor([ 62.3689,  83.5330,  71.8099, 108.0837], grad_fn=<SqueezeBackward0>)\n",
      "tensor([33.0958, 43.1203, 71.3497, 86.5441], grad_fn=<SqueezeBackward0>)\n",
      "tensor([75.4921, 35.0610, 60.4787, 54.1777], grad_fn=<SqueezeBackward0>)\n",
      "tensor([53.4791, 55.8795, 70.6625, 44.9371], grad_fn=<SqueezeBackward0>)\n",
      "tensor([62.2394, 71.9626, 54.5105, 67.8152], grad_fn=<SqueezeBackward0>)\n",
      "tensor([ 86.2581, 101.9592,  52.3236,  72.3945], grad_fn=<SqueezeBackward0>)\n",
      "tensor([55.3902, 60.0874, 81.7059, 75.5785], grad_fn=<SqueezeBackward0>)\n",
      "tensor([74.1126, 66.1789, 66.0626, 87.4046], grad_fn=<SqueezeBackward0>)\n",
      "tensor([91.9036, 74.3715, 78.6462, 70.8622], grad_fn=<SqueezeBackward0>)\n",
      "tensor([74.4148, 92.8507, 50.3362, 78.6293], grad_fn=<SqueezeBackward0>)\n",
      "tensor([92.6536, 79.5382, 90.7354, 69.9237], grad_fn=<SqueezeBackward0>)\n",
      "tensor([66.0611, 63.2752, 70.1665, 93.5784], grad_fn=<SqueezeBackward0>)\n",
      "tensor([ 71.4862,  85.2154, 104.5171,  91.3374], grad_fn=<SqueezeBackward0>)\n",
      "tensor([46.9869, 83.3441, 70.1858, 71.8685], grad_fn=<SqueezeBackward0>)\n",
      "tensor(78.4075, grad_fn=<SqueezeBackward0>)\n",
      "Epoch 1, Loss: 1.5733139234960596e+16\n",
      "tensor([87.9652, 74.8151, 48.2143, 70.4919], grad_fn=<SqueezeBackward0>)\n",
      "tensor([101.4680,  95.4548,  87.3837,  95.6626], grad_fn=<SqueezeBackward0>)\n",
      "tensor([70.8215, 87.1321, 72.9790, 84.5287], grad_fn=<SqueezeBackward0>)\n",
      "tensor([98.7510, 78.6871, 98.6623, 78.3163], grad_fn=<SqueezeBackward0>)\n",
      "tensor([ 88.2506, 111.9963,  92.5231,  79.2221], grad_fn=<SqueezeBackward0>)\n",
      "tensor([ 70.8841,  70.2657, 105.2481, 117.6881], grad_fn=<SqueezeBackward0>)\n",
      "tensor([ 92.4804,  90.6228, 100.5727, 104.2246], grad_fn=<SqueezeBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\howel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([91.9353, 77.3080, 83.3977, 63.7978], grad_fn=<SqueezeBackward0>)\n",
      "tensor([85.0124, 89.5945, 95.4432, 75.6238], grad_fn=<SqueezeBackward0>)\n",
      "tensor([94.2268, 94.2080, 82.6344, 84.0093], grad_fn=<SqueezeBackward0>)\n",
      "tensor([ 74.4055,  86.8778,  93.7822, 100.2402], grad_fn=<SqueezeBackward0>)\n",
      "tensor([111.4592,  72.4203,  96.9538,  83.9950], grad_fn=<SqueezeBackward0>)\n",
      "tensor([ 87.8556,  71.8561, 116.0078,  63.0342], grad_fn=<SqueezeBackward0>)\n",
      "tensor([ 71.2569,  89.4700,  75.6117, 111.8193], grad_fn=<SqueezeBackward0>)\n",
      "tensor([ 99.7049,  87.1512, 101.3273,  86.5017], grad_fn=<SqueezeBackward0>)\n",
      "tensor([ 90.5413,  95.8169, 112.1641,  89.6893], grad_fn=<SqueezeBackward0>)\n",
      "tensor([112.6451, 106.2899,  69.6440,  99.4657], grad_fn=<SqueezeBackward0>)\n",
      "tensor([ 91.3193, 111.0886,  94.0911,  75.2739], grad_fn=<SqueezeBackward0>)\n",
      "tensor([85.0133, 86.0972, 93.8307, 75.6634], grad_fn=<SqueezeBackward0>)\n",
      "tensor([106.8371,  81.3513,  90.9560, 113.5872], grad_fn=<SqueezeBackward0>)\n",
      "tensor([ 95.0082, 106.8555,  83.8811, 128.0913], grad_fn=<SqueezeBackward0>)\n",
      "tensor([112.5260,  74.9325, 104.8717,  84.0831], grad_fn=<SqueezeBackward0>)\n",
      "tensor([126.2773, 115.2122,  79.1198, 102.0420], grad_fn=<SqueezeBackward0>)\n",
      "tensor([109.0611, 115.4039, 100.1309, 100.6323], grad_fn=<SqueezeBackward0>)\n",
      "tensor([102.0258, 108.6243, 115.8269, 123.6221], grad_fn=<SqueezeBackward0>)\n",
      "tensor([ 90.7800, 125.8961, 135.6396, 134.1989], grad_fn=<SqueezeBackward0>)\n",
      "tensor([118.6910,  84.9313, 112.1612, 104.9971], grad_fn=<SqueezeBackward0>)\n",
      "tensor([100.2407, 132.2320, 117.7183, 127.0487], grad_fn=<SqueezeBackward0>)\n",
      "tensor([ 87.7085, 109.8800, 115.7369,  95.9173], grad_fn=<SqueezeBackward0>)\n",
      "tensor([ 73.9490, 107.7176,  95.5526, 116.7634], grad_fn=<SqueezeBackward0>)\n",
      "tensor([101.3735, 101.9560, 103.9393, 104.1281], grad_fn=<SqueezeBackward0>)\n",
      "tensor([ 82.2657, 147.7335, 124.2692, 118.1170], grad_fn=<SqueezeBackward0>)\n",
      "tensor([111.5749, 117.1954,  92.0069, 125.4368], grad_fn=<SqueezeBackward0>)\n",
      "tensor([117.0110, 112.2156, 110.2470,  94.3645], grad_fn=<SqueezeBackward0>)\n",
      "tensor([125.8478, 101.2669, 114.9818, 120.8937], grad_fn=<SqueezeBackward0>)\n",
      "tensor([119.9733, 140.1232,  98.3229, 120.1786], grad_fn=<SqueezeBackward0>)\n",
      "tensor([121.5328, 113.4801, 110.4793, 109.9160], grad_fn=<SqueezeBackward0>)\n",
      "tensor([131.5874, 138.8121,  79.9614, 126.6951], grad_fn=<SqueezeBackward0>)\n",
      "tensor([104.6443, 126.6901, 110.0465, 107.4038], grad_fn=<SqueezeBackward0>)\n",
      "tensor([ 98.8335, 112.0852, 119.3935, 113.1250], grad_fn=<SqueezeBackward0>)\n",
      "tensor([116.3325,  98.0811, 119.9322, 138.3439], grad_fn=<SqueezeBackward0>)\n",
      "tensor([120.2308, 122.5595, 160.6268, 149.0710], grad_fn=<SqueezeBackward0>)\n",
      "tensor([127.7726,  97.8411, 123.3829, 164.5479], grad_fn=<SqueezeBackward0>)\n",
      "tensor([136.8795, 169.8854, 144.8656, 121.8864], grad_fn=<SqueezeBackward0>)\n",
      "tensor([129.5533, 120.5204, 119.5373, 133.1297], grad_fn=<SqueezeBackward0>)\n",
      "tensor([107.4071, 132.2720,  93.0416, 106.9560], grad_fn=<SqueezeBackward0>)\n",
      "tensor([120.6140, 130.8805, 121.2897, 141.4277], grad_fn=<SqueezeBackward0>)\n",
      "tensor([135.9405, 105.1686, 123.5247, 108.9038], grad_fn=<SqueezeBackward0>)\n",
      "tensor([140.8655, 148.9515, 115.3145, 150.8791], grad_fn=<SqueezeBackward0>)\n",
      "tensor([104.1067, 128.5379, 157.2859, 175.4674], grad_fn=<SqueezeBackward0>)\n",
      "tensor([ 93.4659, 126.5978,  92.7164, 122.6715], grad_fn=<SqueezeBackward0>)\n",
      "tensor([116.3756, 121.9177, 133.7531, 129.5356], grad_fn=<SqueezeBackward0>)\n",
      "tensor([124.9536, 161.6223, 124.5709, 107.7558], grad_fn=<SqueezeBackward0>)\n",
      "tensor([147.4746, 162.9450, 139.7464, 129.1507], grad_fn=<SqueezeBackward0>)\n",
      "tensor([112.3412, 145.3752, 102.5610, 136.3991], grad_fn=<SqueezeBackward0>)\n",
      "tensor([127.2115, 113.7200, 119.6717, 137.0090], grad_fn=<SqueezeBackward0>)\n",
      "tensor([122.7565, 137.2605, 124.6118, 142.0018], grad_fn=<SqueezeBackward0>)\n",
      "tensor([111.6920, 156.5320, 145.3657, 119.7744], grad_fn=<SqueezeBackward0>)\n",
      "tensor([125.2374, 167.7843, 133.6774, 157.7938], grad_fn=<SqueezeBackward0>)\n",
      "tensor([166.2217, 155.5436, 146.5737, 164.8223], grad_fn=<SqueezeBackward0>)\n",
      "tensor([160.6025, 134.5897, 144.6095, 149.1430], grad_fn=<SqueezeBackward0>)\n",
      "tensor([153.2638, 130.2058, 166.7336, 142.1253], grad_fn=<SqueezeBackward0>)\n",
      "tensor(124.0026, grad_fn=<SqueezeBackward0>)\n",
      "Epoch 2, Loss: 1.62906322710515e+16\n",
      "tensor([153.8052, 129.2303, 169.4224, 146.6651], grad_fn=<SqueezeBackward0>)\n",
      "tensor([139.3103, 165.3838, 136.8153, 145.1382], grad_fn=<SqueezeBackward0>)\n",
      "tensor([144.2799, 133.1413, 127.1868, 123.8282], grad_fn=<SqueezeBackward0>)\n",
      "tensor([124.4440, 123.4277, 146.7578, 151.7956], grad_fn=<SqueezeBackward0>)\n",
      "tensor([146.9420, 162.0242, 143.1749, 144.4222], grad_fn=<SqueezeBackward0>)\n",
      "tensor([156.3693, 149.2482, 132.6540, 134.9556], grad_fn=<SqueezeBackward0>)\n",
      "tensor([143.7203, 164.5059, 181.7746, 142.2362], grad_fn=<SqueezeBackward0>)\n",
      "tensor([173.4706, 154.8939, 134.6705, 175.2013], grad_fn=<SqueezeBackward0>)\n",
      "tensor([185.7201, 169.6769, 172.8534, 153.0626], grad_fn=<SqueezeBackward0>)\n",
      "tensor([144.1495, 179.4006, 148.3520, 120.8154], grad_fn=<SqueezeBackward0>)\n",
      "tensor([156.7350, 163.1061, 165.4251, 193.9042], grad_fn=<SqueezeBackward0>)\n",
      "tensor([164.0267, 143.4645, 153.9775, 176.8725], grad_fn=<SqueezeBackward0>)\n",
      "tensor([160.9405, 163.7320, 120.7734, 160.4012], grad_fn=<SqueezeBackward0>)\n",
      "tensor([206.8991, 135.9683, 163.1481, 175.1739], grad_fn=<SqueezeBackward0>)\n",
      "tensor([147.5712, 147.4973, 141.7732, 170.3647], grad_fn=<SqueezeBackward0>)\n",
      "tensor([174.5603, 151.6939, 125.3156, 183.6921], grad_fn=<SqueezeBackward0>)\n",
      "tensor([153.9057, 151.2266, 167.5725, 164.2473], grad_fn=<SqueezeBackward0>)\n",
      "tensor([171.0798, 148.5432, 162.9017, 137.0071], grad_fn=<SqueezeBackward0>)\n",
      "tensor([144.3539, 154.9759, 153.7922, 168.7494], grad_fn=<SqueezeBackward0>)\n",
      "tensor([178.6614, 159.5013, 167.8907, 141.8370], grad_fn=<SqueezeBackward0>)\n",
      "tensor([161.2182, 159.0459, 145.8247, 145.1314], grad_fn=<SqueezeBackward0>)\n",
      "tensor([159.6411, 168.9238, 181.5407, 177.2591], grad_fn=<SqueezeBackward0>)\n",
      "tensor([189.5948, 148.9905, 217.9563, 151.6418], grad_fn=<SqueezeBackward0>)\n",
      "tensor([173.8291, 167.7726, 176.4198, 173.4520], grad_fn=<SqueezeBackward0>)\n",
      "tensor([143.3747, 165.8991, 161.1788, 200.2816], grad_fn=<SqueezeBackward0>)\n",
      "tensor([194.6095, 166.1702, 171.6996, 167.5649], grad_fn=<SqueezeBackward0>)\n",
      "tensor([170.5654, 168.1877, 177.0335, 183.4518], grad_fn=<SqueezeBackward0>)\n",
      "tensor([181.4843, 143.2881, 134.3309, 195.1659], grad_fn=<SqueezeBackward0>)\n",
      "tensor([150.2502, 170.6330, 207.5467, 220.3072], grad_fn=<SqueezeBackward0>)\n",
      "tensor([201.4123, 187.1079, 204.6824, 186.9797], grad_fn=<SqueezeBackward0>)\n",
      "tensor([159.2610, 175.0525, 171.0972, 176.1012], grad_fn=<SqueezeBackward0>)\n",
      "tensor([181.7175, 191.1237, 163.4125, 194.4052], grad_fn=<SqueezeBackward0>)\n",
      "tensor([192.2771, 175.4171, 190.6875, 187.9788], grad_fn=<SqueezeBackward0>)\n",
      "tensor([185.0921, 202.4267, 184.1906, 154.5508], grad_fn=<SqueezeBackward0>)\n",
      "tensor([148.4836, 175.5121, 160.9861, 210.8710], grad_fn=<SqueezeBackward0>)\n",
      "tensor([173.1154, 195.1837, 164.4785, 191.9845], grad_fn=<SqueezeBackward0>)\n",
      "tensor([178.0379, 173.5726, 209.3273, 175.6632], grad_fn=<SqueezeBackward0>)\n",
      "tensor([160.2683, 172.9844, 172.0443, 211.6389], grad_fn=<SqueezeBackward0>)\n",
      "tensor([198.8064, 161.0151, 154.4648, 188.7861], grad_fn=<SqueezeBackward0>)\n",
      "tensor([179.0555, 200.3152, 179.6001, 204.4948], grad_fn=<SqueezeBackward0>)\n",
      "tensor([203.8505, 183.9792, 215.9842, 165.6574], grad_fn=<SqueezeBackward0>)\n",
      "tensor([195.9571, 168.0324, 193.2357, 221.5651], grad_fn=<SqueezeBackward0>)\n",
      "tensor([170.2790, 193.7962, 174.8964, 168.4459], grad_fn=<SqueezeBackward0>)\n",
      "tensor([185.9964, 155.9294, 144.2110, 185.8230], grad_fn=<SqueezeBackward0>)\n",
      "tensor([195.2139, 197.4006, 173.3379, 178.4174], grad_fn=<SqueezeBackward0>)\n",
      "tensor([223.0068, 204.8869, 200.5811, 214.4108], grad_fn=<SqueezeBackward0>)\n",
      "tensor([190.2611, 210.9440, 178.5739, 191.3068], grad_fn=<SqueezeBackward0>)\n",
      "tensor([202.3416, 207.5019, 177.3041, 202.3967], grad_fn=<SqueezeBackward0>)\n",
      "tensor([175.0647, 223.7995, 191.6563, 181.4073], grad_fn=<SqueezeBackward0>)\n",
      "tensor([197.9486, 187.9671, 190.6871, 201.8141], grad_fn=<SqueezeBackward0>)\n",
      "tensor([202.6336, 187.7520, 211.3129, 218.9986], grad_fn=<SqueezeBackward0>)\n",
      "tensor([197.1402, 143.9747, 208.0751, 203.4862], grad_fn=<SqueezeBackward0>)\n",
      "tensor([200.5821, 188.9370, 211.0265, 225.0829], grad_fn=<SqueezeBackward0>)\n",
      "tensor([215.6238, 233.7754, 188.0503, 185.5438], grad_fn=<SqueezeBackward0>)\n",
      "tensor([191.8560, 215.9069, 199.3109, 197.9390], grad_fn=<SqueezeBackward0>)\n",
      "tensor([209.4999, 175.8553, 185.3091, 246.5203], grad_fn=<SqueezeBackward0>)\n",
      "tensor([205.4387, 176.2416, 196.7289, 211.0463], grad_fn=<SqueezeBackward0>)\n",
      "tensor([204.6582, 196.2296, 174.4050, 196.7129], grad_fn=<SqueezeBackward0>)\n",
      "tensor([185.7682, 204.4067, 216.2724, 209.0407], grad_fn=<SqueezeBackward0>)\n",
      "tensor([220.5186, 192.5206, 233.3201, 215.6714], grad_fn=<SqueezeBackward0>)\n",
      "tensor([232.7468, 217.5700, 189.1274, 167.2989], grad_fn=<SqueezeBackward0>)\n",
      "tensor([256.6350, 240.9337, 207.1022, 226.2074], grad_fn=<SqueezeBackward0>)\n",
      "tensor(210.5677, grad_fn=<SqueezeBackward0>)\n",
      "Epoch 3, Loss: 1.561957667364526e+16\n",
      "tensor([243.1116, 206.6685, 201.6549, 204.5736], grad_fn=<SqueezeBackward0>)\n",
      "tensor([200.3809, 242.6958, 229.2925, 180.7910], grad_fn=<SqueezeBackward0>)\n",
      "tensor([191.5214, 215.4868, 204.7397, 238.9665], grad_fn=<SqueezeBackward0>)\n",
      "tensor([206.6341, 210.8313, 229.4594, 173.2251], grad_fn=<SqueezeBackward0>)\n",
      "tensor([192.6477, 206.4078, 226.7848, 175.8969], grad_fn=<SqueezeBackward0>)\n",
      "tensor([241.2924, 218.3047, 209.1651, 206.1346], grad_fn=<SqueezeBackward0>)\n",
      "tensor([215.1154, 198.6939, 208.3537, 241.5584], grad_fn=<SqueezeBackward0>)\n",
      "tensor([230.9976, 219.2258, 215.6271, 225.0894], grad_fn=<SqueezeBackward0>)\n",
      "tensor([212.3137, 228.4967, 220.1829, 265.5851], grad_fn=<SqueezeBackward0>)\n",
      "tensor([232.4144, 207.9908, 254.9306, 228.2276], grad_fn=<SqueezeBackward0>)\n",
      "tensor([228.8855, 221.1059, 208.0836, 216.5552], grad_fn=<SqueezeBackward0>)\n",
      "tensor([218.2127, 252.7916, 193.0192, 225.4850], grad_fn=<SqueezeBackward0>)\n",
      "tensor([235.0642, 220.7991, 218.9160, 245.3401], grad_fn=<SqueezeBackward0>)\n",
      "tensor([257.1107, 224.4890, 219.4195, 224.8271], grad_fn=<SqueezeBackward0>)\n",
      "tensor([225.9747, 228.1485, 231.0106, 195.3182], grad_fn=<SqueezeBackward0>)\n",
      "tensor([244.0827, 182.9619, 202.0588, 221.7324], grad_fn=<SqueezeBackward0>)\n",
      "tensor([259.8081, 246.2635, 230.5637, 268.1697], grad_fn=<SqueezeBackward0>)\n",
      "tensor([257.7018, 166.4794, 244.0547, 227.7285], grad_fn=<SqueezeBackward0>)\n",
      "tensor([229.7157, 234.0258, 221.8219, 224.0359], grad_fn=<SqueezeBackward0>)\n",
      "tensor([239.3883, 243.3891, 251.2073, 238.6376], grad_fn=<SqueezeBackward0>)\n",
      "tensor([207.0957, 242.0257, 279.4579, 226.3724], grad_fn=<SqueezeBackward0>)\n",
      "tensor([260.2704, 210.1361, 219.0795, 229.5056], grad_fn=<SqueezeBackward0>)\n",
      "tensor([210.0456, 257.4616, 205.2615, 209.1682], grad_fn=<SqueezeBackward0>)\n",
      "tensor([230.5399, 245.6550, 235.2613, 233.3604], grad_fn=<SqueezeBackward0>)\n",
      "tensor([218.8424, 237.4824, 214.7267, 201.6796], grad_fn=<SqueezeBackward0>)\n",
      "tensor([249.8269, 226.9599, 189.5327, 220.7781], grad_fn=<SqueezeBackward0>)\n",
      "tensor([244.3060, 186.7072, 244.6958, 250.2223], grad_fn=<SqueezeBackward0>)\n",
      "tensor([239.0325, 214.4439, 253.1986, 232.6489], grad_fn=<SqueezeBackward0>)\n",
      "tensor([231.1841, 243.0140, 240.0966, 247.1327], grad_fn=<SqueezeBackward0>)\n",
      "tensor([230.9438, 216.0123, 248.4453, 200.8592], grad_fn=<SqueezeBackward0>)\n",
      "tensor([213.0982, 247.9034, 222.9275, 236.2291], grad_fn=<SqueezeBackward0>)\n",
      "tensor([226.5965, 229.5313, 270.2934, 229.5739], grad_fn=<SqueezeBackward0>)\n",
      "tensor([261.5808, 267.1638, 270.3456, 213.2745], grad_fn=<SqueezeBackward0>)\n",
      "tensor([244.3676, 230.0497, 222.1442, 215.7131], grad_fn=<SqueezeBackward0>)\n",
      "tensor([234.9246, 246.5177, 193.0360, 248.0831], grad_fn=<SqueezeBackward0>)\n",
      "tensor([248.6126, 294.4504, 233.9273, 249.1871], grad_fn=<SqueezeBackward0>)\n",
      "tensor([213.7796, 222.4317, 238.3109, 225.4388], grad_fn=<SqueezeBackward0>)\n",
      "tensor([210.3032, 247.5288, 265.7758, 257.9904], grad_fn=<SqueezeBackward0>)\n",
      "tensor([267.6312, 296.5696, 217.1930, 239.2850], grad_fn=<SqueezeBackward0>)\n",
      "tensor([230.8524, 248.3536, 263.4497, 262.9611], grad_fn=<SqueezeBackward0>)\n",
      "tensor([243.3005, 224.7133, 216.4292, 253.7025], grad_fn=<SqueezeBackward0>)\n",
      "tensor([240.8184, 239.8110, 256.8853, 239.7227], grad_fn=<SqueezeBackward0>)\n",
      "tensor([247.2705, 306.3422, 278.5119, 252.9072], grad_fn=<SqueezeBackward0>)\n",
      "tensor([279.3613, 234.9387, 238.5808, 238.3111], grad_fn=<SqueezeBackward0>)\n",
      "tensor([199.2232, 269.1513, 240.8289, 269.3411], grad_fn=<SqueezeBackward0>)\n",
      "tensor([249.0978, 278.7663, 255.2483, 270.6260], grad_fn=<SqueezeBackward0>)\n",
      "tensor([279.3506, 257.7030, 239.5977, 261.7285], grad_fn=<SqueezeBackward0>)\n",
      "tensor([275.5262, 256.7610, 216.3033, 229.7211], grad_fn=<SqueezeBackward0>)\n",
      "tensor([266.7886, 278.4355, 235.1558, 258.5126], grad_fn=<SqueezeBackward0>)\n",
      "tensor([291.6417, 269.4827, 274.3114, 233.5899], grad_fn=<SqueezeBackward0>)\n",
      "tensor([245.1219, 248.5010, 266.0031, 242.7658], grad_fn=<SqueezeBackward0>)\n",
      "tensor([262.2684, 262.1660, 278.8654, 252.5292], grad_fn=<SqueezeBackward0>)\n",
      "tensor([260.5332, 271.6211, 270.7222, 262.4437], grad_fn=<SqueezeBackward0>)\n",
      "tensor([272.3112, 302.9495, 303.1404, 244.5307], grad_fn=<SqueezeBackward0>)\n",
      "tensor([249.5135, 282.0987, 260.6284, 277.1172], grad_fn=<SqueezeBackward0>)\n",
      "tensor([296.1864, 243.7481, 239.7288, 267.8170], grad_fn=<SqueezeBackward0>)\n",
      "tensor([239.4487, 270.2537, 235.7404, 273.0370], grad_fn=<SqueezeBackward0>)\n",
      "tensor([298.0389, 251.0442, 257.4592, 252.4874], grad_fn=<SqueezeBackward0>)\n",
      "tensor([244.3135, 283.4511, 258.8043, 283.2926], grad_fn=<SqueezeBackward0>)\n",
      "tensor([295.2212, 284.5700, 264.3771, 288.8547], grad_fn=<SqueezeBackward0>)\n",
      "tensor([257.8911, 264.0225, 233.8429, 310.9990], grad_fn=<SqueezeBackward0>)\n",
      "tensor([267.9864, 246.2397, 276.8630, 253.7667], grad_fn=<SqueezeBackward0>)\n",
      "tensor(249.6789, grad_fn=<SqueezeBackward0>)\n",
      "Epoch 4, Loss: 1.5903348324997738e+16\n",
      "tensor([330.4710, 238.8251, 290.8939, 277.1741], grad_fn=<SqueezeBackward0>)\n",
      "tensor([262.0312, 259.1784, 265.5989, 276.0742], grad_fn=<SqueezeBackward0>)\n",
      "tensor([312.1784, 315.8669, 272.7444, 260.8673], grad_fn=<SqueezeBackward0>)\n",
      "tensor([252.9922, 265.3659, 301.5948, 274.7650], grad_fn=<SqueezeBackward0>)\n",
      "tensor([293.4309, 309.9579, 261.7563, 256.6965], grad_fn=<SqueezeBackward0>)\n",
      "tensor([282.1632, 285.2787, 266.2939, 253.7622], grad_fn=<SqueezeBackward0>)\n",
      "tensor([291.5243, 300.7121, 256.5033, 289.1479], grad_fn=<SqueezeBackward0>)\n",
      "tensor([283.0117, 308.6608, 279.8681, 287.6219], grad_fn=<SqueezeBackward0>)\n",
      "tensor([227.7266, 306.9402, 320.1519, 320.6819], grad_fn=<SqueezeBackward0>)\n",
      "tensor([295.2574, 258.2709, 280.7827, 297.0052], grad_fn=<SqueezeBackward0>)\n",
      "tensor([287.7603, 284.9784, 265.7182, 322.0959], grad_fn=<SqueezeBackward0>)\n",
      "tensor([300.7130, 304.4214, 286.7691, 285.9818], grad_fn=<SqueezeBackward0>)\n",
      "tensor([307.5674, 321.6509, 312.6924, 292.1175], grad_fn=<SqueezeBackward0>)\n",
      "tensor([286.8694, 280.3955, 277.8801, 330.8965], grad_fn=<SqueezeBackward0>)\n",
      "tensor([311.2588, 298.1249, 239.3195, 275.2577], grad_fn=<SqueezeBackward0>)\n",
      "tensor([309.6756, 297.5017, 279.4886, 302.7850], grad_fn=<SqueezeBackward0>)\n",
      "tensor([289.3547, 281.3508, 287.7739, 307.0750], grad_fn=<SqueezeBackward0>)\n",
      "tensor([286.5579, 299.7235, 273.2130, 294.4995], grad_fn=<SqueezeBackward0>)\n",
      "tensor([312.5736, 318.6786, 275.9612, 336.3412], grad_fn=<SqueezeBackward0>)\n",
      "tensor([327.3895, 329.2367, 312.2569, 255.7647], grad_fn=<SqueezeBackward0>)\n",
      "tensor([311.6610, 285.5425, 353.4243, 282.6945], grad_fn=<SqueezeBackward0>)\n",
      "tensor([313.7995, 302.7451, 323.9176, 299.8010], grad_fn=<SqueezeBackward0>)\n",
      "tensor([277.9058, 267.2987, 270.9002, 333.9326], grad_fn=<SqueezeBackward0>)\n",
      "tensor([299.3138, 314.9860, 302.1536, 334.4421], grad_fn=<SqueezeBackward0>)\n",
      "tensor([300.9962, 304.5885, 254.0624, 297.0377], grad_fn=<SqueezeBackward0>)\n",
      "tensor([263.8652, 239.5685, 293.2733, 298.9652], grad_fn=<SqueezeBackward0>)\n",
      "tensor([300.8039, 291.3213, 269.5188, 339.2141], grad_fn=<SqueezeBackward0>)\n",
      "tensor([243.2832, 272.0280, 308.8854, 304.1320], grad_fn=<SqueezeBackward0>)\n",
      "tensor([228.9433, 292.4736, 317.4974, 312.7078], grad_fn=<SqueezeBackward0>)\n",
      "tensor([278.4538, 288.8916, 313.6280, 302.4337], grad_fn=<SqueezeBackward0>)\n",
      "tensor([311.0256, 264.7481, 280.7931, 322.4149], grad_fn=<SqueezeBackward0>)\n",
      "tensor([309.6226, 281.6849, 360.2372, 304.6545], grad_fn=<SqueezeBackward0>)\n",
      "tensor([291.0611, 336.0711, 300.4455, 324.8955], grad_fn=<SqueezeBackward0>)\n",
      "tensor([325.3315, 305.9384, 310.5519, 314.6648], grad_fn=<SqueezeBackward0>)\n",
      "tensor([317.3747, 297.9105, 289.8071, 318.5147], grad_fn=<SqueezeBackward0>)\n",
      "tensor([353.9000, 333.9352, 307.2991, 308.6192], grad_fn=<SqueezeBackward0>)\n",
      "tensor([332.6746, 297.0707, 271.9520, 274.2389], grad_fn=<SqueezeBackward0>)\n",
      "tensor([297.9551, 312.2928, 330.1561, 337.3517], grad_fn=<SqueezeBackward0>)\n",
      "tensor([322.2867, 314.5338, 306.9742, 296.3286], grad_fn=<SqueezeBackward0>)\n",
      "tensor([304.8025, 303.9660, 319.7567, 288.8342], grad_fn=<SqueezeBackward0>)\n",
      "tensor([287.1874, 309.5786, 330.0519, 310.1137], grad_fn=<SqueezeBackward0>)\n",
      "tensor([291.7960, 290.2578, 352.0335, 334.5078], grad_fn=<SqueezeBackward0>)\n",
      "tensor([269.2245, 326.0007, 303.8922, 335.6404], grad_fn=<SqueezeBackward0>)\n",
      "tensor([323.6302, 345.3147, 320.6321, 310.1468], grad_fn=<SqueezeBackward0>)\n",
      "tensor([337.4386, 300.0572, 312.5521, 319.4263], grad_fn=<SqueezeBackward0>)\n",
      "tensor([342.9379, 315.6076, 300.6622, 336.9058], grad_fn=<SqueezeBackward0>)\n",
      "tensor([335.8647, 315.4962, 315.1187, 376.2457], grad_fn=<SqueezeBackward0>)\n",
      "tensor([288.8474, 271.8346, 318.8150, 326.8764], grad_fn=<SqueezeBackward0>)\n",
      "tensor([329.4864, 329.3109, 308.0378, 275.2874], grad_fn=<SqueezeBackward0>)\n",
      "tensor([341.8513, 342.7125, 304.7554, 280.2750], grad_fn=<SqueezeBackward0>)\n",
      "tensor([312.2716, 312.9046, 346.3735, 332.7101], grad_fn=<SqueezeBackward0>)\n",
      "tensor([352.4641, 336.4897, 338.5305, 328.9384], grad_fn=<SqueezeBackward0>)\n",
      "tensor([312.2020, 334.3887, 297.6359, 378.9150], grad_fn=<SqueezeBackward0>)\n",
      "tensor([310.5248, 308.5335, 333.9043, 332.0103], grad_fn=<SqueezeBackward0>)\n",
      "tensor([321.3718, 336.0807, 300.5993, 300.2803], grad_fn=<SqueezeBackward0>)\n",
      "tensor([326.1538, 320.1914, 336.5482, 316.8894], grad_fn=<SqueezeBackward0>)\n",
      "tensor([331.9431, 321.4636, 361.9349, 349.1590], grad_fn=<SqueezeBackward0>)\n",
      "tensor([294.6199, 334.2111, 287.5498, 364.9585], grad_fn=<SqueezeBackward0>)\n",
      "tensor([303.2582, 309.8257, 364.7150, 299.3948], grad_fn=<SqueezeBackward0>)\n",
      "tensor([305.3752, 370.3250, 321.0088, 387.4857], grad_fn=<SqueezeBackward0>)\n",
      "tensor([308.8684, 343.9393, 327.6912, 339.3275], grad_fn=<SqueezeBackward0>)\n",
      "tensor([363.2585, 358.9756, 313.7958, 344.7235], grad_fn=<SqueezeBackward0>)\n",
      "tensor(340.2811, grad_fn=<SqueezeBackward0>)\n",
      "Epoch 5, Loss: 1.6404427674559634e+16\n",
      "tensor([324.8812, 345.2907, 329.8050, 357.1360], grad_fn=<SqueezeBackward0>)\n",
      "tensor([311.8747, 386.4913, 358.6211, 342.7410], grad_fn=<SqueezeBackward0>)\n",
      "tensor([365.6277, 357.0490, 329.5115, 363.9757], grad_fn=<SqueezeBackward0>)\n",
      "tensor([295.7007, 328.8648, 350.9262, 347.6406], grad_fn=<SqueezeBackward0>)\n",
      "tensor([343.5956, 333.3311, 318.1668, 366.8192], grad_fn=<SqueezeBackward0>)\n",
      "tensor([371.9724, 367.9077, 382.7425, 313.2411], grad_fn=<SqueezeBackward0>)\n",
      "tensor([321.2309, 369.3313, 354.7101, 367.4895], grad_fn=<SqueezeBackward0>)\n",
      "tensor([308.0220, 366.4917, 354.2233, 335.3735], grad_fn=<SqueezeBackward0>)\n",
      "tensor([350.6114, 336.3225, 347.4493, 338.8459], grad_fn=<SqueezeBackward0>)\n",
      "tensor([349.8954, 366.9434, 343.1657, 344.2454], grad_fn=<SqueezeBackward0>)\n",
      "tensor([360.4485, 350.5659, 358.9172, 333.0174], grad_fn=<SqueezeBackward0>)\n",
      "tensor([369.7158, 373.7823, 337.1807, 358.6609], grad_fn=<SqueezeBackward0>)\n",
      "tensor([329.8691, 343.4456, 362.1702, 349.6969], grad_fn=<SqueezeBackward0>)\n",
      "tensor([397.1631, 383.5829, 348.1564, 345.1800], grad_fn=<SqueezeBackward0>)\n",
      "tensor([422.5365, 335.0173, 356.7232, 383.4493], grad_fn=<SqueezeBackward0>)\n",
      "tensor([339.1729, 346.1882, 379.9197, 349.1651], grad_fn=<SqueezeBackward0>)\n",
      "tensor([340.6194, 375.4918, 327.4745, 363.7746], grad_fn=<SqueezeBackward0>)\n",
      "tensor([327.2236, 313.1031, 356.1980, 390.4961], grad_fn=<SqueezeBackward0>)\n",
      "tensor([307.7786, 354.9616, 364.0511, 319.7331], grad_fn=<SqueezeBackward0>)\n",
      "tensor([354.4772, 324.8570, 348.9498, 372.5305], grad_fn=<SqueezeBackward0>)\n",
      "tensor([320.9465, 349.3536, 400.3394, 320.1788], grad_fn=<SqueezeBackward0>)\n",
      "tensor([353.4659, 376.0952, 366.1399, 349.6186], grad_fn=<SqueezeBackward0>)\n",
      "tensor([380.0456, 362.4184, 382.0526, 366.3230], grad_fn=<SqueezeBackward0>)\n",
      "tensor([368.1264, 388.7838, 360.0242, 340.0080], grad_fn=<SqueezeBackward0>)\n",
      "tensor([342.6489, 297.8850, 334.5424, 310.9963], grad_fn=<SqueezeBackward0>)\n",
      "tensor([304.9136, 329.2811, 409.4360, 377.1648], grad_fn=<SqueezeBackward0>)\n",
      "tensor([368.5938, 313.7986, 405.9091, 366.4989], grad_fn=<SqueezeBackward0>)\n",
      "tensor([413.6068, 374.9485, 385.2584, 317.9296], grad_fn=<SqueezeBackward0>)\n",
      "tensor([374.6217, 376.5464, 398.2555, 419.9686], grad_fn=<SqueezeBackward0>)\n",
      "tensor([342.0136, 388.9053, 411.0444, 366.1481], grad_fn=<SqueezeBackward0>)\n",
      "tensor([352.3790, 377.6479, 376.4692, 351.9956], grad_fn=<SqueezeBackward0>)\n",
      "tensor([391.4133, 369.7341, 419.5988, 406.5546], grad_fn=<SqueezeBackward0>)\n",
      "tensor([366.1001, 384.5981, 345.3975, 370.9166], grad_fn=<SqueezeBackward0>)\n",
      "tensor([380.7890, 430.1091, 373.6397, 385.0727], grad_fn=<SqueezeBackward0>)\n",
      "tensor([429.9443, 381.1256, 354.3306, 375.3495], grad_fn=<SqueezeBackward0>)\n",
      "tensor([408.1299, 392.6391, 365.8723, 381.1940], grad_fn=<SqueezeBackward0>)\n",
      "tensor([351.1890, 399.1331, 361.6859, 377.1621], grad_fn=<SqueezeBackward0>)\n",
      "tensor([349.2549, 390.2629, 351.5443, 360.1376], grad_fn=<SqueezeBackward0>)\n",
      "tensor([347.2567, 418.5309, 388.7031, 382.7646], grad_fn=<SqueezeBackward0>)\n",
      "tensor([419.4871, 379.3550, 386.9792, 337.3044], grad_fn=<SqueezeBackward0>)\n",
      "tensor([422.1783, 349.1525, 334.8419, 365.5327], grad_fn=<SqueezeBackward0>)\n",
      "tensor([403.6400, 352.9367, 393.9430, 361.7948], grad_fn=<SqueezeBackward0>)\n",
      "tensor([366.2962, 378.7890, 386.1602, 370.7610], grad_fn=<SqueezeBackward0>)\n",
      "tensor([357.7831, 371.1364, 384.7635, 386.7247], grad_fn=<SqueezeBackward0>)\n",
      "tensor([412.2605, 314.7957, 374.6233, 392.0106], grad_fn=<SqueezeBackward0>)\n",
      "tensor([418.1537, 395.4621, 373.6296, 340.1456], grad_fn=<SqueezeBackward0>)\n",
      "tensor([422.8571, 396.9111, 410.3468, 398.3452], grad_fn=<SqueezeBackward0>)\n",
      "tensor([392.4699, 389.9735, 393.1688, 351.7155], grad_fn=<SqueezeBackward0>)\n",
      "tensor([381.1949, 396.8094, 298.1404, 409.0883], grad_fn=<SqueezeBackward0>)\n",
      "tensor([422.5615, 452.1217, 365.3221, 394.3214], grad_fn=<SqueezeBackward0>)\n",
      "tensor([358.2527, 368.5173, 370.0047, 379.9839], grad_fn=<SqueezeBackward0>)\n",
      "tensor([396.9977, 407.5896, 408.6082, 366.8936], grad_fn=<SqueezeBackward0>)\n",
      "tensor([366.6313, 356.8033, 439.5002, 320.0248], grad_fn=<SqueezeBackward0>)\n",
      "tensor([427.2339, 398.2471, 382.8221, 422.7189], grad_fn=<SqueezeBackward0>)\n",
      "tensor([379.6366, 399.7070, 436.6701, 368.0303], grad_fn=<SqueezeBackward0>)\n",
      "tensor([461.3250, 431.8719, 440.1003, 395.3935], grad_fn=<SqueezeBackward0>)\n",
      "tensor([423.2566, 383.7773, 388.7113, 403.7736], grad_fn=<SqueezeBackward0>)\n",
      "tensor([364.2627, 414.3796, 411.9503, 379.9000], grad_fn=<SqueezeBackward0>)\n",
      "tensor([433.0342, 455.3209, 406.9785, 410.7505], grad_fn=<SqueezeBackward0>)\n",
      "tensor([421.6120, 425.1082, 397.2553, 420.4480], grad_fn=<SqueezeBackward0>)\n",
      "tensor([400.9187, 422.1066, 372.7570, 403.3358], grad_fn=<SqueezeBackward0>)\n",
      "tensor([409.4890, 398.2853, 461.7707, 402.7527], grad_fn=<SqueezeBackward0>)\n",
      "tensor(389.8246, grad_fn=<SqueezeBackward0>)\n",
      "Epoch 6, Loss: 1.5625702348115252e+16\n",
      "tensor([382.4601, 413.3664, 429.0725, 444.6971], grad_fn=<SqueezeBackward0>)\n",
      "tensor([411.1060, 388.0750, 440.9851, 399.7480], grad_fn=<SqueezeBackward0>)\n",
      "tensor([402.9531, 412.4328, 440.4502, 369.2670], grad_fn=<SqueezeBackward0>)\n",
      "tensor([439.8823, 413.1237, 378.9697, 388.7011], grad_fn=<SqueezeBackward0>)\n",
      "tensor([382.4633, 417.5868, 434.0486, 332.9440], grad_fn=<SqueezeBackward0>)\n",
      "tensor([420.8564, 377.7933, 404.8985, 421.0082], grad_fn=<SqueezeBackward0>)\n",
      "tensor([421.4902, 430.1195, 437.6685, 397.7379], grad_fn=<SqueezeBackward0>)\n",
      "tensor([357.1956, 410.0144, 401.9076, 418.5834], grad_fn=<SqueezeBackward0>)\n",
      "tensor([408.1114, 337.0222, 429.7896, 433.0365], grad_fn=<SqueezeBackward0>)\n",
      "tensor([432.5103, 398.9147, 403.8831, 435.3866], grad_fn=<SqueezeBackward0>)\n",
      "tensor([385.3729, 381.0720, 365.8255, 419.8948], grad_fn=<SqueezeBackward0>)\n",
      "tensor([419.7198, 423.3636, 489.1532, 433.9206], grad_fn=<SqueezeBackward0>)\n",
      "tensor([398.8239, 472.2358, 406.4144, 437.2264], grad_fn=<SqueezeBackward0>)\n",
      "tensor([390.0777, 380.1252, 386.6789, 453.4354], grad_fn=<SqueezeBackward0>)\n",
      "tensor([416.4363, 410.4759, 460.1238, 406.3691], grad_fn=<SqueezeBackward0>)\n",
      "tensor([422.5201, 457.3893, 465.5517, 385.3098], grad_fn=<SqueezeBackward0>)\n",
      "tensor([457.2564, 381.5115, 424.6800, 324.4566], grad_fn=<SqueezeBackward0>)\n",
      "tensor([426.3465, 408.0451, 441.1990, 427.7097], grad_fn=<SqueezeBackward0>)\n",
      "tensor([423.7508, 409.7310, 418.9737, 446.7762], grad_fn=<SqueezeBackward0>)\n",
      "tensor([416.2419, 419.5053, 424.1414, 484.1684], grad_fn=<SqueezeBackward0>)\n",
      "tensor([469.2534, 467.2734, 422.3443, 396.6471], grad_fn=<SqueezeBackward0>)\n",
      "tensor([449.0817, 446.8235, 469.2005, 478.3098], grad_fn=<SqueezeBackward0>)\n",
      "tensor([425.7541, 410.1228, 438.8960, 422.8989], grad_fn=<SqueezeBackward0>)\n",
      "tensor([394.7545, 380.4472, 451.2971, 414.7166], grad_fn=<SqueezeBackward0>)\n",
      "tensor([426.7770, 400.8138, 431.9040, 446.6765], grad_fn=<SqueezeBackward0>)\n",
      "tensor([423.8446, 439.6861, 424.3182, 416.0959], grad_fn=<SqueezeBackward0>)\n",
      "tensor([418.6639, 437.5515, 475.6649, 449.4212], grad_fn=<SqueezeBackward0>)\n",
      "tensor([431.2049, 439.3244, 384.4294, 409.3444], grad_fn=<SqueezeBackward0>)\n",
      "tensor([394.1033, 477.9305, 356.5640, 480.7042], grad_fn=<SqueezeBackward0>)\n",
      "tensor([417.9739, 425.7372, 417.2164, 413.9050], grad_fn=<SqueezeBackward0>)\n",
      "tensor([433.7486, 402.0762, 426.6699, 462.9418], grad_fn=<SqueezeBackward0>)\n",
      "tensor([420.6768, 488.5997, 456.5520, 443.5178], grad_fn=<SqueezeBackward0>)\n",
      "tensor([441.8398, 445.6365, 453.2758, 461.4078], grad_fn=<SqueezeBackward0>)\n",
      "tensor([488.2702, 455.7343, 468.7790, 422.7811], grad_fn=<SqueezeBackward0>)\n",
      "tensor([405.2689, 447.1214, 509.0690, 452.4943], grad_fn=<SqueezeBackward0>)\n",
      "tensor([469.1866, 457.0442, 450.5834, 454.0712], grad_fn=<SqueezeBackward0>)\n",
      "tensor([440.8384, 446.3380, 453.0057, 446.0101], grad_fn=<SqueezeBackward0>)\n",
      "tensor([454.9724, 394.3802, 447.5017, 478.3493], grad_fn=<SqueezeBackward0>)\n",
      "tensor([437.0085, 445.3854, 446.6237, 414.6902], grad_fn=<SqueezeBackward0>)\n",
      "tensor([432.1152, 474.8369, 455.2202, 421.9293], grad_fn=<SqueezeBackward0>)\n",
      "tensor([371.7407, 477.6145, 459.9387, 436.1832], grad_fn=<SqueezeBackward0>)\n",
      "tensor([449.4726, 450.8521, 433.2037, 504.3147], grad_fn=<SqueezeBackward0>)\n",
      "tensor([433.8317, 461.5799, 423.5895, 440.3783], grad_fn=<SqueezeBackward0>)\n",
      "tensor([479.3856, 471.6633, 412.7397, 492.5508], grad_fn=<SqueezeBackward0>)\n",
      "tensor([471.3997, 440.8483, 453.0925, 467.8855], grad_fn=<SqueezeBackward0>)\n",
      "tensor([469.6030, 506.4812, 440.2194, 436.2213], grad_fn=<SqueezeBackward0>)\n",
      "tensor([477.1283, 451.9349, 387.6646, 457.7991], grad_fn=<SqueezeBackward0>)\n",
      "tensor([418.1270, 503.7785, 520.2685, 465.9927], grad_fn=<SqueezeBackward0>)\n",
      "tensor([428.8927, 459.5493, 404.7372, 472.4210], grad_fn=<SqueezeBackward0>)\n",
      "tensor([437.6763, 451.7744, 474.1754, 426.9811], grad_fn=<SqueezeBackward0>)\n",
      "tensor([465.2923, 482.9170, 417.4536, 446.6040], grad_fn=<SqueezeBackward0>)\n",
      "tensor([449.7155, 462.9303, 425.2759, 466.7462], grad_fn=<SqueezeBackward0>)\n",
      "tensor([488.1981, 409.4091, 462.9939, 409.8301], grad_fn=<SqueezeBackward0>)\n",
      "tensor([472.4581, 429.3397, 496.1193, 391.7891], grad_fn=<SqueezeBackward0>)\n",
      "tensor([425.3809, 463.5827, 485.1633, 460.9203], grad_fn=<SqueezeBackward0>)\n",
      "tensor([468.6301, 468.7653, 422.5903, 412.7583], grad_fn=<SqueezeBackward0>)\n",
      "tensor([436.0035, 486.6491, 437.6446, 436.5280], grad_fn=<SqueezeBackward0>)\n",
      "tensor([508.8807, 458.5220, 524.0656, 390.4395], grad_fn=<SqueezeBackward0>)\n",
      "tensor([405.6050, 445.4436, 465.8150, 462.4182], grad_fn=<SqueezeBackward0>)\n",
      "tensor([464.5419, 399.1672, 436.1751, 478.9771], grad_fn=<SqueezeBackward0>)\n",
      "tensor([495.8651, 458.0943, 496.7869, 431.8863], grad_fn=<SqueezeBackward0>)\n",
      "tensor([503.7786, 529.2566, 445.2105, 446.9627], grad_fn=<SqueezeBackward0>)\n",
      "tensor(474.4412, grad_fn=<SqueezeBackward0>)\n",
      "Epoch 7, Loss: 1.5617989195358598e+16\n",
      "tensor([444.0244, 461.4599, 461.5271, 483.3680], grad_fn=<SqueezeBackward0>)\n",
      "tensor([487.4662, 514.8575, 500.1271, 533.3807], grad_fn=<SqueezeBackward0>)\n",
      "tensor([386.4213, 421.7656, 477.3602, 430.8969], grad_fn=<SqueezeBackward0>)\n",
      "tensor([515.6642, 463.5991, 494.0103, 475.0880], grad_fn=<SqueezeBackward0>)\n",
      "tensor([398.2335, 502.6153, 444.0747, 498.9028], grad_fn=<SqueezeBackward0>)\n",
      "tensor([435.8844, 444.9447, 466.2690, 489.3885], grad_fn=<SqueezeBackward0>)\n",
      "tensor([457.3035, 441.9731, 502.9743, 500.0430], grad_fn=<SqueezeBackward0>)\n",
      "tensor([468.9919, 437.0813, 485.6317, 449.3112], grad_fn=<SqueezeBackward0>)\n",
      "tensor([521.5280, 482.5486, 488.3025, 475.2288], grad_fn=<SqueezeBackward0>)\n",
      "tensor([501.8145, 505.2952, 500.2247, 461.5139], grad_fn=<SqueezeBackward0>)\n",
      "tensor([487.5144, 503.3037, 462.9990, 485.3194], grad_fn=<SqueezeBackward0>)\n",
      "tensor([448.7948, 442.3268, 486.1057, 536.2173], grad_fn=<SqueezeBackward0>)\n",
      "tensor([499.2302, 544.1479, 482.1030, 529.4249], grad_fn=<SqueezeBackward0>)\n",
      "tensor([428.0734, 481.9648, 475.5142, 490.3034], grad_fn=<SqueezeBackward0>)\n",
      "tensor([504.9618, 465.6837, 444.1180, 427.0027], grad_fn=<SqueezeBackward0>)\n",
      "tensor([527.3212, 554.0629, 540.7463, 467.3301], grad_fn=<SqueezeBackward0>)\n",
      "tensor([445.3832, 478.2032, 471.3213, 537.2841], grad_fn=<SqueezeBackward0>)\n",
      "tensor([449.8393, 533.3535, 513.6606, 467.6510], grad_fn=<SqueezeBackward0>)\n",
      "tensor([511.3688, 502.7128, 483.4072, 526.4346], grad_fn=<SqueezeBackward0>)\n",
      "tensor([400.2837, 476.5812, 499.6363, 500.2509], grad_fn=<SqueezeBackward0>)\n",
      "tensor([417.3047, 497.6304, 484.7618, 495.6941], grad_fn=<SqueezeBackward0>)\n",
      "tensor([447.0714, 501.7318, 528.4557, 510.1110], grad_fn=<SqueezeBackward0>)\n",
      "tensor([451.3599, 499.6313, 500.8651, 505.7579], grad_fn=<SqueezeBackward0>)\n",
      "tensor([482.5408, 482.9114, 490.6487, 502.4822], grad_fn=<SqueezeBackward0>)\n",
      "tensor([493.4939, 527.4196, 474.9036, 442.9838], grad_fn=<SqueezeBackward0>)\n",
      "tensor([489.5946, 476.1953, 534.0691, 492.2772], grad_fn=<SqueezeBackward0>)\n",
      "tensor([576.8137, 501.3430, 536.9496, 481.6783], grad_fn=<SqueezeBackward0>)\n",
      "tensor([539.5887, 481.3936, 450.2835, 474.2889], grad_fn=<SqueezeBackward0>)\n",
      "tensor([541.8476, 518.3649, 476.2503, 497.0130], grad_fn=<SqueezeBackward0>)\n",
      "tensor([496.6529, 476.1396, 515.1771, 491.8441], grad_fn=<SqueezeBackward0>)\n",
      "tensor([501.3840, 517.4117, 483.6165, 481.7830], grad_fn=<SqueezeBackward0>)\n",
      "tensor([501.5790, 491.1927, 470.8827, 491.8929], grad_fn=<SqueezeBackward0>)\n",
      "tensor([491.2213, 516.9549, 495.5679, 466.2278], grad_fn=<SqueezeBackward0>)\n",
      "tensor([532.6285, 527.7633, 472.9099, 504.9231], grad_fn=<SqueezeBackward0>)\n",
      "tensor([543.1110, 512.9124, 496.1953, 536.6083], grad_fn=<SqueezeBackward0>)\n",
      "tensor([506.4004, 509.0345, 524.7719, 471.5788], grad_fn=<SqueezeBackward0>)\n",
      "tensor([485.9704, 465.6405, 559.6403, 535.8918], grad_fn=<SqueezeBackward0>)\n",
      "tensor([506.2868, 499.2048, 452.2962, 469.1508], grad_fn=<SqueezeBackward0>)\n",
      "tensor([518.0191, 521.9438, 479.0448, 454.1509], grad_fn=<SqueezeBackward0>)\n",
      "tensor([518.9260, 502.9777, 514.0760, 531.6892], grad_fn=<SqueezeBackward0>)\n",
      "tensor([505.9490, 499.0007, 581.3135, 556.4491], grad_fn=<SqueezeBackward0>)\n",
      "tensor([529.8541, 478.9577, 499.1322, 492.9044], grad_fn=<SqueezeBackward0>)\n",
      "tensor([511.4217, 487.0238, 486.9137, 560.1898], grad_fn=<SqueezeBackward0>)\n",
      "tensor([476.1741, 422.5624, 470.7609, 526.7449], grad_fn=<SqueezeBackward0>)\n",
      "tensor([552.4216, 492.8429, 464.1442, 516.0762], grad_fn=<SqueezeBackward0>)\n",
      "tensor([465.3857, 513.8013, 515.4942, 504.2106], grad_fn=<SqueezeBackward0>)\n",
      "tensor([535.6077, 577.7562, 519.7671, 497.5365], grad_fn=<SqueezeBackward0>)\n",
      "tensor([503.9400, 531.4720, 495.2925, 514.6675], grad_fn=<SqueezeBackward0>)\n",
      "tensor([538.8289, 578.0914, 520.1772, 456.7741], grad_fn=<SqueezeBackward0>)\n",
      "tensor([540.6749, 493.8184, 499.8952, 547.7520], grad_fn=<SqueezeBackward0>)\n",
      "tensor([520.3459, 554.9358, 497.5418, 509.3665], grad_fn=<SqueezeBackward0>)\n",
      "tensor([482.8191, 482.3830, 527.3569, 550.4698], grad_fn=<SqueezeBackward0>)\n",
      "tensor([559.8888, 553.4917, 541.5366, 502.3034], grad_fn=<SqueezeBackward0>)\n",
      "tensor([502.7580, 529.5776, 540.4752, 523.6967], grad_fn=<SqueezeBackward0>)\n",
      "tensor([459.5540, 528.9188, 524.3355, 590.2101], grad_fn=<SqueezeBackward0>)\n",
      "tensor([525.4949, 549.4377, 544.7280, 453.2203], grad_fn=<SqueezeBackward0>)\n",
      "tensor([407.7983, 496.9313, 573.3000, 544.4366], grad_fn=<SqueezeBackward0>)\n",
      "tensor([540.6759, 497.3896, 438.5854, 538.4235], grad_fn=<SqueezeBackward0>)\n",
      "tensor([574.1098, 523.5184, 562.7173, 535.6207], grad_fn=<SqueezeBackward0>)\n",
      "tensor([533.1135, 472.6035, 456.2122, 535.5220], grad_fn=<SqueezeBackward0>)\n",
      "tensor([537.6133, 584.0637, 543.2590, 535.9424], grad_fn=<SqueezeBackward0>)\n",
      "tensor([525.8523, 502.0100, 470.1347, 502.3858], grad_fn=<SqueezeBackward0>)\n",
      "tensor(492.2452, grad_fn=<SqueezeBackward0>)\n",
      "Epoch 8, Loss: 1.6115838048305672e+16\n",
      "tensor([619.6005, 504.3049, 489.0451, 518.8654], grad_fn=<SqueezeBackward0>)\n",
      "tensor([511.1450, 515.7925, 518.9080, 592.4431], grad_fn=<SqueezeBackward0>)\n",
      "tensor([524.0256, 542.9022, 525.7215, 568.0165], grad_fn=<SqueezeBackward0>)\n",
      "tensor([483.1444, 565.5887, 552.3689, 518.0532], grad_fn=<SqueezeBackward0>)\n",
      "tensor([492.4860, 517.7255, 583.1125, 552.7525], grad_fn=<SqueezeBackward0>)\n",
      "tensor([527.7095, 585.1846, 530.3376, 568.1705], grad_fn=<SqueezeBackward0>)\n",
      "tensor([545.7600, 533.3060, 544.3069, 472.9854], grad_fn=<SqueezeBackward0>)\n",
      "tensor([500.3848, 589.6727, 568.9240, 558.3684], grad_fn=<SqueezeBackward0>)\n",
      "tensor([541.9596, 498.5020, 595.9401, 489.2149], grad_fn=<SqueezeBackward0>)\n",
      "tensor([591.3588, 511.8659, 447.1760, 617.5549], grad_fn=<SqueezeBackward0>)\n",
      "tensor([502.1277, 523.9449, 454.7057, 521.5280], grad_fn=<SqueezeBackward0>)\n",
      "tensor([546.2178, 546.0821, 562.5175, 562.3347], grad_fn=<SqueezeBackward0>)\n",
      "tensor([540.0336, 565.8164, 555.0394, 541.0946], grad_fn=<SqueezeBackward0>)\n",
      "tensor([569.6684, 578.4593, 541.9626, 523.9449], grad_fn=<SqueezeBackward0>)\n",
      "tensor([514.0335, 610.7415, 547.7467, 485.1847], grad_fn=<SqueezeBackward0>)\n",
      "tensor([555.6248, 567.5476, 624.1841, 594.0189], grad_fn=<SqueezeBackward0>)\n",
      "tensor([556.6384, 556.2451, 517.7053, 547.4279], grad_fn=<SqueezeBackward0>)\n",
      "tensor([496.7542, 493.8295, 599.4727, 557.4070], grad_fn=<SqueezeBackward0>)\n",
      "tensor([542.2646, 533.4889, 568.8801, 530.7310], grad_fn=<SqueezeBackward0>)\n",
      "tensor([512.7650, 571.4060, 551.2299, 582.3893], grad_fn=<SqueezeBackward0>)\n",
      "tensor([525.4470, 535.9703, 513.8627, 550.5955], grad_fn=<SqueezeBackward0>)\n",
      "tensor([557.6228, 604.5018, 495.8286, 547.5511], grad_fn=<SqueezeBackward0>)\n",
      "tensor([572.6329, 565.0325, 504.0648, 560.8732], grad_fn=<SqueezeBackward0>)\n",
      "tensor([590.3050, 570.0065, 604.9311, 540.3665], grad_fn=<SqueezeBackward0>)\n",
      "tensor([520.6179, 552.1926, 517.8477, 540.3274], grad_fn=<SqueezeBackward0>)\n",
      "tensor([539.6074, 612.7164, 497.6971, 573.1851], grad_fn=<SqueezeBackward0>)\n",
      "tensor([586.8845, 531.7391, 619.4980, 597.5714], grad_fn=<SqueezeBackward0>)\n",
      "tensor([561.0129, 575.0952, 529.2142, 465.9272], grad_fn=<SqueezeBackward0>)\n",
      "tensor([563.9423, 551.5226, 598.2634, 575.7145], grad_fn=<SqueezeBackward0>)\n",
      "tensor([558.5394, 569.9825, 438.9939, 588.3529], grad_fn=<SqueezeBackward0>)\n",
      "tensor([531.8671, 568.8498, 580.2112, 589.3575], grad_fn=<SqueezeBackward0>)\n",
      "tensor([517.0686, 566.0805, 575.2729, 554.3708], grad_fn=<SqueezeBackward0>)\n",
      "tensor([604.4391, 557.3675, 611.4933, 619.0342], grad_fn=<SqueezeBackward0>)\n",
      "tensor([569.7443, 487.1372, 579.3152, 580.4245], grad_fn=<SqueezeBackward0>)\n",
      "tensor([540.9753, 592.4833, 527.5062, 573.2054], grad_fn=<SqueezeBackward0>)\n",
      "tensor([552.2049, 471.6786, 569.9340, 638.6888], grad_fn=<SqueezeBackward0>)\n",
      "tensor([602.3118, 571.0491, 597.8975, 564.0232], grad_fn=<SqueezeBackward0>)\n",
      "tensor([642.4534, 610.4177, 549.0966, 532.6241], grad_fn=<SqueezeBackward0>)\n",
      "tensor([625.6476, 557.6524, 564.3661, 543.0981], grad_fn=<SqueezeBackward0>)\n",
      "tensor([595.2841, 624.6820, 510.5481, 545.9879], grad_fn=<SqueezeBackward0>)\n",
      "tensor([598.7895, 534.0229, 581.8514, 533.7095], grad_fn=<SqueezeBackward0>)\n",
      "tensor([592.6918, 520.7482, 545.8380, 600.9312], grad_fn=<SqueezeBackward0>)\n",
      "tensor([607.4184, 500.2932, 616.0413, 641.0914], grad_fn=<SqueezeBackward0>)\n",
      "tensor([598.7772, 533.8424, 586.5444, 573.9423], grad_fn=<SqueezeBackward0>)\n",
      "tensor([563.7515, 619.9546, 594.3129, 594.7842], grad_fn=<SqueezeBackward0>)\n",
      "tensor([599.3362, 592.6282, 554.6111, 533.1792], grad_fn=<SqueezeBackward0>)\n",
      "tensor([573.0482, 550.5683, 594.9219, 556.3945], grad_fn=<SqueezeBackward0>)\n",
      "tensor([485.2771, 618.6570, 606.9118, 659.7687], grad_fn=<SqueezeBackward0>)\n",
      "tensor([553.7249, 582.9932, 592.5760, 570.6016], grad_fn=<SqueezeBackward0>)\n",
      "tensor([630.3014, 621.1019, 636.7120, 626.8761], grad_fn=<SqueezeBackward0>)\n",
      "tensor([506.0363, 593.2881, 587.2851, 609.6043], grad_fn=<SqueezeBackward0>)\n",
      "tensor([602.3075, 596.6507, 564.0928, 588.6137], grad_fn=<SqueezeBackward0>)\n",
      "tensor([589.8514, 615.5746, 598.7278, 608.3018], grad_fn=<SqueezeBackward0>)\n",
      "tensor([525.5775, 561.2233, 592.3812, 541.7812], grad_fn=<SqueezeBackward0>)\n",
      "tensor([579.4484, 597.9930, 636.1813, 570.7812], grad_fn=<SqueezeBackward0>)\n",
      "tensor([623.8566, 568.9079, 580.3845, 565.8608], grad_fn=<SqueezeBackward0>)\n",
      "tensor([653.3452, 575.6138, 606.5447, 614.1826], grad_fn=<SqueezeBackward0>)\n",
      "tensor([613.9026, 597.2330, 661.6247, 591.7778], grad_fn=<SqueezeBackward0>)\n",
      "tensor([638.2337, 618.4476, 593.5352, 593.5618], grad_fn=<SqueezeBackward0>)\n",
      "tensor([591.1330, 665.1579, 549.3300, 591.2639], grad_fn=<SqueezeBackward0>)\n",
      "tensor([584.3728, 613.7468, 624.6003, 597.9285], grad_fn=<SqueezeBackward0>)\n",
      "tensor([596.5035, 564.0137, 596.1198, 589.9147], grad_fn=<SqueezeBackward0>)\n",
      "tensor(558.3109, grad_fn=<SqueezeBackward0>)\n",
      "Epoch 9, Loss: 1.5829434339505364e+16\n",
      "tensor([607.9964, 665.3497, 619.0063, 550.1042], grad_fn=<SqueezeBackward0>)\n",
      "tensor([554.8752, 599.5655, 597.0629, 661.7788], grad_fn=<SqueezeBackward0>)\n",
      "tensor([679.8749, 543.8130, 597.4304, 610.2601], grad_fn=<SqueezeBackward0>)\n",
      "tensor([632.9766, 559.6472, 603.4468, 537.0375], grad_fn=<SqueezeBackward0>)\n",
      "tensor([619.2136, 614.1171, 597.8412, 634.4637], grad_fn=<SqueezeBackward0>)\n",
      "tensor([585.0858, 567.1631, 502.9261, 642.3721], grad_fn=<SqueezeBackward0>)\n",
      "tensor([631.2466, 657.6204, 620.0572, 627.7281], grad_fn=<SqueezeBackward0>)\n",
      "tensor([632.2366, 616.3384, 606.9019, 557.4028], grad_fn=<SqueezeBackward0>)\n",
      "tensor([571.2400, 600.2844, 641.5575, 540.1845], grad_fn=<SqueezeBackward0>)\n",
      "tensor([593.3522, 551.5928, 688.2811, 533.7456], grad_fn=<SqueezeBackward0>)\n",
      "tensor([678.7094, 577.2811, 538.6976, 615.2955], grad_fn=<SqueezeBackward0>)\n",
      "tensor([663.0961, 617.5503, 634.3602, 658.8033], grad_fn=<SqueezeBackward0>)\n",
      "tensor([664.5886, 602.6469, 605.4623, 644.0704], grad_fn=<SqueezeBackward0>)\n",
      "tensor([632.4944, 610.9885, 630.8531, 565.7584], grad_fn=<SqueezeBackward0>)\n",
      "tensor([680.9354, 602.1905, 610.1669, 629.0576], grad_fn=<SqueezeBackward0>)\n",
      "tensor([629.7792, 587.0134, 580.9733, 525.4007], grad_fn=<SqueezeBackward0>)\n",
      "tensor([595.1766, 687.2319, 621.5306, 603.0767], grad_fn=<SqueezeBackward0>)\n",
      "tensor([586.9270, 659.2735, 594.8046, 711.5083], grad_fn=<SqueezeBackward0>)\n",
      "tensor([633.4734, 634.1378, 606.7573, 635.5176], grad_fn=<SqueezeBackward0>)\n",
      "tensor([641.7614, 658.0209, 593.9335, 593.8619], grad_fn=<SqueezeBackward0>)\n",
      "tensor([669.3848, 649.6385, 594.8879, 619.6031], grad_fn=<SqueezeBackward0>)\n",
      "tensor([556.3297, 630.4154, 587.5669, 599.6832], grad_fn=<SqueezeBackward0>)\n",
      "tensor([635.1578, 626.6035, 655.4849, 559.9865], grad_fn=<SqueezeBackward0>)\n",
      "tensor([599.0065, 668.6930, 660.7609, 635.5082], grad_fn=<SqueezeBackward0>)\n",
      "tensor([678.1013, 624.2294, 635.1956, 626.6811], grad_fn=<SqueezeBackward0>)\n",
      "tensor([594.8742, 570.0942, 608.0873, 683.9508], grad_fn=<SqueezeBackward0>)\n",
      "tensor([641.0720, 593.1484, 680.4555, 612.9523], grad_fn=<SqueezeBackward0>)\n",
      "tensor([580.2693, 595.5493, 639.2248, 602.6648], grad_fn=<SqueezeBackward0>)\n",
      "tensor([650.6978, 585.8032, 630.2274, 544.1608], grad_fn=<SqueezeBackward0>)\n",
      "tensor([688.3331, 519.4661, 558.3577, 651.9421], grad_fn=<SqueezeBackward0>)\n",
      "tensor([628.6210, 583.8102, 607.2046, 616.7683], grad_fn=<SqueezeBackward0>)\n",
      "tensor([627.9501, 490.7370, 520.7792, 661.6813], grad_fn=<SqueezeBackward0>)\n",
      "tensor([627.8320, 617.1877, 660.3916, 625.3945], grad_fn=<SqueezeBackward0>)\n",
      "tensor([619.8893, 677.7251, 637.7872, 658.4859], grad_fn=<SqueezeBackward0>)\n",
      "tensor([622.8885, 639.6268, 639.1600, 648.2264], grad_fn=<SqueezeBackward0>)\n",
      "tensor([573.4343, 670.8693, 627.8304, 687.7951], grad_fn=<SqueezeBackward0>)\n",
      "tensor([648.9452, 656.1769, 632.6518, 627.9213], grad_fn=<SqueezeBackward0>)\n",
      "tensor([666.9195, 597.4249, 599.4299, 653.3564], grad_fn=<SqueezeBackward0>)\n",
      "tensor([716.5361, 685.0312, 630.5166, 645.8741], grad_fn=<SqueezeBackward0>)\n",
      "tensor([632.7745, 639.3656, 664.8129, 665.5162], grad_fn=<SqueezeBackward0>)\n",
      "tensor([636.2101, 626.2831, 633.1578, 649.9291], grad_fn=<SqueezeBackward0>)\n",
      "tensor([610.9662, 594.6887, 661.2422, 622.7178], grad_fn=<SqueezeBackward0>)\n",
      "tensor([604.5513, 642.1658, 619.1113, 614.6204], grad_fn=<SqueezeBackward0>)\n",
      "tensor([532.9677, 661.3835, 640.2035, 609.2003], grad_fn=<SqueezeBackward0>)\n",
      "tensor([678.3080, 643.1024, 613.9258, 646.5519], grad_fn=<SqueezeBackward0>)\n",
      "tensor([673.6721, 617.7478, 651.2648, 585.2174], grad_fn=<SqueezeBackward0>)\n",
      "tensor([658.8958, 647.9344, 670.1536, 665.3767], grad_fn=<SqueezeBackward0>)\n",
      "tensor([698.3574, 596.6055, 653.8688, 660.8938], grad_fn=<SqueezeBackward0>)\n",
      "tensor([597.7795, 658.9086, 697.7547, 646.2819], grad_fn=<SqueezeBackward0>)\n",
      "tensor([596.0541, 597.2834, 631.1178, 615.3400], grad_fn=<SqueezeBackward0>)\n",
      "tensor([674.5129, 638.1841, 652.9442, 697.1185], grad_fn=<SqueezeBackward0>)\n",
      "tensor([626.4867, 671.5480, 595.7830, 612.6640], grad_fn=<SqueezeBackward0>)\n",
      "tensor([675.4108, 624.3245, 705.2564, 654.4302], grad_fn=<SqueezeBackward0>)\n",
      "tensor([651.5164, 706.6011, 599.7812, 642.4236], grad_fn=<SqueezeBackward0>)\n",
      "tensor([651.5048, 695.6032, 653.4241, 725.6176], grad_fn=<SqueezeBackward0>)\n",
      "tensor([657.8301, 657.5234, 671.9315, 588.7249], grad_fn=<SqueezeBackward0>)\n",
      "tensor([704.0956, 628.1241, 634.8367, 628.8375], grad_fn=<SqueezeBackward0>)\n",
      "tensor([695.2061, 642.9550, 610.3594, 719.1637], grad_fn=<SqueezeBackward0>)\n",
      "tensor([609.3146, 701.0623, 662.4402, 585.7148], grad_fn=<SqueezeBackward0>)\n",
      "tensor([621.8889, 569.4435, 652.1532, 656.9929], grad_fn=<SqueezeBackward0>)\n",
      "tensor([644.5862, 679.9359, 545.9263, 736.8787], grad_fn=<SqueezeBackward0>)\n",
      "tensor([642.8055, 639.8290, 647.6249, 703.2515], grad_fn=<SqueezeBackward0>)\n",
      "tensor(625.7152, grad_fn=<SqueezeBackward0>)\n",
      "Epoch 10, Loss: 1.5956535176073282e+16\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "class CustomMultilinearModel(nn.Module):\n",
    "    def __init__(self, vocab_size=384, video_feature_size=2048):\n",
    "        super(CustomMultilinearModel, self).__init__()\n",
    "        self.W0 = nn.Parameter(torch.randn(1))  # Bias term\n",
    "        self.W1 = nn.Parameter(torch.randn(1))  # Scalar weight for Tcount\n",
    "        self.W2 = nn.Parameter(torch.randn(vocab_size, 1))  # 384x1 vector for Tvocab\n",
    "        self.W3 = nn.Parameter(torch.randn(video_feature_size, 1))  # 2048x1 vector for mean video feature vector\n",
    "\n",
    "    def forward(self, Tcount, Tvocab, Tvideo_mean):\n",
    "        # Tcount is scalar, Tvocab is 384x1, Tvideo_mean is 2048x1\n",
    "        Y = self.W0 + self.W1 * Tcount + torch.matmul(self.W2.T, Tvocab.T) + torch.matmul(self.W3.T, Tvideo_mean.T)\n",
    "        return Y.squeeze()  # Remove extra dimensions for scalar output\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, title_count, vocab_embeddings, video_features, view_count):\n",
    "        self.title_count = title_count\n",
    "        self.vocab_embeddings = vocab_embeddings\n",
    "        self.video_features = video_features\n",
    "        self.view_count = view_count\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.view_count)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.title_count[idx], self.vocab_embeddings[idx], self.video_features[idx], self.view_count[idx])\n",
    "    \n",
    "\n",
    "# Splitting dataset into training and testing\n",
    "def train_test_split_dataset(dataset, train_size=0.75):\n",
    "    train_len = int(len(dataset) * train_size)\n",
    "    test_len = len(dataset) - train_len\n",
    "    return random_split(dataset, lengths=[train_len, test_len])\n",
    "\n",
    "def custom_loss(outputs, targets, penalty_factor=1000):\n",
    "    \"\"\"\n",
    "    Custom loss function that penalizes negative predictions.\n",
    "\n",
    "    Parameters:\n",
    "    - outputs: model predictions\n",
    "    - targets: true values\n",
    "    - penalty_factor: factor to scale the penalty for negative predictions\n",
    "\n",
    "    Returns:\n",
    "    - loss: computed loss with penalty for negative predictions\n",
    "    \"\"\"\n",
    "    mse_loss = nn.MSELoss()(outputs, targets)\n",
    "    \n",
    "    # Calculate penalty for negative predictions\n",
    "    negative_penalty = (outputs < 0).float() * penalty_factor * torch.abs(outputs)\n",
    "    negative_penalty = negative_penalty.mean()\n",
    "\n",
    "    # Total loss is the sum of MSE loss and negative penalty\n",
    "    loss = mse_loss + negative_penalty\n",
    "    return loss\n",
    "\n",
    "\n",
    "# load data\n",
    "title_count = length_title_view_count['length_title']\n",
    "vocab_embeddings = vocab_embeds\n",
    "video_features = np.array(list(frames_features.values()))\n",
    "view_count = length_title_view_count['view count']\n",
    "\n",
    "\n",
    "\n",
    "# Initialize dataset and split\n",
    "dataset = VideoDataset(title_count, vocab_embeddings, video_features, view_count)\n",
    "train_dataset, test_dataset = train_test_split_dataset(dataset)\n",
    "\n",
    "# DataLoader for batching\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# Initialize model\n",
    "model = CustomMultilinearModel(vocab_size=384, video_feature_size=2048)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()    \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)  # Example scheduler\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        title_count, vocab_embed, video_mean, view_counts = data\n",
    "        # Tcount = torch.tensor([[len(v)] for v in vocab_embed])  # Example way to compute Tcount, adjust as necessary\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(title_count, vocab_embed.float(), video_mean.float())\n",
    "        print(outputs)\n",
    "        loss = custom_loss(outputs, view_counts.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    scheduler.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 679149.7024108777\n",
      "Epoch 2, Loss: 690460.1727592513\n",
      "Epoch 3, Loss: 678677.1711838483\n",
      "Epoch 4, Loss: 680456.4129970674\n",
      "Epoch 5, Loss: 681892.8185142552\n",
      "Epoch 6, Loss: 674556.7592470329\n",
      "Epoch 7, Loss: 671408.3002171961\n",
      "Epoch 8, Loss: 665712.8684883362\n",
      "Epoch 9, Loss: 668585.840153426\n",
      "Epoch 10, Loss: 660025.7221767232\n",
      "fc1_vocab.weight tensor([[-0.2044,  0.3948,  0.2704,  ..., -0.4264, -0.3995, -0.0298],\n",
      "        [-0.1620,  0.3443,  0.2024,  ..., -0.4036, -0.3879,  0.0218],\n",
      "        [-0.2005,  0.4309,  0.2791,  ..., -0.4189, -0.4005,  0.0195],\n",
      "        ...,\n",
      "        [-0.1793,  0.4403,  0.2743,  ..., -0.3543, -0.4035, -0.0376],\n",
      "        [-0.1873,  0.3906,  0.2469,  ..., -0.3803, -0.4459,  0.0060],\n",
      "        [-0.1804,  0.4078,  0.2659,  ..., -0.4216, -0.4418,  0.0076]])\n",
      "fc1_vocab.bias tensor([ 0.5170,  0.4965,  0.4823,  0.4815,  0.5070,  0.4219,  0.5179,  0.4279,\n",
      "         0.5279,  0.5010,  0.4854,  0.4440,  0.5206,  0.4377,  0.4514,  0.4623,\n",
      "         0.4774,  0.4640,  0.4448,  0.5024, -0.0356,  0.4920,  0.4419,  0.4941,\n",
      "         0.4352,  0.4324,  0.4298,  0.4703,  0.5046,  0.5161,  0.4490,  0.5197,\n",
      "         0.5075,  0.4584,  0.5200,  0.5123,  0.5222,  0.4866,  0.4415,  0.4369,\n",
      "         0.4460,  0.5060,  0.4887,  0.4837,  0.4659,  0.4631,  0.4662,  0.4931,\n",
      "         0.4537,  0.4292,  0.4428,  0.4867,  0.4995,  0.4244,  0.4256,  0.4441,\n",
      "         0.4372,  0.5165,  0.4431,  0.4460,  0.5219,  0.4421,  0.5008,  0.5100,\n",
      "         0.4562,  0.4540,  0.4663,  0.4055,  0.4710,  0.4370,  0.4774,  0.4871,\n",
      "         0.4650,  0.4527,  0.4686,  0.4962,  0.4432,  0.5020,  0.4344,  0.4711,\n",
      "         0.5219,  0.4463,  0.4342,  0.4605,  0.5025,  0.4977,  0.4502,  0.4272,\n",
      "         0.4640,  0.4989,  0.3889,  0.4588,  0.4299,  0.5018,  0.4900,  0.4713,\n",
      "         0.4591,  0.4429,  0.5021,  0.5173,  0.5101,  0.4389,  0.4438,  0.4247,\n",
      "         0.4538,  0.4786,  0.5168,  0.4883,  0.4790,  0.4218,  0.4135,  0.4714,\n",
      "         0.4713,  0.4959,  0.4378,  0.4778,  0.4341,  0.4745,  0.4783,  0.4841,\n",
      "         0.4956,  0.4871,  0.5146,  0.5173,  0.5066,  0.5109,  0.4483,  0.4661,\n",
      "         0.4935,  0.5090,  0.4296,  0.5182,  0.4546,  0.4547,  0.4941,  0.4322,\n",
      "         0.4953,  0.4317,  0.4586,  0.4775,  0.4646,  0.4594,  0.4584,  0.4288,\n",
      "         0.4341,  0.5242,  0.4767,  0.5168,  0.4600,  0.4618,  0.4744,  0.4697,\n",
      "         0.4584,  0.4634,  0.4434,  0.4455,  0.4523,  0.4765,  0.4531,  0.4378,\n",
      "         0.4863,  0.4398,  0.4126,  0.5199,  0.4311,  0.4359,  0.4735,  0.4460,\n",
      "         0.4457,  0.4599,  0.4330,  0.4623,  0.5227,  0.5021,  0.5224,  0.4560,\n",
      "         0.5074,  0.5139,  0.4812,  0.4976,  0.4220,  0.4588,  0.4905,  0.4562,\n",
      "         0.4124,  0.4389,  0.5178,  0.4686,  0.5051,  0.4750,  0.5103,  0.4662,\n",
      "         0.4621,  0.4827,  0.4416,  0.5002,  0.4298,  0.4331,  0.4896,  0.4504,\n",
      "         0.4725,  0.4872,  0.5144,  0.4261,  0.4430,  0.4569,  0.5115,  0.5136,\n",
      "         0.4757,  0.4539,  0.5088,  0.4805,  0.4423,  0.4375,  0.5069,  0.4933,\n",
      "         0.5160,  0.5141,  0.4790,  0.4547,  0.5177,  0.4570,  0.4528,  0.4898,\n",
      "         0.4681,  0.4473,  0.5068,  0.4685,  0.4323,  0.5027,  0.4296,  0.5079,\n",
      "         0.4244,  0.4599,  0.4757,  0.4974,  0.4535,  0.4412,  0.4586,  0.4946,\n",
      "         0.4594,  0.4243,  0.4930,  0.4198,  0.5206,  0.4602,  0.4313,  0.4990,\n",
      "         0.4438,  0.4977,  0.4321,  0.4767,  0.5177,  0.4298,  0.4910,  0.4422,\n",
      "         0.4588,  0.4732,  0.4403,  0.4334,  0.4677,  0.4189,  0.4268,  0.4740,\n",
      "         0.4601,  0.5046,  0.4734,  0.5314,  0.4574,  0.4191,  0.4630,  0.4797,\n",
      "         0.4811,  0.5026,  0.4500,  0.4506,  0.4501,  0.4904,  0.4811,  0.4988,\n",
      "         0.4565,  0.4976,  0.4847,  0.4999,  0.4353,  0.4547,  0.4545,  0.4502,\n",
      "         0.4644,  0.4701,  0.5002,  0.4566,  0.4511,  0.4613,  0.4832,  0.4459,\n",
      "         0.4830,  0.4944,  0.4524,  0.5246,  0.4851,  0.4378,  0.4650,  0.5157,\n",
      "         0.4475,  0.5038,  0.4592,  0.4617,  0.5010,  0.4920,  0.5207,  0.4548,\n",
      "         0.5162,  0.5026,  0.4721,  0.5194,  0.4658,  0.4551,  0.4345,  0.4449,\n",
      "         0.4628,  0.4919,  0.5029,  0.4694,  0.4299,  0.5151,  0.5078,  0.4290,\n",
      "         0.4327,  0.4460,  0.4897,  0.4278,  0.4789,  0.4684,  0.5206,  0.5204,\n",
      "         0.5004,  0.4769,  0.4841,  0.4294,  0.4670,  0.4828,  0.4347,  0.4660,\n",
      "         0.5043,  0.4661,  0.4819,  0.4304,  0.4747,  0.4693,  0.4675,  0.4385,\n",
      "         0.4310,  0.4836,  0.4824,  0.3977,  0.4322,  0.4937,  0.4696,  0.4845,\n",
      "         0.4673,  0.4460,  0.4956,  0.4417,  0.5005,  0.4612,  0.5178,  0.4110,\n",
      "         0.4931,  0.4708,  0.5233,  0.4676,  0.4539,  0.4588,  0.4381,  0.5124,\n",
      "         0.4567,  0.5223,  0.5213,  0.4683,  0.4667,  0.4996,  0.4846,  0.4817,\n",
      "         0.5131,  0.4683,  0.4291,  0.5087,  0.4584,  0.4491,  0.5040,  0.4718,\n",
      "         0.4510,  0.4781,  0.4947,  0.4456,  0.4750,  0.4624,  0.4857,  0.4466,\n",
      "         0.4996,  0.4488,  0.4830,  0.4549,  0.5180,  0.5045,  0.4355,  0.5022,\n",
      "         0.4335,  0.4700,  0.4790,  0.4716, -0.0449,  0.5258,  0.5064,  0.4503,\n",
      "         0.4578,  0.5175,  0.5005,  0.4782,  0.4185,  0.4993,  0.4509,  0.4352,\n",
      "         0.4999,  0.4788,  0.4194,  0.4334,  0.4585,  0.4463,  0.5059,  0.4688,\n",
      "         0.5111,  0.4921,  0.5228,  0.4966,  0.5071,  0.5030,  0.5023,  0.4280,\n",
      "         0.4800,  0.4820,  0.4660,  0.4583,  0.4999,  0.4924,  0.4603,  0.5009,\n",
      "         0.4287,  0.4587,  0.5010,  0.4509,  0.4601,  0.4723,  0.4838,  0.5012,\n",
      "         0.4806,  0.4362,  0.4839,  0.4420,  0.4630,  0.4483,  0.4346,  0.4478,\n",
      "         0.4723,  0.4692,  0.4851,  0.4985,  0.4947,  0.4511,  0.4724,  0.5328,\n",
      "         0.4492,  0.5135,  0.4252,  0.4079,  0.5070,  0.4622,  0.4722,  0.4687,\n",
      "         0.5033,  0.4635,  0.4698,  0.4787,  0.4386,  0.4204,  0.4949,  0.5131,\n",
      "         0.4594,  0.5217,  0.4264,  0.4705,  0.5172,  0.4747,  0.4905,  0.4739,\n",
      "         0.4847,  0.5027,  0.4964,  0.4570,  0.5167,  0.5050,  0.4320,  0.5065,\n",
      "         0.4353,  0.4720,  0.4732, -0.0365,  0.5079,  0.4626,  0.4222,  0.5012])\n",
      "fc1_video.weight tensor([[-0.0028, -0.0027, -0.0020,  ..., -0.0040, -0.0031, -0.0030],\n",
      "        [ 0.4617,  0.4638,  0.4690,  ...,  0.4707,  0.4535,  0.4884],\n",
      "        [ 0.4810,  0.4653,  0.4727,  ...,  0.4419,  0.4745,  0.4667],\n",
      "        ...,\n",
      "        [ 0.0087,  0.0152, -0.0135,  ..., -0.0231, -0.0196, -0.0163],\n",
      "        [ 0.4832,  0.4628,  0.4541,  ...,  0.4743,  0.4503,  0.4916],\n",
      "        [ 0.4681,  0.4639,  0.4822,  ...,  0.4541,  0.4617,  0.4527]])\n",
      "fc1_video.bias tensor([-3.1289e-03,  4.6976e-01,  4.9642e-01,  4.6487e-01,  4.7299e-01,\n",
      "         4.6681e-01,  4.8256e-01, -1.6399e-03, -2.1349e-02, -1.2466e-02,\n",
      "         1.4556e-03,  9.1274e-03,  4.6748e-01,  4.6106e-01, -1.6300e-03,\n",
      "        -1.7185e-03,  4.8456e-01,  1.0278e-02,  4.5890e-03,  4.6404e-01,\n",
      "         4.8327e-01,  6.8987e-03,  8.1001e-03, -1.6088e-02,  1.5970e-02,\n",
      "         4.9489e-01,  4.5828e-01, -3.0697e-03, -2.1360e-02, -1.9463e-03,\n",
      "         4.6465e-01,  4.7746e-01, -4.3803e-03,  3.9556e-03,  4.6868e-01,\n",
      "         4.6978e-01,  4.7766e-01,  4.6771e-01,  4.6347e-01,  9.2002e-03,\n",
      "        -2.6793e-03,  4.8098e-01, -1.9914e-02,  4.5745e-01,  4.6500e-01,\n",
      "         4.6386e-01, -1.0060e-02,  4.6885e-01, -1.8003e-02, -1.8244e-02,\n",
      "         4.6289e-01,  4.6946e-01,  4.6609e-01,  4.7639e-01, -1.9619e-03,\n",
      "        -1.4765e-02, -1.6868e-02,  4.6433e-01, -3.8889e-03,  4.7016e-01,\n",
      "        -5.3129e-03,  1.9523e-02,  9.6076e-03,  4.5785e-03, -1.8913e-02,\n",
      "         4.8556e-01,  4.6058e-01,  4.6904e-01,  6.8660e-03,  1.8340e-03,\n",
      "         4.7571e-01,  4.8613e-01,  1.8714e-03,  4.8185e-01,  4.7113e-01,\n",
      "         4.8680e-01,  6.3002e-03,  4.8257e-01, -1.7605e-02,  9.6274e-03,\n",
      "        -7.3798e-03,  1.8290e-02,  1.1986e-02,  4.5787e-01,  4.6309e-01,\n",
      "         4.7779e-01, -1.3746e-02,  4.8823e-01,  4.6462e-01, -1.2036e-02,\n",
      "         4.7464e-01, -2.6936e-03, -1.0881e-02, -3.1942e-03, -9.8835e-03,\n",
      "        -4.7451e-03,  4.8936e-01, -8.1166e-03,  4.6686e-01,  1.0723e-02,\n",
      "         4.9435e-01,  4.8962e-01,  3.7163e-03, -2.4066e-02, -1.9653e-02,\n",
      "         4.8120e-01, -2.1950e-02,  4.8454e-01,  2.0507e-03, -3.4070e-03,\n",
      "        -1.5741e-02, -8.5640e-03, -5.8009e-03,  4.5856e-01,  6.8622e-03,\n",
      "        -2.2778e-02, -2.7259e-03,  4.9186e-01,  4.6050e-01,  4.8300e-01,\n",
      "        -6.8465e-03, -2.9559e-03, -2.5391e-03,  4.6711e-01,  4.7988e-01,\n",
      "        -1.4466e-02, -1.6631e-02, -2.4630e-03,  4.7571e-01,  4.6496e-01,\n",
      "         4.8201e-01, -1.3643e-03,  4.5544e-01, -2.1049e-02,  4.9193e-01,\n",
      "         4.5894e-01,  4.6904e-01,  4.6324e-01,  5.5778e-03,  4.9365e-01,\n",
      "        -1.0155e-02,  4.6326e-01, -1.0086e-02,  4.7155e-01,  4.6671e-01,\n",
      "        -1.6641e-02, -1.0223e-02,  1.8311e-02,  4.6450e-01,  4.8017e-01,\n",
      "        -2.4051e-03,  4.5989e-01,  1.7843e-02,  4.8305e-01,  1.5811e-02,\n",
      "         1.4504e-02, -1.0359e-04, -7.1145e-03,  4.8477e-01,  4.9447e-01,\n",
      "         4.7214e-01,  4.8767e-01,  4.8136e-01,  4.5534e-01,  4.7192e-01,\n",
      "         4.6467e-01, -2.6383e-03,  4.6433e-01, -9.4681e-03, -2.8438e-03,\n",
      "         4.8889e-01,  5.2675e-03, -2.5027e-03,  1.7917e-02, -3.1289e-03,\n",
      "         1.6967e-02, -6.5167e-03,  1.8880e-02,  4.8282e-01,  6.2237e-03,\n",
      "         4.8071e-01,  4.8530e-01,  4.6822e-01, -1.8825e-02,  4.8575e-01,\n",
      "         4.8192e-01,  6.4056e-03,  4.7575e-01,  1.4593e-03, -6.0884e-03,\n",
      "        -2.6283e-03, -3.7622e-03, -3.2412e-03,  4.7276e-01, -2.0862e-02,\n",
      "         4.7806e-01,  4.6425e-01,  4.5645e-01,  4.8029e-01,  4.6971e-01,\n",
      "         4.7250e-01, -1.0150e-02,  4.6757e-01,  4.6477e-01,  4.6405e-01,\n",
      "         4.5904e-01,  4.9392e-01, -2.9617e-03,  4.7808e-01,  1.2527e-02,\n",
      "        -1.0450e-02, -1.9202e-02, -1.3605e-02,  4.7462e-01,  4.8887e-01,\n",
      "        -1.3566e-02, -2.2861e-02,  4.9011e-01,  4.6524e-01, -8.1319e-03,\n",
      "        -1.2985e-02,  4.7862e-01,  4.9053e-01,  6.3256e-03,  4.7683e-01,\n",
      "        -2.9061e-03,  9.3997e-03,  4.8037e-01,  4.9614e-01,  4.7803e-01,\n",
      "         4.5480e-01, -1.2161e-02, -6.0566e-03,  4.7411e-01,  4.8130e-01,\n",
      "         4.8449e-03,  4.6679e-01, -2.0936e-02,  4.7681e-01,  6.0533e-03,\n",
      "         4.7091e-01,  9.9253e-05,  1.0220e-03, -1.0111e-02, -1.7827e-02,\n",
      "         2.0651e-03, -1.9699e-02,  4.7355e-01,  5.7589e-03,  4.8059e-01,\n",
      "        -4.1267e-04,  1.8650e-02,  4.8409e-01,  4.6721e-01,  4.7919e-01,\n",
      "        -2.9741e-03,  4.6506e-01,  6.2301e-03,  1.7414e-02,  4.9710e-01,\n",
      "         4.6010e-01,  4.6364e-01, -1.8130e-02, -9.1144e-03,  1.5985e-02,\n",
      "         8.9394e-03,  4.6817e-01,  4.7443e-01,  4.8126e-01, -1.0869e-02,\n",
      "         1.9769e-02,  4.7229e-01,  4.9676e-01,  4.8621e-01, -8.6810e-03,\n",
      "         4.6289e-01,  1.1356e-03,  2.1521e-03,  4.5285e-01,  4.7006e-01,\n",
      "        -1.3092e-04, -1.6849e-02,  4.5551e-01,  4.7656e-01, -3.1370e-03,\n",
      "         4.7408e-01, -1.7388e-02,  4.9047e-01,  1.3612e-02,  4.8221e-03,\n",
      "         4.7118e-01,  4.5558e-01, -7.3761e-03,  4.7607e-01,  7.4411e-03,\n",
      "         4.9769e-01,  5.1476e-03, -5.4992e-03,  4.6600e-01,  4.8215e-01,\n",
      "        -2.4007e-03,  4.8098e-01,  4.7335e-01,  4.9445e-01,  4.8262e-01,\n",
      "         4.6871e-01, -1.3682e-02, -4.4346e-04,  4.5573e-01,  4.8067e-01,\n",
      "        -2.8856e-03,  4.6792e-01,  1.2149e-02,  4.5859e-01,  4.8817e-01,\n",
      "         4.8301e-01,  4.6322e-01,  4.7825e-01,  4.7230e-01, -5.4645e-03,\n",
      "        -1.5992e-02, -6.3164e-04, -2.0236e-03,  4.9105e-01,  4.6293e-01,\n",
      "         4.8907e-01,  2.2094e-03, -1.9630e-02,  4.9097e-01,  4.5685e-01,\n",
      "        -3.1884e-03, -2.3278e-03,  4.7125e-01,  4.6107e-01, -2.3922e-03,\n",
      "         4.5624e-01, -1.5850e-02,  4.7438e-01,  4.7243e-01,  6.3924e-05,\n",
      "         4.9141e-01,  4.7344e-01,  4.6480e-01,  1.6629e-02, -2.0849e-03,\n",
      "         4.6435e-01,  2.7096e-03, -5.8466e-03,  4.6132e-01,  4.6786e-01,\n",
      "         4.5308e-01, -3.5539e-03,  4.6552e-01,  4.8255e-01,  4.7631e-01,\n",
      "         4.7888e-01,  4.6938e-01,  4.5658e-03, -1.6669e-02,  4.6969e-01,\n",
      "        -2.0984e-02,  1.1193e-02,  4.9477e-01,  4.8176e-01,  1.6531e-02,\n",
      "         4.5419e-01,  1.5586e-02,  4.6463e-01,  4.8812e-01,  2.2837e-03,\n",
      "         4.6457e-01,  3.5716e-03,  1.2159e-02, -2.5156e-03, -7.8804e-03,\n",
      "        -6.8461e-03, -2.0852e-03,  4.9487e-01,  4.9197e-01,  4.8576e-01,\n",
      "         4.8295e-01,  1.9292e-02,  4.7022e-01, -3.2314e-03,  4.5448e-01,\n",
      "        -1.6651e-02,  4.6236e-01, -8.0272e-03, -1.9568e-02,  4.8063e-01,\n",
      "        -2.7679e-03, -2.0766e-02,  1.5151e-02,  4.8373e-01, -3.1877e-03,\n",
      "        -3.1948e-03,  4.7043e-01, -2.8403e-03,  4.6273e-01,  4.7835e-01,\n",
      "         7.2226e-03,  4.3134e-03,  4.7019e-01, -8.1295e-03,  4.6655e-01,\n",
      "         4.7220e-01, -2.1844e-02,  4.7464e-01, -2.6095e-03,  4.9262e-01,\n",
      "        -2.3453e-03,  4.8584e-01, -3.1291e-03,  1.7027e-02,  6.1330e-03,\n",
      "         8.5562e-04, -1.4176e-02, -3.5772e-03,  1.3010e-02,  1.8561e-02,\n",
      "         4.6756e-01,  4.5704e-01,  1.5932e-02,  4.6614e-01, -1.0636e-02,\n",
      "        -4.6413e-03,  4.6810e-01,  4.6246e-01,  4.7276e-01,  4.6140e-01,\n",
      "        -4.0485e-03,  5.0019e-01, -2.1213e-02,  4.6584e-01,  4.6966e-01,\n",
      "         4.8164e-01, -2.3430e-03, -2.3310e-02,  4.6455e-01, -3.1502e-03,\n",
      "         4.8228e-01, -2.0972e-03,  1.7153e-02,  4.6734e-01, -2.1371e-03,\n",
      "        -2.1552e-03, -2.1228e-02,  4.6599e-01,  6.8365e-03, -4.3405e-03,\n",
      "         4.5551e-01,  1.4703e-02,  4.6543e-01,  4.6114e-01,  4.7578e-01,\n",
      "         1.0893e-03, -2.0978e-02,  1.5646e-03,  4.7159e-01,  4.6531e-01,\n",
      "         4.6312e-01,  4.8105e-01,  4.8940e-01,  4.9281e-01,  1.6221e-02,\n",
      "         4.6440e-01,  4.8556e-01, -2.3064e-02,  4.7144e-01,  4.6621e-01,\n",
      "        -1.4516e-03,  4.6805e-01,  4.6859e-01,  4.6346e-01,  4.7541e-01,\n",
      "        -1.0192e-02, -8.8578e-03,  4.9922e-01, -9.5482e-03,  4.7722e-01,\n",
      "         1.1737e-02,  4.6242e-01,  4.8430e-01, -2.7957e-03,  4.5996e-01,\n",
      "         4.8722e-01,  6.5807e-03, -1.5874e-02,  5.9340e-03,  4.7224e-01,\n",
      "        -1.2744e-02,  4.7320e-01,  4.6705e-01,  4.5783e-01,  4.8399e-01,\n",
      "         4.8361e-01, -1.6775e-02,  4.9503e-01, -1.8681e-02,  4.9792e-01,\n",
      "        -6.5005e-04, -2.3797e-02, -1.7813e-02,  3.5702e-03, -1.7834e-02,\n",
      "         4.6443e-01, -9.9397e-03,  4.9035e-01, -2.5696e-03,  5.5087e-03,\n",
      "         4.7828e-01,  4.6669e-01])\n",
      "fc1_title.weight tensor([[ 8.4360e-01],\n",
      "        [ 1.0701e-01],\n",
      "        [ 8.6675e-01],\n",
      "        [-3.9913e-05],\n",
      "        [ 7.5059e-01],\n",
      "        [ 7.4010e-01],\n",
      "        [-6.6931e-01],\n",
      "        [-3.1810e-02],\n",
      "        [ 8.1078e-01],\n",
      "        [ 7.5479e-01],\n",
      "        [-2.0069e-01],\n",
      "        [ 9.4507e-01],\n",
      "        [-2.9182e-02],\n",
      "        [-7.2285e-05],\n",
      "        [-6.6896e-01],\n",
      "        [-7.1427e-01],\n",
      "        [-2.3345e-01],\n",
      "        [-1.3204e-01],\n",
      "        [-6.5983e-01],\n",
      "        [-7.0707e-02],\n",
      "        [-1.2081e-05],\n",
      "        [-6.5230e-01],\n",
      "        [-1.3094e-01],\n",
      "        [ 1.2396e+00],\n",
      "        [-1.7668e-01],\n",
      "        [ 7.8607e-01],\n",
      "        [ 1.2656e+00],\n",
      "        [ 7.6238e-01],\n",
      "        [ 7.7149e-01],\n",
      "        [-1.2542e-01],\n",
      "        [ 1.2184e+00],\n",
      "        [-2.8784e-01],\n",
      "        [ 1.2821e+00],\n",
      "        [ 8.6147e-01],\n",
      "        [-2.5858e-01],\n",
      "        [ 1.2041e+00],\n",
      "        [ 1.0845e+00],\n",
      "        [-2.0660e-01],\n",
      "        [ 1.2827e+00],\n",
      "        [-1.4814e-05],\n",
      "        [-6.7965e-01],\n",
      "        [ 6.6315e-06],\n",
      "        [ 5.6519e-01],\n",
      "        [ 5.1319e-01],\n",
      "        [ 1.2817e+00],\n",
      "        [-1.8370e-01],\n",
      "        [-1.8078e-01],\n",
      "        [-6.7803e-01],\n",
      "        [ 1.2502e+00],\n",
      "        [ 3.2145e-01],\n",
      "        [ 8.4034e-01],\n",
      "        [ 3.1461e-01],\n",
      "        [ 1.2283e+00],\n",
      "        [ 8.8785e-01],\n",
      "        [ 5.8616e-01],\n",
      "        [-1.8277e-01],\n",
      "        [ 8.8575e-01],\n",
      "        [ 1.1801e+00],\n",
      "        [-9.4789e-02],\n",
      "        [ 7.4785e-01],\n",
      "        [ 1.2220e+00],\n",
      "        [-2.1657e-01],\n",
      "        [ 8.4448e-01],\n",
      "        [ 1.1196e+00],\n",
      "        [-2.6287e-01],\n",
      "        [ 8.9545e-01],\n",
      "        [ 5.2249e-01],\n",
      "        [ 1.0500e+00],\n",
      "        [ 1.3980e+00],\n",
      "        [ 7.7081e-01],\n",
      "        [-7.6141e-03],\n",
      "        [-7.1333e-01],\n",
      "        [ 1.2418e+00],\n",
      "        [ 1.3742e+00],\n",
      "        [-1.4983e-01],\n",
      "        [ 8.6879e-01],\n",
      "        [-3.4565e-01],\n",
      "        [ 9.4705e-01],\n",
      "        [-5.4358e-02],\n",
      "        [ 9.9227e-01],\n",
      "        [ 8.8208e-01],\n",
      "        [-3.1590e-01],\n",
      "        [-5.5503e-01],\n",
      "        [-5.9119e-01],\n",
      "        [ 1.2357e+00],\n",
      "        [-3.2477e-01],\n",
      "        [ 3.0749e-01],\n",
      "        [ 6.2987e-01],\n",
      "        [ 6.1380e-01],\n",
      "        [-3.2656e-05],\n",
      "        [ 9.7512e-01],\n",
      "        [ 8.7718e-01],\n",
      "        [ 5.1651e-05],\n",
      "        [ 8.2402e-01],\n",
      "        [-5.0188e-01],\n",
      "        [-6.6969e-01],\n",
      "        [-9.9983e-02],\n",
      "        [-2.5394e-01],\n",
      "        [ 9.6166e-01],\n",
      "        [ 9.4544e-01],\n",
      "        [ 9.9137e-01],\n",
      "        [-5.3548e-01],\n",
      "        [-6.7552e-01],\n",
      "        [ 1.3419e+00],\n",
      "        [-6.4104e-01],\n",
      "        [ 1.3304e+00],\n",
      "        [-6.9303e-01],\n",
      "        [-6.9059e-01],\n",
      "        [ 1.4007e+00],\n",
      "        [ 2.1685e-01],\n",
      "        [ 1.4126e+00],\n",
      "        [ 7.3033e-01],\n",
      "        [ 1.2764e+00],\n",
      "        [-2.5232e-01],\n",
      "        [ 1.2033e+00],\n",
      "        [-2.8321e-02],\n",
      "        [ 2.8508e-01],\n",
      "        [ 6.3203e-01],\n",
      "        [-3.4579e-01],\n",
      "        [ 1.0290e+00],\n",
      "        [-2.5922e-01],\n",
      "        [ 5.8132e-01],\n",
      "        [-5.4256e-01],\n",
      "        [-5.3773e-02],\n",
      "        [ 1.2688e+00],\n",
      "        [ 1.1868e+00],\n",
      "        [-6.7144e-03],\n",
      "        [ 1.1541e-01],\n",
      "        [ 7.0864e-01],\n",
      "        [-1.7116e-01],\n",
      "        [ 2.0895e-05],\n",
      "        [-3.2469e-01],\n",
      "        [ 9.8485e-01],\n",
      "        [ 1.0877e+00],\n",
      "        [ 8.7262e-01],\n",
      "        [-4.2788e-01],\n",
      "        [-1.7834e-01],\n",
      "        [-5.9676e-01],\n",
      "        [-3.4180e-01],\n",
      "        [-1.6170e-01],\n",
      "        [-1.3868e-01],\n",
      "        [-7.4242e-01],\n",
      "        [-3.8031e-01],\n",
      "        [-4.5119e-01],\n",
      "        [ 1.4245e+00],\n",
      "        [-8.0727e-05],\n",
      "        [ 3.9224e-01],\n",
      "        [ 1.3637e+00],\n",
      "        [ 1.1146e+00],\n",
      "        [-7.1901e-03],\n",
      "        [ 4.5004e-02],\n",
      "        [ 1.3911e+00],\n",
      "        [ 1.2159e+00],\n",
      "        [-7.7524e-03],\n",
      "        [ 9.0393e-01],\n",
      "        [-4.6819e-01],\n",
      "        [-4.4929e-01],\n",
      "        [ 4.4124e-06],\n",
      "        [ 3.8844e-01],\n",
      "        [ 1.3117e+00],\n",
      "        [ 1.0710e-05],\n",
      "        [ 9.1127e-01],\n",
      "        [-2.0284e-01],\n",
      "        [ 5.5067e-01],\n",
      "        [ 6.5419e-01],\n",
      "        [ 8.9314e-01],\n",
      "        [ 6.4560e-01],\n",
      "        [-9.4706e-03],\n",
      "        [-6.3886e-01],\n",
      "        [ 1.4480e+00],\n",
      "        [-9.0007e-02],\n",
      "        [-3.5941e-01],\n",
      "        [-5.8976e-01],\n",
      "        [ 1.0169e+00],\n",
      "        [ 8.1349e-01],\n",
      "        [ 1.3802e+00],\n",
      "        [-5.8570e-01],\n",
      "        [-6.1226e-03],\n",
      "        [ 1.9083e-05],\n",
      "        [ 1.1425e+00],\n",
      "        [ 4.3292e-05],\n",
      "        [ 1.0455e+00],\n",
      "        [ 6.7003e-01],\n",
      "        [ 1.1594e+00],\n",
      "        [ 5.8818e-01],\n",
      "        [-4.9872e-02],\n",
      "        [-4.9780e-01],\n",
      "        [-7.1570e-03],\n",
      "        [ 3.7277e-01],\n",
      "        [-4.4541e-01],\n",
      "        [ 5.4046e-01],\n",
      "        [ 5.1231e-01],\n",
      "        [ 1.0398e+00],\n",
      "        [ 4.4218e-01],\n",
      "        [ 2.7645e-01],\n",
      "        [ 9.6687e-01],\n",
      "        [ 6.4716e-01],\n",
      "        [-1.6077e-01],\n",
      "        [-5.0503e-01],\n",
      "        [ 3.5794e-01],\n",
      "        [ 1.3821e+00],\n",
      "        [ 2.7040e-06],\n",
      "        [-6.9893e-02],\n",
      "        [ 9.8329e-01],\n",
      "        [-3.7798e-01],\n",
      "        [ 8.9889e-01],\n",
      "        [ 6.7178e-01],\n",
      "        [-3.4165e-01],\n",
      "        [ 8.0369e-01],\n",
      "        [-1.9561e-05],\n",
      "        [ 1.1382e+00],\n",
      "        [-7.0992e-01],\n",
      "        [-6.5042e-01],\n",
      "        [ 1.1039e+00],\n",
      "        [ 3.1556e-01],\n",
      "        [ 6.4587e-01],\n",
      "        [-3.2391e-01],\n",
      "        [ 1.1919e+00],\n",
      "        [-1.1862e-01],\n",
      "        [-8.3217e-03],\n",
      "        [ 1.1195e-01],\n",
      "        [ 1.0392e+00],\n",
      "        [-2.4475e-02],\n",
      "        [ 1.1799e+00],\n",
      "        [-2.4878e-01],\n",
      "        [-2.1734e-02],\n",
      "        [ 5.5267e-02],\n",
      "        [-1.5396e-01],\n",
      "        [-1.6186e-05],\n",
      "        [ 8.5151e-01],\n",
      "        [ 8.6456e-01],\n",
      "        [ 9.2606e-01],\n",
      "        [-7.4811e-01],\n",
      "        [ 1.0823e+00],\n",
      "        [-1.0425e-02],\n",
      "        [ 5.8287e-01],\n",
      "        [-2.9682e-01],\n",
      "        [ 1.2229e+00],\n",
      "        [-3.8667e-01],\n",
      "        [-3.4860e-01],\n",
      "        [-2.7414e-01],\n",
      "        [ 6.7403e-01],\n",
      "        [ 7.1064e-01],\n",
      "        [-4.8181e-01],\n",
      "        [ 7.4184e-01],\n",
      "        [-1.6826e-01],\n",
      "        [ 1.1955e-01],\n",
      "        [ 1.2573e+00],\n",
      "        [ 1.2144e+00],\n",
      "        [-8.3261e-03],\n",
      "        [-2.6134e-01],\n",
      "        [ 4.1261e-01],\n",
      "        [ 1.4452e+00],\n",
      "        [ 8.4371e-01],\n",
      "        [ 8.3869e-01],\n",
      "        [-4.4151e-02],\n",
      "        [ 7.8148e-01],\n",
      "        [-1.8310e-01],\n",
      "        [ 1.2974e+00],\n",
      "        [ 1.2043e+00],\n",
      "        [-6.3018e-01],\n",
      "        [-3.3424e-01],\n",
      "        [ 9.3616e-01],\n",
      "        [-7.3012e-01],\n",
      "        [ 1.2366e+00],\n",
      "        [ 1.0297e+00],\n",
      "        [ 1.1652e+00],\n",
      "        [-1.1421e-02],\n",
      "        [ 1.4605e+00],\n",
      "        [ 1.3722e+00],\n",
      "        [ 5.9193e-01],\n",
      "        [ 1.1243e+00],\n",
      "        [-5.3716e-01],\n",
      "        [-4.8795e-05],\n",
      "        [ 1.1269e+00],\n",
      "        [ 1.4480e+00],\n",
      "        [-5.8700e-01],\n",
      "        [ 1.1770e+00],\n",
      "        [-3.7469e-01],\n",
      "        [-5.5582e-01],\n",
      "        [ 7.6384e-01],\n",
      "        [ 4.9761e-01],\n",
      "        [ 9.4336e-01],\n",
      "        [ 5.5568e-01],\n",
      "        [ 7.1475e-01],\n",
      "        [ 6.1901e-01],\n",
      "        [ 1.2378e+00],\n",
      "        [-2.8379e-01],\n",
      "        [ 3.4946e-01],\n",
      "        [ 1.1206e+00],\n",
      "        [ 1.0548e+00],\n",
      "        [-8.5073e-02],\n",
      "        [ 4.0611e-01],\n",
      "        [ 4.2046e-05],\n",
      "        [ 5.4295e-01],\n",
      "        [ 6.7997e-01],\n",
      "        [-2.8393e-01],\n",
      "        [ 1.2165e+00],\n",
      "        [-1.2399e-01],\n",
      "        [ 1.4416e+00],\n",
      "        [ 5.5236e-01],\n",
      "        [ 7.7332e-01],\n",
      "        [-8.0454e-03],\n",
      "        [ 1.0510e+00],\n",
      "        [ 1.1837e+00],\n",
      "        [ 1.3897e+00],\n",
      "        [-5.7308e-01],\n",
      "        [-6.1895e-01],\n",
      "        [-1.8332e-01],\n",
      "        [ 7.8993e-01],\n",
      "        [-7.4225e-01],\n",
      "        [-1.6667e-01],\n",
      "        [-4.1226e-04],\n",
      "        [ 6.4137e-01],\n",
      "        [ 8.2783e-01],\n",
      "        [-1.1063e-01],\n",
      "        [ 7.3070e-01],\n",
      "        [ 1.4044e+00],\n",
      "        [ 1.2352e+00],\n",
      "        [ 9.8106e-01],\n",
      "        [ 1.2635e+00],\n",
      "        [-1.3756e-01],\n",
      "        [ 1.2523e+00],\n",
      "        [ 5.2319e-01],\n",
      "        [ 7.5239e-01],\n",
      "        [ 4.7272e-01],\n",
      "        [ 3.7724e-01],\n",
      "        [ 7.3953e-01],\n",
      "        [-2.0140e-01],\n",
      "        [-4.3126e-05],\n",
      "        [ 1.2344e+00],\n",
      "        [ 1.0446e+00],\n",
      "        [ 2.3117e-01],\n",
      "        [ 9.2910e-01],\n",
      "        [ 1.2136e+00],\n",
      "        [ 1.4109e+00],\n",
      "        [-2.6493e-01],\n",
      "        [-3.6962e-01],\n",
      "        [ 1.9660e-01],\n",
      "        [ 1.1487e+00],\n",
      "        [ 5.2343e-01],\n",
      "        [-5.5366e-01],\n",
      "        [ 5.1455e-02],\n",
      "        [ 7.5730e-01],\n",
      "        [ 9.2552e-01],\n",
      "        [ 6.9102e-01],\n",
      "        [ 6.5689e-01],\n",
      "        [ 1.3099e+00],\n",
      "        [ 1.4469e+00],\n",
      "        [ 9.4442e-01],\n",
      "        [-5.6157e-01],\n",
      "        [ 4.2784e-01],\n",
      "        [-3.1848e-01],\n",
      "        [ 4.9853e-01],\n",
      "        [ 7.7065e-01],\n",
      "        [ 7.4912e-01],\n",
      "        [-6.1713e-01],\n",
      "        [ 1.1802e+00],\n",
      "        [ 3.7759e-01],\n",
      "        [-5.9930e-01],\n",
      "        [ 7.1552e-01],\n",
      "        [-5.4186e-01],\n",
      "        [ 6.4210e-01],\n",
      "        [ 5.6534e-01],\n",
      "        [ 8.2912e-01],\n",
      "        [ 7.8226e-01],\n",
      "        [ 4.1792e-05],\n",
      "        [ 5.1971e-05],\n",
      "        [ 1.3324e+00],\n",
      "        [-6.9557e-01],\n",
      "        [-2.4334e-02],\n",
      "        [-7.7006e-03],\n",
      "        [-4.6744e-01],\n",
      "        [ 1.2375e+00],\n",
      "        [ 5.8186e-01],\n",
      "        [ 1.3283e+00],\n",
      "        [ 1.2993e+00],\n",
      "        [-4.3975e-01],\n",
      "        [ 8.0172e-01],\n",
      "        [-6.0550e-01],\n",
      "        [-6.3508e-01],\n",
      "        [ 1.0058e+00],\n",
      "        [ 5.1528e-01],\n",
      "        [ 2.7040e-01],\n",
      "        [ 8.1677e-01],\n",
      "        [ 1.0735e+00],\n",
      "        [ 5.5518e-01],\n",
      "        [ 6.6694e-01],\n",
      "        [ 1.1956e+00],\n",
      "        [-4.2077e-01],\n",
      "        [-4.8770e-01],\n",
      "        [ 6.3365e-01],\n",
      "        [ 1.3215e+00],\n",
      "        [ 1.2834e+00],\n",
      "        [-7.2350e-03],\n",
      "        [-5.9619e-01],\n",
      "        [-5.4873e-01],\n",
      "        [ 1.1559e+00],\n",
      "        [-5.4045e-01],\n",
      "        [ 8.8492e-01],\n",
      "        [-5.0281e-01],\n",
      "        [-3.1094e-01],\n",
      "        [ 1.2220e+00],\n",
      "        [ 1.1495e+00],\n",
      "        [ 5.0826e-01],\n",
      "        [ 9.7391e-01],\n",
      "        [ 4.4604e-01],\n",
      "        [ 9.3491e-01],\n",
      "        [ 7.6570e-02],\n",
      "        [-6.0808e-01],\n",
      "        [-2.2519e-01],\n",
      "        [-7.0731e-01],\n",
      "        [-1.7966e-01],\n",
      "        [-1.1507e-02],\n",
      "        [ 1.2059e+00],\n",
      "        [ 1.7820e-02],\n",
      "        [ 6.8476e-01],\n",
      "        [ 5.8185e-01],\n",
      "        [ 3.2497e-01],\n",
      "        [ 2.7685e-01],\n",
      "        [ 9.9543e-01],\n",
      "        [-6.9756e-01],\n",
      "        [ 7.6360e-01],\n",
      "        [-2.3849e-01],\n",
      "        [ 1.2676e+00],\n",
      "        [ 1.0404e+00],\n",
      "        [ 9.5119e-01],\n",
      "        [-1.1393e-01],\n",
      "        [-1.1764e-01],\n",
      "        [ 1.0146e+00],\n",
      "        [-7.1732e-01],\n",
      "        [-2.3695e-01],\n",
      "        [-4.8103e-01],\n",
      "        [-3.2457e-01],\n",
      "        [ 5.5554e-01],\n",
      "        [ 9.4562e-01],\n",
      "        [-2.6908e-06],\n",
      "        [ 6.2721e-01],\n",
      "        [-4.0008e-01],\n",
      "        [ 1.2700e+00],\n",
      "        [ 3.6748e-01],\n",
      "        [ 7.0310e-01],\n",
      "        [-6.1492e-01],\n",
      "        [ 4.0212e-01],\n",
      "        [ 3.1056e-01],\n",
      "        [ 1.0133e+00],\n",
      "        [ 7.2035e-01],\n",
      "        [ 7.0024e-01],\n",
      "        [-8.4184e-02],\n",
      "        [ 8.0163e-01],\n",
      "        [ 1.4067e+00],\n",
      "        [ 1.1191e+00],\n",
      "        [-7.1086e-01],\n",
      "        [-2.1986e-01],\n",
      "        [-6.8002e-01],\n",
      "        [ 1.9017e-01],\n",
      "        [ 2.5375e-01],\n",
      "        [ 5.1420e-04],\n",
      "        [ 7.6163e-01],\n",
      "        [-2.4377e-02],\n",
      "        [-6.0096e-01],\n",
      "        [-6.3128e-01],\n",
      "        [ 1.1117e+00],\n",
      "        [-1.1976e-01],\n",
      "        [ 6.2117e-01],\n",
      "        [ 1.3448e+00],\n",
      "        [-4.4236e-01],\n",
      "        [-6.0066e-01],\n",
      "        [ 1.4580e+00],\n",
      "        [ 7.4805e-01],\n",
      "        [ 1.0876e-01],\n",
      "        [-4.9223e-01],\n",
      "        [-8.0500e-03],\n",
      "        [-2.5654e-01],\n",
      "        [-7.7942e-05],\n",
      "        [-7.3164e-01],\n",
      "        [ 5.3566e-01],\n",
      "        [-5.1428e-01],\n",
      "        [-1.8361e-01],\n",
      "        [ 5.9810e-05],\n",
      "        [-3.0323e-01],\n",
      "        [-2.3549e-01],\n",
      "        [-4.5562e-01],\n",
      "        [-2.0947e-01],\n",
      "        [ 9.0613e-01],\n",
      "        [-5.2960e-01],\n",
      "        [-6.6455e-01],\n",
      "        [-4.4687e-02],\n",
      "        [ 8.9009e-01],\n",
      "        [-6.0182e-01],\n",
      "        [ 2.1678e-01],\n",
      "        [ 1.2100e+00],\n",
      "        [ 8.8194e-01],\n",
      "        [ 1.3248e+00],\n",
      "        [ 1.5571e-05],\n",
      "        [ 6.7117e-01],\n",
      "        [-4.2064e-01],\n",
      "        [-3.0881e-01],\n",
      "        [ 3.2368e-01],\n",
      "        [-5.9921e-01],\n",
      "        [ 1.1215e+00],\n",
      "        [ 1.2407e+00],\n",
      "        [ 1.3400e+00],\n",
      "        [ 7.5630e-01],\n",
      "        [ 1.3740e+00],\n",
      "        [-5.4036e-01],\n",
      "        [-4.6096e-01],\n",
      "        [-5.7002e-01],\n",
      "        [ 3.8633e-01],\n",
      "        [-3.7427e-01],\n",
      "        [-4.1653e-01],\n",
      "        [ 7.6095e-01]])\n",
      "fc1_title.bias tensor([ 2.3226e-01,  1.0809e-01,  1.2900e+00, -6.5754e-01,  8.9151e-01,\n",
      "         7.8830e-01,  3.5996e-02,  1.0343e-05,  8.7387e-01,  6.3703e-01,\n",
      "        -7.9306e-05,  1.5047e-01, -4.9080e-01, -3.5714e-02,  5.8432e-01,\n",
      "        -6.4173e-01, -6.8358e-01, -3.0075e-05, -4.7203e-01, -7.0694e-05,\n",
      "        -6.4341e-01, -2.4397e-01, -2.2199e-04,  4.2612e-02,  9.7531e-02,\n",
      "         1.5109e-01,  1.0602e+00,  1.2649e+00,  8.8725e-01,  1.8733e-05,\n",
      "         1.3082e+00, -4.1572e-01,  5.9180e-01,  2.4635e-01, -4.3128e-05,\n",
      "         4.3974e-02,  1.0947e+00, -4.7515e-01,  1.0133e+00, -3.6434e-01,\n",
      "        -4.2814e-01, -5.8478e-01,  4.2876e-01,  1.4148e+00,  1.0409e+00,\n",
      "        -5.2627e-01, -7.4488e-01, -3.6440e-05,  4.9665e-01,  1.4376e+00,\n",
      "        -3.6456e-01,  1.3496e+00,  1.2572e-01,  7.4988e-01,  2.5527e-01,\n",
      "        -6.8486e-06, -4.8109e-02,  1.2972e+00, -7.2139e-05, -3.3622e-01,\n",
      "        -1.9414e-01,  6.3768e-01,  1.0016e+00,  1.2854e+00,  2.0476e-01,\n",
      "        -6.3756e-02,  1.1660e-01,  6.8021e-01,  1.1566e+00,  1.1259e+00,\n",
      "        -7.7050e-03, -4.4209e-04,  2.9232e-01, -4.3942e-01, -2.8792e-01,\n",
      "         8.5195e-01,  6.3446e-01,  3.6074e-01,  1.9860e-01, -2.1536e-01,\n",
      "         1.8614e-01,  6.4422e-01, -7.2208e-01,  6.4267e-05,  1.3878e+00,\n",
      "         1.9426e-01,  9.6415e-01, -3.0733e-01,  6.7625e-01, -4.9573e-01,\n",
      "        -5.8519e-02,  1.2685e+00, -5.9231e-01, -2.8380e-01, -2.6944e-01,\n",
      "        -1.6063e-04, -2.7361e-01,  1.0451e-01,  9.1550e-01, -3.7944e-02,\n",
      "         1.4080e+00,  2.0964e-01,  5.6054e-01,  2.2437e-01,  2.8923e-01,\n",
      "         1.4681e+00,  5.2435e-01,  3.8921e-01,  1.2529e+00,  2.1988e-01,\n",
      "        -4.9282e-01,  6.1499e-02, -1.4225e-01,  7.3191e-01, -1.0213e-01,\n",
      "        -1.9366e-01,  4.9759e-01,  3.8479e-01,  6.6844e-02,  1.1091e+00,\n",
      "        -4.9863e-01,  8.9873e-01, -1.3968e-01, -1.6333e-01, -3.3380e-02,\n",
      "         1.2280e+00, -1.3911e-02,  1.3303e+00,  3.0753e-01,  6.8687e-01,\n",
      "        -5.6957e-01, -3.8805e-01,  1.1336e+00,  9.8800e-01, -4.9457e-01,\n",
      "        -1.2106e-01,  4.4074e-01,  6.3335e-01, -2.3982e-01, -6.7467e-01,\n",
      "         3.4683e-05,  1.1955e-04, -7.1074e-01, -6.1267e-01,  1.3806e+00,\n",
      "        -7.9799e-03,  1.0960e+00,  9.3094e-01,  8.2032e-01, -7.7416e-03,\n",
      "         5.2879e-01, -2.8153e-01,  4.5420e-01, -7.6835e-03,  1.1643e+00,\n",
      "        -4.6728e-02,  7.3495e-05, -4.8262e-01,  9.0042e-01,  5.4315e-01,\n",
      "        -2.5346e-01,  2.3593e-01, -6.2863e-01,  1.3095e+00, -2.7696e-01,\n",
      "         1.1078e+00,  1.1265e-01,  1.1762e-05,  9.1452e-05,  1.0326e+00,\n",
      "         1.7070e-02,  7.3027e-01, -3.4712e-05, -1.9196e-02,  1.3168e+00,\n",
      "        -4.3826e-01, -6.7301e-01, -5.8609e-03, -6.5049e-01, -2.4678e-01,\n",
      "        -3.1735e-01,  1.3480e+00,  1.1867e+00, -4.5245e-01, -8.7718e-02,\n",
      "         1.3154e-01,  1.0823e-05, -7.1617e-03,  9.5333e-01,  4.3661e-01,\n",
      "         7.9399e-01,  8.7375e-01, -6.2480e-02,  1.2499e+00,  1.4005e+00,\n",
      "         2.3312e-01, -3.5037e-01,  5.1332e-01, -1.5825e-01,  1.1251e+00,\n",
      "         8.2314e-01, -5.0659e-01, -4.8039e-01,  2.7968e-01,  4.5042e-01,\n",
      "        -6.0668e-02, -1.4068e-01,  4.7622e-02,  8.0181e-01, -6.5267e-01,\n",
      "         6.2119e-01,  9.8323e-05,  5.1419e-01, -3.9851e-01,  1.4585e+00,\n",
      "         1.4600e+00,  2.4632e-01,  7.0830e-01, -5.9057e-05, -8.8020e-03,\n",
      "         2.6180e-01,  1.1475e+00,  8.4166e-02,  6.9155e-01, -4.2759e-01,\n",
      "         4.4636e-02,  1.1910e+00, -6.1917e-01, -8.1847e-02, -8.6403e-02,\n",
      "         2.1087e-01,  2.6497e-01,  1.3950e-05, -2.9170e-01, -4.1406e-01,\n",
      "         7.6302e-02,  4.9820e-05,  1.3297e+00, -4.7907e-01, -6.0282e-01,\n",
      "         9.4676e-02,  9.7346e-02, -6.8883e-02, -1.4268e-01,  3.7098e-01,\n",
      "         3.1168e-01,  1.2087e-01,  7.1648e-01,  2.0807e-01, -8.3446e-03,\n",
      "         6.9057e-01,  1.3137e+00,  1.4295e+00, -1.2081e-01,  1.3190e-02,\n",
      "        -7.1552e-01,  7.4039e-01,  4.1680e-01, -2.6249e-01,  1.1681e-01,\n",
      "        -7.0445e-05,  9.7322e-02,  9.4179e-01,  3.1612e-01,  4.2411e-02,\n",
      "        -4.7905e-01, -4.4855e-01, -4.8663e-01,  4.9027e-01, -1.7119e-01,\n",
      "        -1.5039e-01,  9.7388e-02,  7.3149e-01, -2.3454e-01,  1.1782e+00,\n",
      "        -2.6046e-01,  2.5348e-01, -1.4864e-01,  8.0493e-05, -7.9824e-02,\n",
      "         1.1631e-01,  6.3293e-01,  1.3177e+00, -4.9007e-01,  7.0212e-01,\n",
      "         1.0290e+00,  1.2668e+00, -2.2614e-01,  1.0794e+00, -2.3379e-01,\n",
      "        -2.7841e-01, -3.1693e-05,  1.2523e+00, -3.2031e-01,  9.1172e-01,\n",
      "        -9.7026e-02, -4.8688e-06,  1.0706e+00, -4.0680e-05, -3.3189e-01,\n",
      "         7.7312e-01,  1.4096e+00, -8.4574e-03,  4.9951e-01,  9.1452e-01,\n",
      "         1.3486e+00, -2.7222e-01, -4.7662e-05,  5.2216e-01,  5.1911e-01,\n",
      "        -6.2052e-06,  4.5279e-02, -5.6334e-01, -2.9076e-01,  6.6650e-01,\n",
      "        -6.2084e-01,  1.2972e+00,  1.1180e+00, -4.9542e-01,  2.1290e-01,\n",
      "        -3.8445e-02,  6.2620e-01,  9.5423e-01,  1.3276e+00,  4.7855e-01,\n",
      "         1.2598e+00,  7.0061e-01,  2.1429e-01,  8.5001e-02, -5.8619e-02,\n",
      "        -4.5928e-01,  3.2615e-01,  1.4347e+00,  1.3670e+00,  1.3981e+00,\n",
      "         2.1158e-01, -3.1732e-05,  6.5991e-05,  1.0561e+00,  3.3584e-01,\n",
      "         9.1381e-01,  3.1441e-01,  1.1121e+00, -9.9686e-02,  1.2709e+00,\n",
      "         9.4984e-01,  1.4958e-01,  6.8452e-01,  1.1964e+00,  9.6305e-01,\n",
      "        -2.7561e-01,  5.4941e-01, -7.6723e-02,  1.8272e-01,  1.1487e-01,\n",
      "        -5.1163e-01, -1.9766e-01,  1.5968e-01,  3.8204e-01, -1.1272e-03,\n",
      "         2.6338e-01, -4.8533e-01,  1.4117e+00,  6.3361e-01,  2.5402e-01,\n",
      "         5.7943e-01, -4.5098e-01, -1.9678e-01,  3.1021e-01, -1.0185e-01,\n",
      "         3.4957e-05, -7.3959e-03,  1.3757e-01,  1.1062e+00, -3.5217e-01,\n",
      "        -1.4999e-02, -4.4500e-01,  7.2620e-01,  2.4242e-01,  6.4844e-05,\n",
      "         6.2964e-01,  3.3386e-01,  6.6196e-01,  1.4380e+00,  6.4540e-01,\n",
      "        -4.9053e-01,  8.2574e-01,  1.1148e+00,  1.3361e+00,  1.0857e-01,\n",
      "        -1.7737e-01, -2.1252e-01,  7.1274e-01,  1.3514e+00, -7.4385e-03,\n",
      "        -1.0295e-04,  2.7297e-01,  1.1895e+00, -4.5877e-01,  8.3817e-01,\n",
      "        -7.7018e-06,  7.1017e-01,  3.7810e-01,  2.7697e-03,  1.1051e+00,\n",
      "         1.1077e+00,  6.6711e-01,  6.3992e-01,  1.0559e+00,  4.0148e-01,\n",
      "         6.5314e-01,  3.3272e-01,  2.1206e-01, -2.9957e-01,  1.0197e+00,\n",
      "         8.7395e-01,  6.6688e-01,  5.9146e-01,  6.6584e-01,  1.0540e+00,\n",
      "        -3.0009e-01, -1.1070e-03,  8.8878e-01, -6.5328e-01,  7.6316e-01,\n",
      "        -5.0350e-01, -1.2305e-01, -3.8782e-05,  3.1173e-01, -4.2230e-02,\n",
      "        -4.0218e-01, -1.4132e-01, -4.9473e-01,  2.7142e-01,  1.8762e-01,\n",
      "        -3.3711e-01, -4.3882e-01,  5.3802e-02, -4.1661e-01, -2.2517e-01,\n",
      "         9.0053e-01,  4.8985e-01,  4.4653e-05,  4.0692e-01,  1.3685e+00,\n",
      "         1.3880e+00,  3.5434e-01,  5.9540e-01, -9.5974e-06,  3.7928e-01,\n",
      "         1.2081e+00, -4.6767e-01, -2.1768e-01,  5.4724e-01,  6.1477e-01,\n",
      "         9.1284e-01,  1.2140e+00, -8.7570e-02,  3.4800e-01, -1.3254e-01,\n",
      "         4.1809e-01, -1.2636e-01,  9.6454e-01,  2.3628e-05, -3.9585e-01,\n",
      "         1.0222e-01, -2.8331e-01, -7.2368e-01,  3.9658e-01,  1.4550e-01,\n",
      "         1.3574e+00, -7.3376e-01, -7.3606e-03,  6.0953e-01, -1.8999e-01,\n",
      "         3.7843e-05,  1.3535e+00, -4.5748e-01, -3.6556e-01, -1.7029e-01,\n",
      "        -3.2927e-01, -2.1819e-01,  2.2811e-01,  1.0595e+00,  3.0224e-01,\n",
      "        -6.0834e-01,  4.4645e-01, -2.0712e-05,  9.4531e-01,  1.3864e-01,\n",
      "         2.2013e-01,  1.4376e+00,  1.3349e+00, -3.0027e-01, -6.4104e-01,\n",
      "         4.7635e-01,  3.5389e-01,  5.6400e-01,  1.4643e+00, -3.8088e-01,\n",
      "         9.8038e-01,  1.0934e+00,  9.9787e-01,  5.2029e-01, -1.9199e-01,\n",
      "         1.0747e-01, -1.5055e-01,  7.4060e-01,  6.2373e-01,  3.6678e-01,\n",
      "        -6.7053e-01, -4.4904e-01])\n",
      "fc2.weight tensor([[ 1.4798e-02, -5.9023e-06, -1.5621e-02,  ...,  5.1185e-05,\n",
      "          9.4159e-04, -4.8416e-05],\n",
      "        [ 4.1220e-01,  1.0707e-01,  4.0744e-01,  ..., -1.6556e-05,\n",
      "          4.5760e-01,  4.5753e-01],\n",
      "        [ 4.1829e-01,  1.0737e-01,  3.8230e-01,  ..., -1.8457e-02,\n",
      "          4.8524e-01,  4.5717e-01],\n",
      "        ...,\n",
      "        [ 4.2334e-01,  1.0755e-01,  4.1824e-01,  ...,  3.2064e-06,\n",
      "          4.6990e-01,  4.7009e-01],\n",
      "        [ 5.2975e-03,  6.0081e-05, -7.4368e-03,  ...,  5.3848e-03,\n",
      "         -2.5517e-02,  2.1379e-04],\n",
      "        [ 4.1442e-01,  1.0767e-01,  4.0672e-01,  ...,  2.5298e-05,\n",
      "          4.7089e-01,  4.6831e-01]])\n",
      "fc2.bias tensor([-2.7378e-03,  3.8509e-01,  3.5591e-01,  1.9391e-02,  2.3035e-03,\n",
      "         3.3172e-01,  4.1241e-01, -2.8261e-03,  3.2344e-01,  2.8687e-03,\n",
      "        -1.6377e-02,  3.9966e-01, -1.6681e-02,  5.0545e-03,  3.8247e-01,\n",
      "        -4.1846e-03, -2.0103e-02,  3.8544e-01,  3.4524e-01,  3.6820e-01,\n",
      "         3.9378e-01,  3.8753e-01,  4.0890e-01,  3.5557e-01, -5.9235e-03,\n",
      "        -2.5776e-02,  3.9107e-01, -2.7129e-02, -2.0265e-02,  5.7049e-03,\n",
      "         1.2289e-02,  3.9297e-01, -5.1115e-03,  3.7065e-01,  3.6916e-01,\n",
      "         3.9433e-01,  5.6516e-03,  1.9528e-02,  3.8979e-01,  4.3461e-04,\n",
      "        -2.1359e-02, -1.9642e-02, -2.0504e-02, -9.5011e-03,  3.7227e-01,\n",
      "         3.6023e-01,  9.7437e-03,  4.1064e-01, -1.3917e-02,  6.7852e-03,\n",
      "         3.8339e-01,  6.6146e-04,  1.0434e-02,  3.6437e-01, -3.4238e-03,\n",
      "        -1.9676e-03,  3.9027e-01,  3.8780e-01, -9.3068e-03, -7.3737e-03,\n",
      "        -8.3141e-03, -3.0806e-03,  1.8638e-02,  3.7256e-01, -3.3539e-03,\n",
      "         3.9383e-01,  3.5439e-01, -2.0573e-03,  3.6857e-01,  3.8207e-01,\n",
      "         1.9076e-02,  3.8980e-01,  3.6935e-01,  1.5860e-02,  3.8100e-01,\n",
      "         3.9451e-01,  3.7342e-01,  1.2916e-02,  3.8005e-01, -3.4572e-03,\n",
      "        -2.4666e-02,  3.9623e-01,  3.5944e-01, -3.1420e-03, -3.1564e-03,\n",
      "        -2.6507e-02,  3.5160e-01,  3.6561e-01,  3.9338e-01, -1.6900e-02,\n",
      "         4.0299e-01,  3.5408e-01,  3.9528e-01,  3.8842e-01,  3.7627e-01,\n",
      "         3.6438e-01,  3.5975e-01, -1.3457e-02, -7.3579e-03,  3.5695e-01,\n",
      "         3.4351e-01,  3.2045e-01,  3.9347e-01,  8.9429e-03, -3.2873e-03,\n",
      "        -3.0149e-03, -1.0370e-02, -2.7734e-02, -1.6492e-02,  3.7693e-01,\n",
      "        -3.0742e-03, -2.3194e-02, -3.5014e-03,  3.9411e-01,  3.6985e-01,\n",
      "         3.9312e-01,  3.5753e-01,  3.9389e-01,  1.5958e-02, -1.6801e-02,\n",
      "        -2.1557e-03, -6.5400e-03, -3.6829e-03,  3.5485e-01, -1.7264e-02,\n",
      "        -3.0926e-03,  3.9139e-01,  3.5710e-01, -3.3085e-03,  1.5264e-02,\n",
      "         3.8010e-01,  3.7117e-01, -3.1964e-03,  3.9400e-01,  3.8024e-01,\n",
      "         3.8761e-01,  3.6822e-01, -3.3480e-03, -2.3548e-03, -1.3193e-02,\n",
      "         3.3053e-01, -1.9837e-02,  3.8959e-01, -1.8428e-02, -1.0580e-02,\n",
      "         3.7846e-01,  1.8799e-02,  3.9794e-01, -5.8885e-03,  3.9158e-01,\n",
      "        -2.2726e-02,  3.6862e-01, -1.9744e-02, -8.7571e-03,  3.7850e-01,\n",
      "         3.4930e-01,  3.7632e-01,  9.8937e-03,  3.9712e-01,  3.9450e-01,\n",
      "         3.4136e-01,  3.9419e-01,  3.7828e-01, -1.5844e-02,  1.3583e-02,\n",
      "        -2.7515e-03, -3.6227e-03,  3.8477e-01,  3.9326e-01, -4.8185e-03,\n",
      "        -4.2349e-03, -2.6119e-03,  3.6825e-01,  3.8491e-01,  3.8563e-01,\n",
      "        -1.4599e-02,  1.4377e-02, -8.5777e-03,  3.3947e-01,  3.8458e-01,\n",
      "        -2.4719e-03,  3.7284e-01,  3.5647e-01,  3.8050e-01,  1.7528e-02,\n",
      "         3.8271e-01, -8.3628e-03,  3.9375e-01,  7.8272e-03,  3.9373e-01,\n",
      "         3.7902e-01, -3.3764e-03,  3.5904e-01,  3.9422e-01,  3.8181e-01,\n",
      "         7.5775e-03,  3.3272e-01, -3.2297e-03,  3.6524e-01, -1.0084e-02,\n",
      "         3.5975e-01,  3.5016e-01,  3.7730e-01,  3.5027e-01,  3.3002e-01,\n",
      "         3.9392e-01,  1.0874e-02,  3.9493e-01,  3.9405e-01,  3.9481e-01,\n",
      "         6.8705e-03,  1.0251e-02,  3.7718e-01, -1.7621e-02,  3.4340e-01,\n",
      "         3.7585e-01,  9.4812e-04,  3.9652e-01,  8.7976e-03,  3.8432e-01,\n",
      "         3.9367e-01,  3.8803e-01,  3.4955e-01, -3.0862e-03,  4.4419e-03,\n",
      "        -1.1538e-02,  3.4703e-01,  3.6758e-01, -1.4613e-03,  3.4626e-01,\n",
      "         3.4240e-01, -4.3383e-03, -3.4287e-03,  2.0439e-02, -3.3024e-03,\n",
      "        -3.0390e-03,  4.8734e-03,  8.3917e-03,  3.9427e-01,  3.7953e-01,\n",
      "         3.4302e-01, -8.9071e-03,  3.8417e-01,  3.5528e-01, -3.3413e-03,\n",
      "         3.7556e-01,  1.8471e-04,  3.7997e-01,  3.9501e-01,  4.5273e-03,\n",
      "        -5.1070e-03, -3.2708e-03,  3.8145e-01,  3.3874e-01,  3.7306e-01,\n",
      "        -1.8682e-02,  3.9372e-01,  3.7418e-01, -1.2796e-04,  3.4905e-01,\n",
      "         3.9359e-01,  3.5188e-01,  3.5599e-01, -9.4945e-03, -3.5138e-03,\n",
      "         4.0493e-01,  1.4157e-03, -1.3952e-03,  3.1937e-01, -4.2059e-03,\n",
      "        -3.2851e-03,  3.9146e-01,  3.3889e-01,  3.9378e-01, -2.0875e-03,\n",
      "        -2.3211e-03,  3.2452e-01,  3.8519e-01, -1.2157e-02,  3.5098e-01,\n",
      "        -1.2381e-03, -2.5885e-03,  3.5190e-01,  3.6333e-01,  3.7559e-01,\n",
      "         3.7156e-01,  3.9001e-01,  3.3533e-01,  3.9476e-01,  3.8839e-01,\n",
      "         1.6457e-02,  3.3568e-01,  3.7098e-01,  2.2073e-02,  3.5858e-01,\n",
      "        -2.7398e-02,  3.7538e-01,  4.0591e-01, -6.2976e-03,  1.4763e-02,\n",
      "         3.9063e-01,  3.7555e-01,  3.8190e-01,  1.5740e-02, -1.3852e-03,\n",
      "        -2.1363e-02,  3.6291e-01,  3.7228e-01,  1.3007e-02, -1.2797e-02,\n",
      "        -2.6275e-03, -9.0740e-03, -3.9384e-03,  3.9961e-01, -1.6199e-03,\n",
      "         3.4923e-01,  3.4207e-01,  1.2581e-02,  3.1940e-01,  3.7561e-01,\n",
      "         3.9407e-01,  3.6992e-01,  1.2940e-02, -2.5617e-03,  3.5598e-01,\n",
      "         3.7534e-01,  1.3258e-02,  3.1781e-01, -5.0916e-03, -3.6182e-03,\n",
      "         5.7760e-03, -6.3620e-03,  1.1066e-02,  3.7339e-01, -2.9341e-03,\n",
      "         3.5533e-01,  1.8935e-02, -3.2839e-03,  3.7540e-01, -1.9592e-03,\n",
      "         3.9516e-01,  8.3088e-03,  2.2121e-02, -2.1775e-02,  3.2404e-01,\n",
      "        -1.6221e-02,  3.9389e-01,  3.7030e-01, -6.6792e-03, -4.2901e-03,\n",
      "         8.3407e-03,  3.5752e-01, -2.9693e-03,  3.8027e-01, -2.7404e-02,\n",
      "        -2.1217e-02,  3.7138e-01,  1.5873e-02,  3.6415e-01,  3.9430e-01,\n",
      "         3.8502e-01,  3.8389e-01, -1.9308e-03,  3.9455e-01,  3.6170e-01,\n",
      "         3.6439e-01,  2.0250e-02,  3.6917e-01, -2.2787e-02, -3.1528e-03,\n",
      "        -1.3592e-02, -2.3909e-02,  3.4995e-01,  1.9136e-03, -2.9880e-03,\n",
      "        -2.8255e-03,  3.4592e-01,  3.4189e-01,  3.5908e-01, -3.0555e-03,\n",
      "        -1.2514e-02,  9.6372e-03, -2.3093e-02, -2.5672e-02,  1.0613e-02,\n",
      "         3.8267e-01,  3.8473e-01, -1.2916e-02,  3.9389e-01,  5.4096e-03,\n",
      "         3.9288e-01,  8.0111e-03,  3.8914e-01, -1.0456e-02,  3.3401e-01,\n",
      "         4.0875e-01, -7.5587e-03, -1.0372e-02, -2.4120e-03,  4.0192e-01,\n",
      "         3.8794e-01,  3.3188e-01,  3.2070e-01,  8.5559e-03, -2.2202e-03,\n",
      "         4.0144e-01,  3.7775e-01,  3.8090e-01,  3.7436e-01, -2.5913e-03,\n",
      "         3.7461e-01, -4.5045e-03,  3.9034e-01, -2.8759e-03,  1.0320e-02,\n",
      "         3.9589e-01,  3.5148e-01,  2.2551e-02, -2.2287e-04, -2.3753e-02,\n",
      "         3.8338e-01, -6.4723e-03, -2.4377e-02,  3.2563e-01,  3.6026e-01,\n",
      "         3.3446e-01,  3.3616e-01,  3.8314e-01,  4.0250e-01,  3.7805e-01,\n",
      "         2.0133e-02,  3.6417e-01,  2.1018e-02, -1.9813e-02, -1.5625e-02,\n",
      "         3.6839e-01, -1.3356e-02,  1.6415e-02,  4.3688e-03, -6.2680e-03,\n",
      "        -2.4539e-02,  2.2415e-02,  3.6934e-01, -2.7613e-03,  1.7134e-02,\n",
      "         3.7340e-01,  4.1267e-01,  3.4365e-01, -2.6995e-03, -1.3382e-02,\n",
      "         3.3616e-01,  3.8059e-01,  3.4687e-01,  3.5254e-01,  3.8113e-01,\n",
      "         3.9451e-01,  1.7271e-02, -1.5963e-02, -2.0902e-02,  3.4058e-01,\n",
      "         3.5699e-03, -1.0811e-02, -2.0129e-02,  3.9742e-01,  3.8348e-01,\n",
      "         2.0548e-02,  3.6394e-01, -3.1950e-03,  3.9488e-01,  2.0036e-02,\n",
      "         3.8942e-01, -3.1581e-03,  4.8559e-03,  3.6197e-01,  6.6537e-03,\n",
      "        -3.2067e-03,  3.8953e-01, -4.0443e-04, -1.1290e-02, -2.2703e-02,\n",
      "         3.9314e-01,  1.9233e-02, -2.3524e-02,  3.7751e-01, -7.7931e-03,\n",
      "         3.5804e-01,  3.3047e-01, -2.1744e-03,  3.3238e-01,  3.4281e-01,\n",
      "        -3.7189e-03,  3.7470e-01,  3.7566e-01, -3.0659e-03, -2.3029e-02,\n",
      "         3.5518e-01, -1.1659e-02, -3.1263e-03,  3.5658e-01,  2.7191e-03,\n",
      "         3.7051e-01,  3.6693e-01,  3.4986e-01, -1.4971e-02,  3.8742e-01,\n",
      "        -4.0339e-03,  3.9448e-01,  3.9652e-01,  3.3827e-01,  3.9046e-01,\n",
      "        -3.8064e-03,  3.8534e-01])\n",
      "fc3.weight tensor([[-1.1281e-02,  4.7176e-01,  4.8674e-01, -2.6331e-02, -3.4296e-02,\n",
      "          5.2558e-01,  4.7793e-01, -2.4021e-02,  5.1832e-01, -2.7707e-02,\n",
      "         -2.1345e-03,  4.8896e-01, -1.8652e-02, -4.7118e-03,  5.0022e-01,\n",
      "         -6.0520e-04,  1.0376e-04,  4.7641e-01,  5.1417e-01,  4.9624e-01,\n",
      "          4.6144e-01,  4.7149e-01,  4.8001e-01,  5.1898e-01, -1.2930e-02,\n",
      "         -1.9180e-02,  4.6519e-01, -4.7893e-03, -1.2277e-02, -1.8342e-02,\n",
      "         -5.9998e-03,  4.6507e-01,  3.1874e-03,  4.8432e-01,  5.0209e-01,\n",
      "          4.6524e-01, -1.8599e-02, -2.6386e-02,  4.6683e-01, -3.3552e-02,\n",
      "         -3.0981e-03, -3.7034e-02, -4.0109e-02, -1.3348e-02,  5.0880e-01,\n",
      "          4.9447e-01, -1.5497e-02,  4.8448e-01, -1.1571e-02, -1.2440e-02,\n",
      "          4.7627e-01, -2.2531e-03, -2.1556e-02,  5.0008e-01,  2.9814e-03,\n",
      "          1.9468e-03,  4.7737e-01,  4.7319e-01, -3.0948e-02, -2.2774e-02,\n",
      "          8.7694e-04, -1.0571e-02,  1.7301e-03,  5.0535e-01, -7.4549e-04,\n",
      "          4.7326e-01,  5.1187e-01, -1.1919e-02,  4.9818e-01,  4.9107e-01,\n",
      "         -3.7652e-02,  4.6814e-01,  5.0263e-01, -3.7209e-02,  5.1926e-01,\n",
      "          4.6516e-01,  4.9817e-01, -3.5704e-02,  4.7974e-01, -2.4301e-03,\n",
      "         -2.2763e-02,  4.7144e-01,  5.1367e-01,  3.2522e-03, -2.8444e-04,\n",
      "         -4.9631e-03,  4.8832e-01,  4.9118e-01,  4.6035e-01, -1.9000e-02,\n",
      "          4.7219e-01,  5.0156e-01,  4.6293e-01,  4.7143e-01,  5.0149e-01,\n",
      "          4.9358e-01,  5.0186e-01, -1.6438e-02, -2.5769e-02,  5.1981e-01,\n",
      "          5.1443e-01,  5.2052e-01,  4.6868e-01, -1.6339e-02, -1.8147e-02,\n",
      "         -1.5112e-02, -2.9537e-04, -1.6035e-02, -3.7231e-02,  4.8974e-01,\n",
      "          3.4404e-04, -5.4384e-03,  2.9609e-03,  4.6267e-01,  5.0748e-01,\n",
      "          4.7183e-01,  5.1784e-01,  4.6407e-01, -6.3920e-03, -3.5030e-02,\n",
      "          9.5186e-04,  1.4620e-03, -2.8342e-04,  4.8615e-01, -2.2649e-02,\n",
      "          3.4215e-04,  4.6732e-01,  5.1575e-01, -1.2711e-02, -1.7118e-02,\n",
      "          5.0819e-01,  4.8750e-01,  2.1749e-03,  4.8584e-01,  4.8359e-01,\n",
      "          4.9449e-01,  5.1421e-01,  2.8708e-03,  2.9872e-03, -3.1056e-02,\n",
      "          5.1500e-01, -2.1857e-02,  4.7506e-01, -3.7259e-02, -3.0423e-03,\n",
      "          4.9113e-01, -1.7265e-02,  4.8824e-01, -1.1874e-02,  4.6884e-01,\n",
      "         -2.4727e-02,  5.0510e-01, -1.1005e-03, -2.5108e-02,  4.9194e-01,\n",
      "          5.1613e-01,  4.9476e-01,  2.3473e-03,  4.9264e-01,  4.6299e-01,\n",
      "          5.0392e-01,  4.6306e-01,  5.0583e-01, -2.9804e-02,  3.2212e-04,\n",
      "         -2.6508e-03,  3.5752e-03,  4.8263e-01,  4.6242e-01, -1.9011e-02,\n",
      "         -8.5347e-03, -1.6223e-02,  4.9898e-01,  4.7519e-01,  5.0568e-01,\n",
      "         -2.3195e-02,  1.2989e-03, -3.2405e-02,  5.1409e-01,  4.8734e-01,\n",
      "         -1.0860e-02,  4.8265e-01,  5.1684e-01,  4.7906e-01, -2.1816e-03,\n",
      "          4.8570e-01,  5.7748e-04,  4.6428e-01, -3.4903e-02,  4.9264e-01,\n",
      "          4.8247e-01, -4.4136e-03,  4.9283e-01,  4.6311e-01,  4.7755e-01,\n",
      "         -1.4759e-02,  5.0358e-01,  2.9054e-03,  5.0829e-01, -3.8440e-02,\n",
      "          4.9824e-01,  5.1669e-01,  4.9266e-01,  5.1998e-01,  5.1215e-01,\n",
      "          4.6197e-01, -2.5453e-02,  4.7047e-01,  4.6302e-01,  4.6514e-01,\n",
      "         -2.9931e-02, -2.2437e-02,  4.8387e-01, -3.2109e-02,  5.1284e-01,\n",
      "          5.1273e-01, -4.4638e-03,  4.9172e-01, -2.0387e-02,  4.9010e-01,\n",
      "          4.6209e-01,  4.8541e-01,  4.9603e-01, -6.4430e-04, -5.6052e-03,\n",
      "         -2.3607e-02,  5.1414e-01,  5.1408e-01, -1.1298e-02,  5.0616e-01,\n",
      "          4.9654e-01,  2.9328e-03,  2.7820e-03, -1.9352e-02,  3.0963e-03,\n",
      "          3.0864e-03, -3.6414e-03, -3.0552e-02,  4.6770e-01,  4.9923e-01,\n",
      "          4.9333e-01, -1.3051e-02,  4.8306e-01,  4.8764e-01, -6.7361e-03,\n",
      "          4.8942e-01,  1.6913e-04,  4.8314e-01,  4.8215e-01, -7.6549e-03,\n",
      "         -2.2811e-02, -1.5266e-02,  4.8375e-01,  5.1806e-01,  4.8372e-01,\n",
      "         -1.2936e-02,  4.8481e-01,  4.9049e-01, -1.5694e-02,  5.2545e-01,\n",
      "          4.6958e-01,  5.1505e-01,  5.2172e-01,  2.3125e-03,  2.8652e-03,\n",
      "          4.8007e-01, -1.8634e-02, -3.5408e-02,  5.1558e-01, -6.4689e-03,\n",
      "         -1.0498e-02,  4.8444e-01,  5.1887e-01,  4.6377e-01, -1.6354e-02,\n",
      "          2.2331e-04,  5.1075e-01,  4.7552e-01,  1.7038e-03,  5.1029e-01,\n",
      "         -2.4292e-02,  2.4123e-03,  5.0034e-01,  4.9520e-01,  4.8843e-01,\n",
      "          5.1114e-01,  4.7525e-01,  4.9702e-01,  4.6596e-01,  4.9376e-01,\n",
      "         -5.0288e-03,  5.0815e-01,  4.8152e-01, -2.4334e-02,  5.0341e-01,\n",
      "         -7.2853e-03,  4.9190e-01,  4.8194e-01, -4.0602e-02, -3.0419e-02,\n",
      "          4.6982e-01,  4.9239e-01,  4.8187e-01, -3.2414e-02, -3.0834e-02,\n",
      "         -1.7494e-03,  5.1376e-01,  5.0927e-01, -3.8443e-03, -4.3067e-03,\n",
      "          2.4355e-03, -2.0131e-02, -5.4773e-03,  4.8268e-01, -2.3001e-02,\n",
      "          5.1297e-01,  4.9974e-01, -3.7947e-02,  5.1996e-01,  5.0289e-01,\n",
      "          4.6632e-01,  5.0390e-01, -2.6420e-02, -2.8687e-03,  5.1068e-01,\n",
      "          5.0331e-01, -1.7325e-02,  5.1429e-01, -3.5284e-02, -5.5372e-03,\n",
      "         -4.7112e-04, -3.9203e-02, -2.4777e-02,  4.9729e-01, -2.2614e-03,\n",
      "          5.1377e-01, -2.7928e-02,  3.2939e-03,  4.9276e-01, -3.1519e-02,\n",
      "          4.6201e-01, -3.7177e-02, -1.1176e-02, -3.2929e-02,  5.1453e-01,\n",
      "         -4.5787e-03,  4.7297e-01,  4.8485e-01, -1.8305e-04, -5.1433e-03,\n",
      "         -3.9720e-02,  5.0216e-01, -1.0027e-02,  5.2186e-01, -2.8205e-02,\n",
      "         -7.5272e-03,  5.1895e-01, -2.8703e-02,  5.1893e-01,  4.6445e-01,\n",
      "          5.0348e-01,  4.8617e-01, -1.6479e-02,  4.6789e-01,  5.1624e-01,\n",
      "          5.1394e-01, -2.6218e-02,  4.8227e-01, -1.0168e-02, -4.5874e-03,\n",
      "         -1.5016e-02, -9.0113e-03,  4.9469e-01, -2.6819e-02, -1.1020e-02,\n",
      "          2.3465e-03,  5.1132e-01,  5.2247e-01,  5.0657e-01,  1.8578e-03,\n",
      "         -3.1904e-02, -3.6234e-02, -2.0193e-02, -1.0859e-02, -6.0166e-03,\n",
      "          4.9314e-01,  4.9591e-01, -2.2992e-02,  4.6543e-01, -1.6738e-03,\n",
      "          4.8143e-01, -1.6920e-03,  4.7770e-01, -9.8272e-03,  5.1532e-01,\n",
      "          4.7625e-01, -1.7945e-02, -2.1097e-03, -3.8106e-02,  4.8828e-01,\n",
      "          5.0061e-01,  5.1170e-01,  5.1936e-01, -2.8643e-02, -3.1413e-02,\n",
      "          4.7717e-01,  4.7658e-01,  4.9035e-01,  5.0002e-01, -2.3132e-02,\n",
      "          4.9594e-01,  3.3090e-03,  4.6726e-01,  3.2583e-03,  1.6592e-03,\n",
      "          4.7176e-01,  4.9415e-01, -1.9903e-02, -3.4569e-02, -2.1365e-03,\n",
      "          4.7543e-01,  1.4525e-03, -3.0452e-02,  5.0576e-01,  5.0778e-01,\n",
      "          5.1318e-01,  5.0939e-01,  4.7619e-01,  4.7704e-01,  4.8074e-01,\n",
      "         -4.0599e-02,  5.0905e-01, -1.0872e-02, -1.8658e-02, -2.6818e-02,\n",
      "          4.9341e-01,  1.0292e-03, -3.6451e-02, -1.8297e-02, -9.5588e-03,\n",
      "         -2.9720e-02, -3.7747e-02,  5.0277e-01, -4.1214e-02, -2.7150e-02,\n",
      "          5.0002e-01,  4.7931e-01,  4.9940e-01, -6.2509e-03, -1.8631e-02,\n",
      "          5.1513e-01,  4.7609e-01,  5.1678e-01,  5.1577e-01,  5.0680e-01,\n",
      "          4.6436e-01, -2.9512e-02, -1.3097e-02, -3.3603e-02,  4.9406e-01,\n",
      "         -4.5016e-03, -3.7298e-02, -2.5126e-02,  4.8106e-01,  4.9260e-01,\n",
      "         -3.9747e-02,  4.9569e-01,  3.2751e-03,  4.8417e-01,  1.3537e-03,\n",
      "          4.6841e-01, -1.5501e-02, -2.9006e-02,  4.8824e-01, -2.2285e-02,\n",
      "          3.1370e-03,  4.7110e-01, -9.9325e-03, -3.3563e-02, -2.6896e-02,\n",
      "          4.6542e-01, -2.7872e-02, -3.5138e-02,  4.8269e-01, -2.0192e-02,\n",
      "          5.0941e-01,  5.2357e-01, -1.5566e-02,  5.0868e-01,  5.2066e-01,\n",
      "          2.8422e-04,  5.0007e-01,  4.8778e-01, -2.4464e-05, -1.2617e-02,\n",
      "          4.9592e-01, -7.9419e-03,  2.2357e-03,  5.1602e-01, -2.6572e-02,\n",
      "          5.0347e-01,  5.1020e-01,  5.1198e-01, -1.5236e-02,  5.0745e-01,\n",
      "         -2.9959e-02,  4.6592e-01,  4.8999e-01,  5.0134e-01,  4.7150e-01,\n",
      "         -3.4886e-02,  4.7203e-01]])\n",
      "fc3.bias tensor([0.1702])\n",
      "fc1_title.weight: Frobenius norm = 17.0360, Normalized Frobenius norm = 0.7529 (Elements: 512)\n",
      "fc1_title.bias: Frobenius norm = 14.8578, Normalized Frobenius norm = 0.6566 (Elements: 512)\n",
      "fc1_vocab.bias: Frobenius norm = 10.6628, Normalized Frobenius norm = 0.4712 (Elements: 512)\n",
      "fc3.weight: Frobenius norm = 8.0136, Normalized Frobenius norm = 0.3542 (Elements: 512)\n",
      "fc1_video.bias: Frobenius norm = 7.6480, Normalized Frobenius norm = 0.3380 (Elements: 512)\n",
      "fc1_video.weight: Frobenius norm = 338.9778, Normalized Frobenius norm = 0.3310 (Elements: 1048576)\n",
      "fc2.bias: Frobenius norm = 6.0607, Normalized Frobenius norm = 0.2678 (Elements: 512)\n",
      "fc2.weight: Frobenius norm = 237.1816, Normalized Frobenius norm = 0.2675 (Elements: 786432)\n",
      "fc1_vocab.weight: Frobenius norm = 109.8145, Normalized Frobenius norm = 0.2477 (Elements: 196608)\n",
      "fc3.bias: Frobenius norm = 0.1702, Normalized Frobenius norm = 0.1702 (Elements: 1)\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import math\n",
    "\n",
    "class CustomNeuralNetworkModel(nn.Module):\n",
    "    def __init__(self, vocab_size=384, video_feature_size=2048, hidden_size=512, dropout_rate=0.01):\n",
    "        super(CustomNeuralNetworkModel, self).__init__()\n",
    "        # Define the first layer weights\n",
    "        self.fc1_vocab = nn.Linear(vocab_size, hidden_size)\n",
    "        self.fc1_video = nn.Linear(video_feature_size, hidden_size)\n",
    "        self.fc1_title = nn.Linear(1, hidden_size)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Define the second layer (hidden layer) weights\n",
    "        self.fc2 = nn.Linear(hidden_size * 3, hidden_size)  # Combining all features into a single hidden layer\n",
    "\n",
    "        # Output layer\n",
    "        self.fc3 = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        # Non-linear activation function\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, Tcount, Tvocab, Tvideo_mean):\n",
    "        # Ensure all inputs are of type torch.float32\n",
    "        Tcount = Tcount.float()\n",
    "        Tvocab = Tvocab.float()\n",
    "        Tvideo_mean = Tvideo_mean.float()\n",
    "\n",
    "        # Process each input through its respective first layer and apply ReLU activation\n",
    "        Tcount = self.relu(self.fc1_title(Tcount))\n",
    "        Tvocab = self.relu(self.fc1_vocab(Tvocab))\n",
    "        Tvideo_mean = self.relu(self.fc1_video(Tvideo_mean))\n",
    "        \n",
    "        # Concatenate all features\n",
    "        combined_features = torch.cat((Tcount, Tvocab, Tvideo_mean), dim=1)\n",
    "        \n",
    "        # Pass through additional hidden layers with ReLU activation and dropout\n",
    "        hidden_output = self.relu(self.fc2(combined_features))\n",
    "        hidden_output = self.dropout(hidden_output)\n",
    "        \n",
    "        # Output layer\n",
    "        Y = self.fc3(hidden_output)\n",
    "        return Y.squeeze()  # Remove extra dimensions for scalar output\n",
    "\n",
    "def custom_loss(outputs, targets, error_threshold=20.0, reward_factor=1000000):\n",
    "    \"\"\"\n",
    "    Custom loss function that penalizes:\n",
    "    1. Negative predictions.\n",
    "    2. Predictions with percentage error greater than a specified threshold (e.g., 20%).\n",
    "\n",
    "    Parameters:\n",
    "    - outputs: model predictions.\n",
    "    - targets: true values.\n",
    "    - penalty_factor: factor to scale the penalty for negative predictions and high percentage errors.\n",
    "    - error_threshold: percentage error threshold beyond which to apply penalties.\n",
    "\n",
    "    Returns:\n",
    "    - loss: computed loss with penalties for negative predictions and high percentage errors.\n",
    "    \"\"\"    \n",
    "    # Calculate penalty for negative predictions\n",
    "    negative_penalty = (outputs < 0).float() * 50000000 * torch.abs(outputs)\n",
    "    negative_penalty = negative_penalty.mean()\n",
    "\n",
    "    percentage_error = torch.abs((outputs - targets) / targets) * 100\n",
    "    error_penalty = (percentage_error > error_threshold).float() * 5000000 * torch.abs(outputs - targets)\n",
    "    error_penalty = error_penalty.mean()\n",
    "\n",
    "    # Reward for predictions within the error threshold\n",
    "    reward = (percentage_error <= error_threshold).float() * reward_factor * torch.abs(outputs - targets)\n",
    "    reward = reward.mean()\n",
    "\n",
    "    loss = negative_penalty + error_penalty - reward\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calculate_percentage_error(predicted, actual):\n",
    "    \"\"\"\n",
    "    Calculate the percentage error between predicted and actual values.\n",
    "    \n",
    "    Parameters:\n",
    "    - predicted: Predicted values by the model.\n",
    "    - actual: Actual values.\n",
    "    \n",
    "    Returns:\n",
    "    - percentage_error: The mean percentage error.\n",
    "    \"\"\"\n",
    "    percentage_error = torch.abs((predicted - actual) / actual) * 100\n",
    "    return percentage_error.mean()\n",
    "\n",
    "def standardize(tensor, mean, std):\n",
    "    return (tensor - mean) / (std + 1e-3)\n",
    "\n",
    "# Assuming you have a DataLoader for your training data: train_loader\n",
    "# First, calculate the mean and std for vocab_embed and video_mean features from the training data\n",
    "\n",
    "# Placeholder for mean and std calculation\n",
    "vocab_embed_mean, vocab_embed_std = torch.tensor(0.0), torch.tensor(1.0)\n",
    "video_mean_mean, video_mean_std = torch.tensor(0.0), torch.tensor(1.0)\n",
    "\n",
    "# Initialize model with a specified hidden size\n",
    "model = CustomNeuralNetworkModel(vocab_size=384, video_feature_size=2048, hidden_size=512)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0004, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    total_percentage_error = 0.0\n",
    "    count = 0\n",
    "\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        title_count, vocab_embed, video_mean, view_counts = data\n",
    "        title_count = title_count.unsqueeze(1).float()\n",
    "        \n",
    "        # Standardize features\n",
    "        vocab_embed = standardize(vocab_embed.float(), vocab_embed_mean, vocab_embed_std)\n",
    "        video_mean = standardize(video_mean.float(), video_mean_mean, video_mean_std)\n",
    "        \n",
    "        view_counts = view_counts.float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(title_count, vocab_embed, video_mean)\n",
    "\n",
    "        l1_lambda = 0.0001\n",
    "        l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "        loss = custom_loss(outputs, view_counts) + l1_lambda * l1_norm\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        total_percentage_error += calculate_percentage_error(outputs, view_counts).item()\n",
    "        count += 1\n",
    "\n",
    "    mean_percentage_error = total_percentage_error / count\n",
    "    print(f'Epoch {epoch+1}, Loss: {math.sqrt(running_loss/len(train_loader))}')\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)\n",
    "\n",
    "\n",
    "def calculate_and_compare_norms(model):\n",
    "    norm_data = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            frobenius_norm = torch.norm(param, p='fro').item()\n",
    "            total_elements = param.numel()  # Total number of elements in the parameter tensor\n",
    "            normalized_frobenius_norm = frobenius_norm / math.sqrt(total_elements)\n",
    "            norm_data.append((name, frobenius_norm, normalized_frobenius_norm, total_elements))\n",
    "\n",
    "    # Sort the data based on the normalized Frobenius norm to find the most important weights\n",
    "    norm_data.sort(key=lambda x: x[2], reverse=True)\n",
    "    return norm_data\n",
    "\n",
    "norm_data = calculate_and_compare_norms(model)\n",
    "for data in norm_data:\n",
    "    print(f\"{data[0]}: Frobenius norm = {data[1]:.4f}, Normalized Frobenius norm = {data[2]:.4f} (Elements: {data[3]})\")\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "torch.save(model.state_dict(), 'view_count_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'img_to_array' from 'keras.preprocessing.image' (c:\\Users\\howel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\preprocessing\\image.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LSTM, Dense, Input, concatenate, GlobalAveragePooling2D\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ResNet50\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m img_to_array, load_img\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'img_to_array' from 'keras.preprocessing.image' (c:\\Users\\howel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\preprocessing\\image.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Dense, Input, concatenate, GlobalAveragePooling2D\n",
    "from keras.applications import ResNet50\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def extract_video_features(video_frames_folder):\n",
    "    \"\"\"\n",
    "    Extracts features from video frames using ResNet50.\n",
    "    \"\"\"\n",
    "    cnn_model = ResNet50(weights='imagenet', include_top=False)\n",
    "    features_list = []\n",
    "    for frame in sorted(os.listdir(video_frames_folder)):\n",
    "        img_path = os.path.join(video_frames_folder, frame)\n",
    "        img = load_img(img_path, target_size=(224, 224))\n",
    "        img_array = img_to_array(img)\n",
    "        expanded_img_array = np.expand_dims(img_array, axis=0)\n",
    "        features = cnn_model.predict(expanded_img_array)\n",
    "        pooled_features = GlobalAveragePooling2D()(features)\n",
    "        features_list.append(pooled_features)\n",
    "    return np.array(features_list)\n",
    "\n",
    "def preprocess_metadata(metadata_df):\n",
    "    \"\"\"\n",
    "    Preprocesses metadata using standard scaling.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    scaled_metadata = scaler.fit_transform(metadata_df)\n",
    "    return scaled_metadata\n",
    "\n",
    "def create_hybrid_model(metadata_shape, video_feature_shape):\n",
    "    \"\"\"\n",
    "    Creates a hybrid model combining NN for metadata and CNN-LSTM for video content.\n",
    "    \"\"\"\n",
    "    # Metadata input branch\n",
    "    metadata_input = Input(shape=(metadata_shape,), name='metadata_input')\n",
    "    metadata_branch = Dense(128, activation='relu')(metadata_input)\n",
    "    \n",
    "    # Video content input branch\n",
    "    video_input = Input(shape=(None, video_feature_shape), name='video_input')\n",
    "    video_branch = LSTM(256, return_sequences=True)(video_input)\n",
    "    video_branch = LSTM(128)(video_branch)\n",
    "    \n",
    "    # Merge branches\n",
    "    merged = concatenate([metadata_branch, video_branch])\n",
    "    merged = Dense(64, activation='relu')(merged)\n",
    "    output = Dense(1, activation='linear')(merged)\n",
    "    \n",
    "    model = Model(inputs=[metadata_input, video_input], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Assume 'metadata.csv' contains video metadata and 'videos/' contains video frame folders\n",
    "metadata_df = pd.read_csv('metadata.csv')\n",
    "video_features = []  # Placeholder for extracted video features\n",
    "for video_folder in sorted(os.listdir('videos/')):\n",
    "    folder_path = os.path.join('videos/', video_folder)\n",
    "    video_features.append(extract_video_features(folder_path))\n",
    "\n",
    "# Preprocess metadata\n",
    "metadata = preprocess_metadata(metadata_df)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train_meta, X_test_meta, X_train_video, X_test_video, y_train, y_test = train_test_split(\n",
    "    metadata, video_features, scores, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "model = create_hybrid_model(metadata.shape[1], video_features.shape[2])\n",
    "\n",
    "# Assuming 'scores' is a numpy array containing the target engagement scores\n",
    "model.fit([X_train_meta, X_train_video], y_train, validation_split=0.2, epochs=10)\n",
    "\n",
    "# Evaluate the model\n",
    "model.evaluate([X_test_meta, X_test_video], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'img_to_array' from 'keras.preprocessing.image' (c:\\Users\\howel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\preprocessing\\image.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m img_to_array, load_img\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load the pre-trained ResNet50 model for feature extraction\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'img_to_array' from 'keras.preprocessing.image' (c:\\Users\\howel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\preprocessing\\image.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the pre-trained ResNet50 model for feature extraction\n",
    "def load_cnn_model():\n",
    "    return keras.applications.ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "\n",
    "# Function to extract features from a single image\n",
    "def extract_features(img_path, cnn_model):\n",
    "    img = load_img(img_path, target_size=(224, 224))\n",
    "    img_array = img_to_array(img)\n",
    "    expanded_img_array = np.expand_dims(img_array, axis=0)\n",
    "    preprocessed_img = keras.applications.resnet50.preprocess_input(expanded_img_array)\n",
    "    features = cnn_model.predict(preprocessed_img)\n",
    "    return features\n",
    "\n",
    "# Function to process a video and calculate mean feature vector\n",
    "def process_video(video_frames_folder, cnn_model):\n",
    "    frame_features = []\n",
    "    for frame in sorted(os.listdir(video_frames_folder)):\n",
    "        frame_path = os.path.join(video_frames_folder, frame)\n",
    "        features = extract_features(frame_path, cnn_model)\n",
    "        frame_features.append(features)\n",
    "    if frame_features:\n",
    "        mean_feature_vector = np.mean(np.array(frame_features).squeeze(), axis=0)\n",
    "        return mean_feature_vector\n",
    "    return None\n",
    "\n",
    "# Load the saved LSTM model\n",
    "def load_lstm_model(model_path='model/most_replayed_model.keras'):\n",
    "    return keras.models.load_model(model_path)\n",
    "\n",
    "# Main script to predict intensity scores for a video\n",
    "def predict_intensity_scores(video_frames_folder, cnn_model, lstm_model):\n",
    "    # Process the video to get the mean feature vector\n",
    "    mean_feature_vector = process_video(video_frames_folder, cnn_model)\n",
    "    if mean_feature_vector is not None:\n",
    "        # Predict the intensity score using the LSTM model\n",
    "        mean_feature_vector = np.expand_dims(mean_feature_vector, axis=0)  # Add batch dimension\n",
    "        predicted_intensity_score = lstm_model.predict(mean_feature_vector)\n",
    "        return predicted_intensity_score\n",
    "    return None\n",
    "\n",
    "# Assuming you have true intensity scores for evaluation\n",
    "def evaluate_model(true_intensity_scores, predicted_intensity_scores):\n",
    "    mse = mean_squared_error(true_intensity_scores, predicted_intensity_scores)\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Example usage\n",
    "video_frames_folder = \"videos/_7VXXHn-AaY\"  # Update this path\n",
    "true_intensity_scores = np.array([0.5])  # Example true intensity score\n",
    "\n",
    "cnn_model = load_cnn_model()\n",
    "lstm_model = load_lstm_model()\n",
    "predicted_intensity_scores = predict_intensity_scores(video_frames_folder, cnn_model, lstm_model)\n",
    "\n",
    "if predicted_intensity_scores is not None:\n",
    "    print(f\"Predicted Intensity Score: {predicted_intensity_scores}\")\n",
    "    evaluate_model(true_intensity_scores, predicted_intensity_scores)\n",
    "else:\n",
    "    print(\"Could not process video.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
